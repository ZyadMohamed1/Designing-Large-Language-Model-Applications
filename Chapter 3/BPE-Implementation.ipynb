{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe766fe8",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b2b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from datasets import load_dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fca222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = {}\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        \n",
    "    def get_stats(self, word_freqs):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in word_freqs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, word_freqs):\n",
    "        new_word_freqs = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        \n",
    "        for word in word_freqs:\n",
    "            new_word = p.sub(''.join(pair), word)\n",
    "            new_word_freqs[new_word] = word_freqs[word]\n",
    "        return new_word_freqs\n",
    "    \n",
    "    def get_word_tokens(self, text):\n",
    "        # Simple tokenization - split on whitespace and punctuation\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        word_freqs = Counter(words)\n",
    "        \n",
    "        # Add end-of-word marker and split into characters\n",
    "        processed_words = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            processed_words[' '.join(word) + ' </w>'] = freq\n",
    "        \n",
    "        return processed_words\n",
    "    \n",
    "    def train(self, texts):\n",
    "        print(\"Processing texts and building word frequencies...\")\n",
    "        \n",
    "        # Collect all words and their frequencies\n",
    "        all_word_freqs = defaultdict(int)\n",
    "        for text in texts:\n",
    "            word_freqs = self.get_word_tokens(text)\n",
    "            for word, freq in word_freqs.items():\n",
    "                all_word_freqs[word] += freq\n",
    "        \n",
    "        self.word_freqs = dict(all_word_freqs)\n",
    "        print(f\"Found {len(self.word_freqs)} unique words\")\n",
    "        \n",
    "        # Get initial vocabulary (all characters)\n",
    "        vocab = set()\n",
    "        for word in self.word_freqs.keys():\n",
    "            vocab.update(word.split())\n",
    "        \n",
    "        # Create initial vocab with indices\n",
    "        self.vocab = {token: i for i, token in enumerate(sorted(vocab))}\n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        # Perform BPE merges\n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        print(f\"Performing {num_merges} merges...\")\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(self.word_freqs)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            self.word_freqs = self.merge_vocab(best_pair, self.word_freqs)\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            # Add new token to vocabulary\n",
    "            new_token = ''.join(best_pair)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"Completed {i + 1} merges, current vocab size: {len(self.vocab)}\")\n",
    "        \n",
    "        print(f\"Training complete! Final vocabulary size: {len(self.vocab)}\")\n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        word = ' '.join(word) + ' </w>'\n",
    "        \n",
    "        # Apply merges in order\n",
    "        for pair in self.merges:\n",
    "            bigram = re.escape(' '.join(pair))\n",
    "            p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "            word = p.sub(''.join(pair), word)\n",
    "        \n",
    "        return word.split()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = self.encode_word(word)\n",
    "            tokens.extend(word_tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def get_vocab_tokens(self, min_length=3):\n",
    "        # Create reverse vocab for easy lookup\n",
    "        reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Get tokens with their characteristics\n",
    "        tokens_info = []\n",
    "        for idx, token in reverse_vocab.items():\n",
    "            if len(token) >= min_length and not token.endswith('</w>'):\n",
    "                tokens_info.append({\n",
    "                    'token': token,\n",
    "                    'length': len(token),\n",
    "                    'is_word_end': token.endswith('</w>'),\n",
    "                    'index': idx\n",
    "                })\n",
    "        \n",
    "        # Sort by length (descending) then alphabetically\n",
    "        tokens_info.sort(key=lambda x: (-x['length'], x['token']))\n",
    "        return tokens_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6dd861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary(bpe_tokenizer, sample_size=50):\n",
    "    print(\"\\n=== VOCABULARY ANALYSIS ===\")\n",
    "    \n",
    "    vocab_tokens = bpe_tokenizer.get_vocab_tokens()\n",
    "    \n",
    "    print(f\"Total vocabulary size: {len(bpe_tokenizer.vocab)}\")\n",
    "    print(f\"Tokens with length >= 3: {len(vocab_tokens)}\")\n",
    "    \n",
    "    print(f\"\\nTop {sample_size} longest tokens:\")\n",
    "    for i, token_info in enumerate(vocab_tokens[:sample_size]):\n",
    "        print(f\"{i+1:2d}. '{token_info['token']}' (length: {token_info['length']})\")\n",
    "    \n",
    "    # Analyze token length distribution\n",
    "    length_dist = defaultdict(int)\n",
    "    for token in bpe_tokenizer.vocab.keys():\n",
    "        length_dist[len(token)] += 1\n",
    "    \n",
    "    print(f\"\\nToken length distribution:\")\n",
    "    for length in sorted(length_dist.keys()):\n",
    "        print(f\"Length {length}: {length_dist[length]} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d296aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_ml_papers(bpe_tokenizer, num_samples=10):\n",
    "    print(\"\\n=== TESTING ON ML PAPERS ===\")\n",
    "    \n",
    "    # Try to load ML papers dataset\n",
    "    try:\n",
    "        print(\"Attempting to load ML papers dataset...\")\n",
    "        dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\", streaming=True)\n",
    "        papers = []\n",
    "        \n",
    "        print(\"Loading ML papers sample...\")\n",
    "        for i, paper in enumerate(dataset):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            # Combine title and abstract for analysis\n",
    "            title = paper.get('title', '')\n",
    "            abstract = paper.get('abstract', '')\n",
    "            if title and abstract:\n",
    "                text = title + ' ' + abstract\n",
    "                papers.append(text)\n",
    "        \n",
    "        print(f\"Loaded {len(papers)} papers\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ML papers dataset not available: {e}\")\n",
    "        print(\"Using sample ML paper texts...\")\n",
    "        \n",
    "        # Sample ML paper abstracts for testing\n",
    "        papers = [\n",
    "            \"\"\"Attention Is All You Need: We propose a new simple network architecture, the Transformer, \n",
    "            based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    "            Experiments on two machine translation tasks show these models to be superior in quality \n",
    "            while being more parallelizable and requiring significantly less time to train.\"\"\",\n",
    "            \n",
    "            \"\"\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: We introduce \n",
    "            a new language representation model called BERT, which stands for Bidirectional Encoder \n",
    "            Representations from Transformers. BERT is designed to pre-train deep bidirectional \n",
    "            representations from unlabeled text by jointly conditioning on both left and right context.\"\"\",\n",
    "            \n",
    "            \"\"\"Deep Residual Learning for Image Recognition: We present a residual learning framework \n",
    "            to ease the training of networks that are substantially deeper than those used previously. \n",
    "            We explicitly reformulate the layers as learning residual functions with reference to the \n",
    "            layer inputs, instead of learning unreferenced functions.\"\"\",\n",
    "            \n",
    "            \"\"\"Generative Adversarial Networks: We propose a new framework for estimating generative \n",
    "            models via an adversarial process, in which we simultaneously train two models: a generative \n",
    "            model G that captures the data distribution, and a discriminative model D that estimates \n",
    "            the probability that a sample came from the training data.\"\"\",\n",
    "            \n",
    "            \"\"\"Adam: A Method for Stochastic Optimization: We introduce Adam, an algorithm for first-order \n",
    "            gradient-based optimization of stochastic objective functions, based on adaptive estimates \n",
    "            of lower-order moments. The method is straightforward to implement, is computationally \n",
    "            efficient, and has little memory requirements.\"\"\"\n",
    "        ]\n",
    "    \n",
    "    # Test tokenization on technical terms\n",
    "    technical_terms = [\n",
    "        'neural network', 'machine learning', 'deep learning', 'transformer',\n",
    "        'attention mechanism', 'backpropagation', 'gradient descent', \n",
    "        'convolutional', 'recurrent', 'reinforcement learning',\n",
    "        'natural language processing', 'computer vision', 'optimization',\n",
    "        'generative adversarial', 'residual learning', 'bidirectional'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nTechnical term tokenization:\")\n",
    "    for term in technical_terms:\n",
    "        tokens = bpe_tokenizer.encode(term)\n",
    "        print(f\"'{term}' -> {tokens}\")\n",
    "    \n",
    "    # Analyze a sample paper\n",
    "    if papers:\n",
    "        print(f\"\\nSample paper tokenization:\")\n",
    "        sample_text = papers[0][:300] + \"...\"  # First 300 chars\n",
    "        tokens = bpe_tokenizer.encode(sample_text)\n",
    "        print(f\"Text: {sample_text}\")\n",
    "        print(f\"Tokens ({len(tokens)}): {tokens[:25]}...\")  # First 25 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acbd9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"=== SIMPLE BPE TOKENIZER IMPLEMENTATION ===\")\n",
    "    \n",
    "    # Initialize BPE tokenizer\n",
    "    bpe = SimpleBPE(vocab_size=10000)\n",
    "    \n",
    "    # Try multiple dataset options\n",
    "    print(\"Loading training dataset...\")\n",
    "    texts = []\n",
    "    \n",
    "    # Option 1: Try OpenWebText\n",
    "    try:\n",
    "        print(\"Attempting to load OpenWebText dataset...\")\n",
    "        dataset = load_dataset(\"openwebtext\", split=\"train\", streaming=True)\n",
    "        \n",
    "        print(\"Collecting training texts from OpenWebText...\")\n",
    "        for i, example in enumerate(dataset):\n",
    "            if i >= 500:  # Use first 500 documents\n",
    "                break\n",
    "            if len(example['text']) > 100:  # Skip very short texts\n",
    "                texts.append(example['text'])\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(f\"Loaded {i + 1} documents...\")\n",
    "        \n",
    "        print(f\"Successfully loaded {len(texts)} texts from OpenWebText\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"OpenWebText not available: {e}\")\n",
    "        \n",
    "        # Option 2: Try BookCorpus\n",
    "        try:\n",
    "            print(\"Attempting to load BookCorpus dataset...\")\n",
    "            dataset = load_dataset(\"bookcorpus\", split=\"train\", streaming=True)\n",
    "            \n",
    "            print(\"Collecting training texts from BookCorpus...\")\n",
    "            for i, example in enumerate(dataset):\n",
    "                if i >= 500:\n",
    "                    break\n",
    "                if len(example['text']) > 100:\n",
    "                    texts.append(example['text'])\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"Loaded {i + 1} books...\")\n",
    "            \n",
    "            print(f\"Successfully loaded {len(texts)} texts from BookCorpus\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"BookCorpus not available: {e}\")\n",
    "            \n",
    "            # Option 3: Try C4 (Common Crawl)\n",
    "            try:\n",
    "                print(\"Attempting to load C4 dataset...\")\n",
    "                dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)\n",
    "                \n",
    "                print(\"Collecting training texts from C4...\")\n",
    "                for i, example in enumerate(dataset):\n",
    "                    if i >= 300:\n",
    "                        break\n",
    "                    if len(example['text']) > 200:\n",
    "                        texts.append(example['text'])\n",
    "                    if (i + 1) % 50 == 0:\n",
    "                        print(f\"Loaded {i + 1} documents...\")\n",
    "                \n",
    "                print(f\"Successfully loaded {len(texts)} texts from C4\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"C4 not available: {e}\")\n",
    "                \n",
    "                # Option 4: Try Wikitext\n",
    "                try:\n",
    "                    print(\"Attempting to load Wikitext dataset...\")\n",
    "                    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "                    \n",
    "                    print(\"Processing Wikitext data...\")\n",
    "                    # Wikitext comes as one big text, split it into chunks\n",
    "                    full_text = dataset['text']\n",
    "                    text_chunks = []\n",
    "                    current_chunk = \"\"\n",
    "                    \n",
    "                    for line in full_text:\n",
    "                        if line.strip():\n",
    "                            current_chunk += line + \" \"\n",
    "                            if len(current_chunk) > 1000:  # Create chunks of ~1000 chars\n",
    "                                text_chunks.append(current_chunk.strip())\n",
    "                                current_chunk = \"\"\n",
    "                    \n",
    "                    texts = text_chunks[:1000]  # Take first 1000 chunks\n",
    "                    print(f\"Successfully loaded {len(texts)} text chunks from Wikitext\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Wikitext not available: {e}\")\n",
    "                    print(\"Using comprehensive sample texts for demonstration...\")\n",
    "                    \n",
    "                    # Extensive fallback sample texts\n",
    "                    texts = [\n",
    "                        \"\"\"The development of artificial intelligence has been one of the most significant technological \n",
    "                        advances of the 21st century. Machine learning algorithms can now process vast amounts of data \n",
    "                        and identify complex patterns that would be impossible for humans to detect manually.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Natural language processing involves the computational analysis of human language. Modern NLP \n",
    "                        systems use transformer architectures and attention mechanisms to understand context and generate \n",
    "                        coherent responses to user queries.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Computer vision systems have revolutionized image recognition and object detection. Convolutional \n",
    "                        neural networks can now achieve superhuman performance on many visual recognition tasks.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Deep learning models require massive computational resources for training. Graphics processing \n",
    "                        units and specialized tensor processing units have become essential hardware for machine learning \n",
    "                        research and deployment.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Reinforcement learning algorithms learn through interaction with their environment. These systems \n",
    "                        can master complex games and control systems by optimizing reward signals over time.\"\"\",\n",
    "                        \n",
    "                        \"\"\"The transformer architecture introduced the concept of self-attention, allowing models to focus \n",
    "                        on different parts of the input sequence when making predictions. This breakthrough enabled the \n",
    "                        development of large language models.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Gradient descent optimization algorithms are fundamental to training neural networks. Variants \n",
    "                        like Adam and RMSprop help models converge more efficiently by adapting learning rates during \n",
    "                        training.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Regularization techniques like dropout and batch normalization help prevent overfitting in \n",
    "                        deep neural networks. These methods improve generalization performance on unseen data.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Transfer learning allows models trained on large datasets to be fine-tuned for specific tasks \n",
    "                        with limited data. This approach has democratized access to powerful machine learning capabilities.\"\"\",\n",
    "                        \n",
    "                        \"\"\"Ensemble methods combine multiple models to improve prediction accuracy and robustness. Random \n",
    "                        forests and gradient boosting are popular ensemble techniques in machine learning.\"\"\"\n",
    "                    ] * 100  # Repeat for more diverse training data\n",
    "    \n",
    "    # Train the BPE tokenizer\n",
    "    bpe.train(texts)\n",
    "    \n",
    "    # Analyze the vocabulary\n",
    "    analyze_vocabulary(bpe)\n",
    "    \n",
    "    # Test on ML papers\n",
    "    test_on_ml_papers(bpe)\n",
    "    \n",
    "    # Save the tokenizer\n",
    "    print(\"\\nSaving tokenizer...\")\n",
    "    with open('bpe_tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(bpe, f)\n",
    "    \n",
    "    print(\"BPE tokenizer saved as 'bpe_tokenizer.pkl'\")\n",
    "    \n",
    "    return bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d9a88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SIMPLE BPE TOKENIZER IMPLEMENTATION ===\n",
      "Loading training dataset...\n",
      "Attempting to load OpenWebText dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbf3f7406fa4daa9162050a5bddacde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\datasets--openwebtext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300d17a7f37e44cab162efd1f984d966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openwebtext.py:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenWebText not available: The repository for openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/openwebtext.\n",
      "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
      "Attempting to load BookCorpus dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343744522fbc41c1b55f670c16c336ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/18.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\datasets--bookcorpus. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87141571f84441bdafaa596dbe6d2c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bookcorpus.py:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BookCorpus not available: The repository for bookcorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bookcorpus.\n",
      "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
      "Attempting to load C4 dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493c54681d6945cc9dcd4bcd8ff9be8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\datasets--c4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146374704e164ca7849daa29cbe6a09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "c4.py:   0%|          | 0.00/3.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4 not available: The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4.\n",
      "Please pass the argument `trust_remote_code=True` to allow custom code to be run.\n",
      "Attempting to load Wikitext dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fede867f05d34e70b4d927a3dbf5245e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikitext not available: (ReadTimeoutError(\"HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6afc159a-a7b1-432f-9ec8-cea3a722e83b)')\n",
      "Using comprehensive sample texts for demonstration...\n",
      "Processing texts and building word frequencies...\n",
      "Found 176 unique words\n",
      "Initial vocabulary size: 29\n",
      "Performing 9971 merges...\n",
      "Training complete! Final vocabulary size: 635\n",
      "\n",
      "=== VOCABULARY ANALYSIS ===\n",
      "Total vocabulary size: 635\n",
      "Tokens with length >= 3: 319\n",
      "\n",
      "Top 50 longest tokens:\n",
      " 1. 'architecture' (length: 12)\n",
      " 2. 'architectur' (length: 11)\n",
      " 3. 'technologic' (length: 11)\n",
      " 4. 'architectu' (length: 10)\n",
      " 5. 'capabiliti' (length: 10)\n",
      " 6. 'democratiz' (length: 10)\n",
      " 7. 'intelligen' (length: 10)\n",
      " 8. 'prediction' (length: 10)\n",
      " 9. 'revolution' (length: 10)\n",
      "10. 'significan' (length: 10)\n",
      "11. 'architect' (length: 9)\n",
      "12. 'democrati' (length: 9)\n",
      "13. 'efficient' (length: 9)\n",
      "14. 'fundament' (length: 9)\n",
      "15. 'introduce' (length: 9)\n",
      "16. 'performan' (length: 9)\n",
      "17. 'reinforce' (length: 9)\n",
      "18. 'robustnes' (length: 9)\n",
      "19. 'technolog' (length: 9)\n",
      "20. 'transform' (length: 9)\n",
      "21. 'algorith' (length: 8)\n",
      "22. 'approach' (length: 8)\n",
      "23. 'architec' (length: 8)\n",
      "24. 'artifici' (length: 8)\n",
      "25. 'capabili' (length: 8)\n",
      "26. 'mechanis' (length: 8)\n",
      "27. 'powerful' (length: 8)\n",
      "28. 'research' (length: 8)\n",
      "29. 'response' (length: 8)\n",
      "30. 'robustne' (length: 8)\n",
      "31. 'signific' (length: 8)\n",
      "32. 'specific' (length: 8)\n",
      "33. 'techniqu' (length: 8)\n",
      "34. 'technolo' (length: 8)\n",
      "35. 'transfor' (length: 8)\n",
      "36. 'accurac' (length: 7)\n",
      "37. 'analysi' (length: 7)\n",
      "38. 'complex' (length: 7)\n",
      "39. 'computa' (length: 7)\n",
      "40. 'control' (length: 7)\n",
      "41. 'convolu' (length: 7)\n",
      "42. 'democra' (length: 7)\n",
      "43. 'develop' (length: 7)\n",
      "44. 'environ' (length: 7)\n",
      "45. 'essenti' (length: 7)\n",
      "46. 'generat' (length: 7)\n",
      "47. 'graphic' (length: 7)\n",
      "48. 'identif' (length: 7)\n",
      "49. 'impossi' (length: 7)\n",
      "50. 'intelli' (length: 7)\n",
      "\n",
      "Token length distribution:\n",
      "Length 1: 28 tokens\n",
      "Length 2: 74 tokens\n",
      "Length 3: 81 tokens\n",
      "Length 4: 73 tokens\n",
      "Length 5: 63 tokens\n",
      "Length 6: 68 tokens\n",
      "Length 7: 44 tokens\n",
      "Length 8: 41 tokens\n",
      "Length 9: 27 tokens\n",
      "Length 10: 26 tokens\n",
      "Length 11: 31 tokens\n",
      "Length 12: 32 tokens\n",
      "Length 13: 7 tokens\n",
      "Length 14: 13 tokens\n",
      "Length 15: 12 tokens\n",
      "Length 16: 6 tokens\n",
      "Length 17: 6 tokens\n",
      "Length 18: 3 tokens\n",
      "\n",
      "=== TESTING ON ML PAPERS ===\n",
      "Attempting to load ML papers dataset...\n",
      "Loading ML papers sample...\n",
      "Loaded 10 papers\n",
      "\n",
      "Technical term tokenization:\n",
      "'neural network' -> ['neural</w>', 'networ', 'k', '</w>']\n",
      "'machine learning' -> ['machine</w>', 'learning</w>']\n",
      "'deep learning' -> ['deep</w>', 'learning</w>']\n",
      "'transformer' -> ['transformer</w>']\n",
      "'attention mechanism' -> ['attention</w>', 'mechanis', 'm', '</w>']\n",
      "'backpropagation' -> ['b', 'ac', 'k', 'pro', 'p', 'a', 'g', 'a', 'tion</w>']\n",
      "'gradient descent' -> ['gradient</w>', 'descent</w>']\n",
      "'convolutional' -> ['convolutional</w>']\n",
      "'recurrent' -> ['re', 'c', 'ur', 'r', 'ent</w>']\n",
      "'reinforcement learning' -> ['reinforcement</w>', 'learning</w>']\n",
      "'natural language processing' -> ['natural</w>', 'language</w>', 'processing</w>']\n",
      "'computer vision' -> ['computer</w>', 'vision</w>']\n",
      "'optimization' -> ['optimization</w>']\n",
      "'generative adversarial' -> ['gener', 'a', 'ti', 've</w>', 'adv', 'er', 's', 'ar', 'i', 'al</w>']\n",
      "'residual learning' -> ['re', 'si', 'd', 'u', 'al</w>', 'learning</w>']\n",
      "'bidirectional' -> ['b', 'i', 'di', 're', 'c', 'tional</w>']\n",
      "\n",
      "Sample paper tokenization:\n",
      "Text: Learning from compressed observations   The problem of statistical learning is to construct a predictor of a random\n",
      "variable $Y$ as a function of a related random variable $X$ on the basis of an\n",
      "i.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\n",
      "predictors are drawn from some s...\n",
      "Tokens (136): ['learning</w>', 'f', 'ro', 'm', '</w>', 'comp', 'res', 'se', 'd</w>', 'ob', 's', 'er', 'va', 'tion', 's</w>', 'the</w>', 'pro', 'b', 'le', 'm', '</w>', 'of</w>', 's', 'ta', 'ti']...\n",
      "\n",
      "Saving tokenizer...\n",
      "BPE tokenizer saved as 'bpe_tokenizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tokenizer = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019e739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
