{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb01372",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8675fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff3ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(url):\n",
    "    print(f\"Downloading from {url}\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    tokens = []\n",
    "    for line in response.content.splitlines():\n",
    "        if line:\n",
    "            token_b64, rank = line.split()\n",
    "            try:\n",
    "                # Decode base64 token\n",
    "                token_bytes = base64.b64decode(token_b64)\n",
    "                token = token_bytes.decode('utf-8')\n",
    "                tokens.append(token)\n",
    "            except:\n",
    "                # Keep problematic tokens as bytes representation\n",
    "                tokens.append(str(token_bytes))\n",
    "    \n",
    "    return set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe5ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_types(tokens):\n",
    "    categories = {\n",
    "        'english_words': [],\n",
    "        'numbers': [],\n",
    "        'punctuation': [],\n",
    "        'whitespace': [],\n",
    "        'non_english': [],\n",
    "        'mixed': [],\n",
    "        'other': []\n",
    "    }\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token.isascii():\n",
    "            categories['english_words'].append(token)\n",
    "        elif token.isdigit():\n",
    "            categories['numbers'].append(token)\n",
    "        elif all(not c.isalnum() and c.isascii() for c in token) and token.strip():\n",
    "            categories['punctuation'].append(token)\n",
    "        elif token.isspace():\n",
    "            categories['whitespace'].append(token)\n",
    "        elif not token.isascii():\n",
    "            categories['non_english'].append(token)\n",
    "        elif any(c.isalpha() for c in token) and any(c.isdigit() for c in token):\n",
    "            categories['mixed'].append(token)\n",
    "        else:\n",
    "            categories['other'].append(token)\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c037ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"üîç Comparing GPT Tokenizer Vocabularies\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Download vocabularies\n",
    "    o200k_tokens = get_vocab(\"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\")\n",
    "    cl100k_tokens = get_vocab(\"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\")\n",
    "    \n",
    "    print(f\"\\nüìä Vocabulary Sizes:\")\n",
    "    print(f\"o200k_base (GPT-4o): {len(o200k_tokens):,} tokens\")\n",
    "    print(f\"cl100k_base (GPT-4): {len(cl100k_tokens):,} tokens\")\n",
    "    \n",
    "    # Find differences\n",
    "    only_o200k = o200k_tokens - cl100k_tokens\n",
    "    only_cl100k = cl100k_tokens - o200k_tokens\n",
    "    common = o200k_tokens & cl100k_tokens\n",
    "    \n",
    "    print(f\"\\nüîÑ Token Overlap:\")\n",
    "    print(f\"Common tokens: {len(common):,}\")\n",
    "    print(f\"Only in o200k: {len(only_o200k):,}\")\n",
    "    print(f\"Only in cl100k: {len(only_cl100k):,}\")\n",
    "    \n",
    "    # Analyze what's unique to each\n",
    "    print(f\"\\nüÜï New in o200k_base (examples):\")\n",
    "    o200k_categories = analyze_token_types(only_o200k)\n",
    "    for category, tokens in o200k_categories.items():\n",
    "        if tokens:\n",
    "            print(f\"  {category}: {len(tokens)} tokens\")\n",
    "            # Show first few examples\n",
    "            examples = sorted(tokens)[:3]\n",
    "            print(f\"    Examples: {examples}\")\n",
    "    \n",
    "    print(f\"\\nüóëÔ∏è Removed from cl100k_base (examples):\")\n",
    "    cl100k_categories = analyze_token_types(only_cl100k)\n",
    "    for category, tokens in cl100k_categories.items():\n",
    "        if tokens:\n",
    "            print(f\"  {category}: {len(tokens)} tokens\")\n",
    "            examples = sorted(tokens)[:3]\n",
    "            print(f\"    Examples: {examples}\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\nüí° Key Differences:\")\n",
    "    \n",
    "    # Count non-English tokens\n",
    "    o200k_non_english = len([t for t in o200k_tokens if not t.isascii()])\n",
    "    cl100k_non_english = len([t for t in cl100k_tokens if not t.isascii()])\n",
    "    \n",
    "    print(f\"‚Ä¢ Non-English tokens:\")\n",
    "    print(f\"  o200k: {o200k_non_english:,} ({o200k_non_english/len(o200k_tokens)*100:.1f}%)\")\n",
    "    print(f\"  cl100k: {cl100k_non_english:,} ({cl100k_non_english/len(cl100k_tokens)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"‚Ä¢ o200k_base has {len(only_o200k):,} new tokens for better multilingual support\")\n",
    "    print(f\"‚Ä¢ Vocabulary doubled in size for improved token efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052310bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comparing GPT Tokenizer Vocabularies\n",
      "==================================================\n",
      "Downloading from https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\n",
      "Downloading from https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\n",
      "\n",
      "üìä Vocabulary Sizes:\n",
      "o200k_base (GPT-4o): 199,998 tokens\n",
      "cl100k_base (GPT-4): 100,256 tokens\n",
      "\n",
      "üîÑ Token Overlap:\n",
      "Common tokens: 85,033\n",
      "Only in o200k: 114,965\n",
      "Only in cl100k: 15,223\n",
      "\n",
      "üÜï New in o200k_base (examples):\n",
      "  english_words: 15418 tokens\n",
      "    Examples: ['ABD', 'ABE', 'ABET']\n",
      "  numbers: 164 tokens\n",
      "    Examples: ['Ÿ†', 'Ÿ°', 'Ÿ¢']\n",
      "  punctuation: 426 tokens\n",
      "    Examples: ['\\x00\\x00', '\\n\\n\\n//', '\\n\\n//']\n",
      "  whitespace: 48 tokens\n",
      "    Examples: ['\\n\\n\\r\\n', '\\n\\n \\n', '\\n\\n \\n\\n']\n",
      "  non_english: 65341 tokens\n",
      "    Examples: [' (¬ß', ' (¬´', ' (‚Äò']\n",
      "  mixed: 875 tokens\n",
      "    Examples: [\"b' \\\\xc6'\", \"b' \\\\xd2'\", \"b' \\\\xd3'\"]\n",
      "  other: 32693 tokens\n",
      "    Examples: ['\\x01E', '\\tAccount', '\\tArrays']\n",
      "\n",
      "üóëÔ∏è Removed from cl100k_base (examples):\n",
      "  english_words: 4908 tokens\n",
      "    Examples: ['ACCEPT', 'ACEMENT', 'ACHER']\n",
      "  numbers: 2 tokens\n",
      "    Examples: ['‚ÇÄ', '‚ÇÅ']\n",
      "  punctuation: 1006 tokens\n",
      "    Examples: ['\\x1b[', ' !!}', ' !!}</']\n",
      "  whitespace: 55 tokens\n",
      "    Examples: ['\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n', '\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t ', '\\t\\t\\t\\t\\t\\t\\t\\t\\t  ']\n",
      "  non_english: 195 tokens\n",
      "    Examples: [' Fran√ß', ' Verf√ºg', ' autom√°t']\n",
      "  mixed: 112 tokens\n",
      "    Examples: [\"b' (\\\\xe2\\\\x88'\", \"b' \\\\xd1'\", \"b' \\\\xd1\\\\x80\\\\xd0\\\\xb0\\\\xd0'\"]\n",
      "  other: 8945 tokens\n",
      "    Examples: ['\\tADD', '\\tAL', '\\tAM']\n",
      "\n",
      "üí° Key Differences:\n",
      "‚Ä¢ Non-English tokens:\n",
      "  o200k: 69,601 (34.8%)\n",
      "  cl100k: 4,261 (4.3%)\n",
      "‚Ä¢ o200k_base has 114,965 new tokens for better multilingual support\n",
      "‚Ä¢ Vocabulary doubled in size for improved token efficiency\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c923078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
