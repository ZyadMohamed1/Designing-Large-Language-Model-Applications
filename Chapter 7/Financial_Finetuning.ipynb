{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odxmAZeZOm0q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOJAsP0kOpgX"
      },
      "outputs": [],
      "source": [
        "class FinancialPretrainingPipeline:\n",
        "    def __init__(self, model_name=\"meta-llama/Llama-3.2-1B\", max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.original_model = None\n",
        "\n",
        "    def load_model_and_tokenizer(self):\n",
        "        print(\"Loading Meta LLaMA 3.2-1B tokenizer and model...\")\n",
        "\n",
        "        # Load tokenizer - LLaMA 3.2 uses a different tokenizer\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        # Set padding token if not present\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Load model with appropriate settings for LLaMA 3.2-1B\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,  # May be needed for newer models\n",
        "        )\n",
        "\n",
        "        # Keep a copy of original model for comparison\n",
        "        self.original_model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        print(f\"Model loaded: {self.model_name}\")\n",
        "        print(f\"Model parameters: {self.model.num_parameters():,}\")\n",
        "        print(f\"Model size: ~{self.model.num_parameters() / 1e9:.1f}B parameters\")\n",
        "\n",
        "    def load_financial_dataset(self):\n",
        "        print(\"Loading financial documents dataset...\")\n",
        "\n",
        "        texts = []\n",
        "\n",
        "        # Try multiple financial datasets with error handling\n",
        "        datasets_to_try = [\n",
        "            {\n",
        "                'name': 'zeroshot/twitter-financial-news-sentiment',\n",
        "                'text_field': 'text',\n",
        "                'description': 'Financial Twitter sentiment data'\n",
        "            },\n",
        "            {\n",
        "                'name': 'financial_phrasebank',\n",
        "                'text_field': 'sentence',\n",
        "                'description': 'Financial PhraseBank'\n",
        "            },\n",
        "            {\n",
        "                'name': 'gbharti/finance-alpaca',\n",
        "                'text_field': 'output',\n",
        "                'description': 'Finance Alpaca dataset'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for dataset_info in datasets_to_try:\n",
        "            try:\n",
        "                print(f\"Trying to load {dataset_info['description']}...\")\n",
        "                dataset = load_dataset(dataset_info['name'], split=\"train\")\n",
        "\n",
        "                # Extract text from dataset\n",
        "                for item in dataset:\n",
        "                    text_content = None\n",
        "\n",
        "                    # Try different possible text fields\n",
        "                    for field in [dataset_info['text_field'], 'text', 'content', 'sentence', 'output', 'input']:\n",
        "                        if field in item and item[field]:\n",
        "                            text_content = str(item[field])\n",
        "                            break\n",
        "\n",
        "                    if text_content and len(text_content.strip()) > 50:  # Only keep substantial text\n",
        "                        texts.append(text_content.strip())\n",
        "\n",
        "                print(f\"✅ Loaded {len([t for t in texts])} texts from {dataset_info['description']}\")\n",
        "\n",
        "                if len(texts) > 10000:  # If we have enough data, break\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to load {dataset_info['name']}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # If no datasets worked, create synthetic financial text\n",
        "        if len(texts) < 100:\n",
        "            print(\"⚠️  No datasets loaded successfully. Creating synthetic financial text...\")\n",
        "            texts = self.create_synthetic_financial_text()\n",
        "\n",
        "        # Filter and clean texts\n",
        "        cleaned_texts = []\n",
        "        for text in texts:\n",
        "            if len(text.strip()) > 50 and len(text.strip()) < 2048:  # Reasonable length\n",
        "                cleaned_texts.append(text.strip())\n",
        "\n",
        "        print(f\"Total documents after cleaning: {len(cleaned_texts)}\")\n",
        "        return cleaned_texts\n",
        "\n",
        "    def create_synthetic_financial_text(self):\n",
        "        print(\"Creating synthetic financial documents...\")\n",
        "\n",
        "        templates = [\n",
        "            \"The quarterly earnings report shows revenue of ${} million, representing a {}% increase from the previous quarter.\",\n",
        "            \"The Federal Reserve announced a {}% interest rate adjustment, impacting bond yields and equity markets.\",\n",
        "            \"Market analysis indicates that {} sector is experiencing significant volatility due to regulatory changes.\",\n",
        "            \"The P/E ratio of {} has reached {}, suggesting the stock may be {} relative to industry averages.\",\n",
        "            \"Economic indicators point to {} inflation trends, with consumer spending {} by {}% this quarter.\",\n",
        "            \"The merger between {} and {} is expected to create synergies worth ${} million annually.\",\n",
        "            \"Dividend yield for {} has increased to {}%, making it attractive for income investors.\",\n",
        "            \"The cryptocurrency market shows {} correlation with traditional assets during recent market turbulence.\",\n",
        "            \"Supply chain disruptions have affected {} industry margins, with costs rising {}% year-over-year.\",\n",
        "            \"Central bank policy decisions regarding {} are expected to influence currency exchange rates significantly.\"\n",
        "        ]\n",
        "\n",
        "        import random\n",
        "\n",
        "        texts = []\n",
        "        for _ in range(5000):  # Generate 5000 synthetic financial texts\n",
        "            template = random.choice(templates)\n",
        "\n",
        "            # Fill in template with realistic financial terms\n",
        "            companies = [\"Apple\", \"Microsoft\", \"Amazon\", \"Tesla\", \"Google\", \"JPMorgan\", \"Goldman Sachs\"]\n",
        "            sectors = [\"technology\", \"healthcare\", \"financial services\", \"energy\", \"consumer goods\"]\n",
        "            percentages = [f\"{random.uniform(-10, 20):.1f}\" for _ in range(3)]\n",
        "            amounts = [f\"{random.randint(100, 5000)}\" for _ in range(2)]\n",
        "\n",
        "            try:\n",
        "                if \"{}\" in template:\n",
        "                    # Fill template with appropriate values\n",
        "                    filled_template = template.format(\n",
        "                        random.choice(amounts),\n",
        "                        random.choice(percentages),\n",
        "                        random.choice(sectors),\n",
        "                        random.choice(companies),\n",
        "                        random.choice(percentages),\n",
        "                        random.choice([\"undervalued\", \"overvalued\"]),\n",
        "                        random.choice([\"rising\", \"declining\", \"stable\"]),\n",
        "                        random.choice([\"increasing\", \"decreasing\"]),\n",
        "                        random.choice(percentages),\n",
        "                        random.choice(companies),\n",
        "                        random.choice(companies),\n",
        "                        random.choice(amounts),\n",
        "                        random.choice(companies),\n",
        "                        random.choice(percentages),\n",
        "                        random.choice([\"positive\", \"negative\", \"mixed\"]),\n",
        "                        random.choice(sectors),\n",
        "                        random.choice(percentages),\n",
        "                        random.choice([\"monetary policy\", \"fiscal policy\", \"trade policy\"])\n",
        "                    )\n",
        "                    texts.append(filled_template)\n",
        "                else:\n",
        "                    texts.append(template)\n",
        "            except:\n",
        "                texts.append(\"Financial markets showed mixed performance with varied sector rotation patterns.\")\n",
        "\n",
        "        return texts\n",
        "\n",
        "    def tokenize_dataset(self, texts):\n",
        "        print(\"Tokenizing dataset...\")\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            # Tokenize and truncate to max_length\n",
        "            tokenized = self.tokenizer(\n",
        "                examples,\n",
        "                truncation=True,\n",
        "                padding=False,\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=None\n",
        "            )\n",
        "            return tokenized\n",
        "\n",
        "        # Process in batches to avoid memory issues\n",
        "        tokenized_texts = []\n",
        "        batch_size = 1000\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            tokenized_batch = tokenize_function(batch)\n",
        "            tokenized_texts.extend(tokenized_batch['input_ids'])\n",
        "\n",
        "        # Create dataset\n",
        "        tokenized_dataset = Dataset.from_dict({\n",
        "            'input_ids': tokenized_texts\n",
        "        })\n",
        "\n",
        "        print(f\"Tokenized {len(tokenized_texts)} documents\")\n",
        "        return tokenized_dataset\n",
        "\n",
        "    def calculate_tokens_processed(self, dataset):\n",
        "        total_tokens = sum(len(item['input_ids']) for item in dataset)\n",
        "        print(f\"Total tokens in dataset: {total_tokens:,}\")\n",
        "        return total_tokens\n",
        "\n",
        "    def setup_training_args(self, output_dir=\"./financial_llama_3_2_1b\", num_epochs=1):\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=num_epochs,\n",
        "            per_device_train_batch_size=4,  # Can use larger batch size with 1B model\n",
        "            gradient_accumulation_steps=4,   # Effective batch size = 4 * 4 = 16\n",
        "            warmup_steps=500,\n",
        "            learning_rate=5e-5,              # Lower learning rate for continued pre-training\n",
        "            weight_decay=0.01,\n",
        "            logging_steps=50,\n",
        "            save_steps=1000,\n",
        "            save_total_limit=3,\n",
        "            prediction_loss_only=True,\n",
        "            fp16=True,  # Use mixed precision\n",
        "            dataloader_drop_last=True,\n",
        "            remove_unused_columns=False,\n",
        "            report_to=None,  # Disable wandb/tensorboard\n",
        "            max_grad_norm=1.0,               # Gradient clipping\n",
        "        )\n",
        "\n",
        "    def continue_pretraining(self, dataset, target_tokens=1_000_000_000):\n",
        "        print(\"Starting continued pre-training...\")\n",
        "\n",
        "        # Calculate how many epochs needed for target tokens\n",
        "        total_tokens = self.calculate_tokens_processed(dataset)\n",
        "        epochs_needed = max(1, target_tokens // total_tokens)\n",
        "\n",
        "        print(f\"Target tokens: {target_tokens:,}\")\n",
        "        print(f\"Epochs needed: {epochs_needed}\")\n",
        "\n",
        "        # Setup training\n",
        "        training_args = self.setup_training_args(num_epochs=epochs_needed)\n",
        "\n",
        "        # Data collator for language modeling\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,  # Causal LM, not masked LM\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        # Start training\n",
        "        print(\"Training started...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the fine-tuned model\n",
        "        trainer.save_model(\"./financial_llama_3_2_1b_final\")\n",
        "        self.tokenizer.save_pretrained(\"./financial_llama_3_2_1b_final\")\n",
        "        print(\"Training completed and model saved!\")\n",
        "\n",
        "    def run_benchmark_tests(self):\n",
        "        print(\"Running benchmark tests...\")\n",
        "\n",
        "        # Common sense reasoning prompts\n",
        "        benchmark_prompts = [\n",
        "            \"The capital of France is\",\n",
        "            \"2 + 2 equals\",\n",
        "            \"The largest planet in our solar system is\",\n",
        "            \"Water boils at\",\n",
        "            \"The first president of the United States was\",\n",
        "            # Financial domain prompts\n",
        "            \"A stock's P/E ratio represents\",\n",
        "            \"When interest rates rise, bond prices typically\",\n",
        "            \"EBITDA stands for\",\n",
        "            \"A bull market is characterized by\",\n",
        "            \"The Federal Reserve's primary tool for monetary policy is\"\n",
        "        ]\n",
        "\n",
        "        results = {\n",
        "            'original': [],\n",
        "            'fine_tuned': []\n",
        "        }\n",
        "\n",
        "        print(\"Testing original model...\")\n",
        "        for prompt in tqdm(benchmark_prompts):\n",
        "            response = self.generate_response(self.original_model, prompt)\n",
        "            results['original'].append({\n",
        "                'prompt': prompt,\n",
        "                'response': response\n",
        "            })\n",
        "\n",
        "        print(\"Testing fine-tuned model...\")\n",
        "        for prompt in tqdm(benchmark_prompts):\n",
        "            response = self.generate_response(self.model, prompt)\n",
        "            results['fine_tuned'].append({\n",
        "                'prompt': prompt,\n",
        "                'response': response\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_response(self, model, prompt, max_new_tokens=50):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response[len(prompt):].strip()\n",
        "\n",
        "    def evaluate_perplexity(self, test_texts):\n",
        "        print(\"Calculating perplexity...\")\n",
        "\n",
        "        def calculate_perplexity(model, texts):\n",
        "            model.eval()\n",
        "            total_loss = 0\n",
        "            total_tokens = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for text in tqdm(texts[:100], desc=\"Calculating perplexity\"):  # Sample for speed\n",
        "                    inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "                    loss = outputs.loss\n",
        "\n",
        "                    total_loss += loss.item() * inputs[\"input_ids\"].size(1)\n",
        "                    total_tokens += inputs[\"input_ids\"].size(1)\n",
        "\n",
        "            avg_loss = total_loss / total_tokens\n",
        "            perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "            return perplexity\n",
        "\n",
        "        original_ppl = calculate_perplexity(self.original_model, test_texts)\n",
        "        finetuned_ppl = calculate_perplexity(self.model, test_texts)\n",
        "\n",
        "        print(f\"Original model perplexity: {original_ppl:.2f}\")\n",
        "        print(f\"Fine-tuned model perplexity: {finetuned_ppl:.2f}\")\n",
        "\n",
        "        return original_ppl, finetuned_ppl\n",
        "\n",
        "    def analyze_results(self, benchmark_results, perplexity_results):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"BENCHMARK RESULTS ANALYSIS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Compare responses\n",
        "        for i, prompt in enumerate([r['prompt'] for r in benchmark_results['original']]):\n",
        "            print(f\"\\nPrompt: {prompt}\")\n",
        "            print(f\"Original: {benchmark_results['original'][i]['response']}\")\n",
        "            print(f\"Fine-tuned: {benchmark_results['fine_tuned'][i]['response']}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "        # Perplexity analysis\n",
        "        original_ppl, finetuned_ppl = perplexity_results\n",
        "        print(f\"\\nPERPLEXITY ANALYSIS:\")\n",
        "        print(f\"Original model: {original_ppl:.2f}\")\n",
        "        print(f\"Fine-tuned model: {finetuned_ppl:.2f}\")\n",
        "\n",
        "        if finetuned_ppl > original_ppl:\n",
        "            print(\"⚠️  POTENTIAL DEGRADATION DETECTED\")\n",
        "            print(f\"Perplexity increased by {((finetuned_ppl - original_ppl) / original_ppl * 100):.1f}%\")\n",
        "        else:\n",
        "            print(\"✅ No significant degradation detected\")\n",
        "\n",
        "        # Save results\n",
        "        results_summary = {\n",
        "            'benchmark_responses': benchmark_results,\n",
        "            'perplexity': {\n",
        "                'original': original_ppl,\n",
        "                'fine_tuned': finetuned_ppl,\n",
        "                'degradation_pct': ((finetuned_ppl - original_ppl) / original_ppl * 100)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open('pretraining_results.json', 'w') as f:\n",
        "            json.dump(results_summary, f, indent=2)\n",
        "\n",
        "        print(\"\\nResults saved to 'pretraining_results.json'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caCAgc08Oy2-"
      },
      "outputs": [],
      "source": [
        "# Additional utility functions for monitoring and analysis\n",
        "\n",
        "def plot_training_metrics(log_file=\"./financial_llama_3_2_1b/trainer_state.json\"):\n",
        "    try:\n",
        "        with open(log_file, 'r') as f:\n",
        "            training_state = json.load(f)\n",
        "\n",
        "        # Extract loss values\n",
        "        log_history = training_state.get('log_history', [])\n",
        "        train_losses = [entry['train_loss'] for entry in log_history if 'train_loss' in entry]\n",
        "        steps = [entry['step'] for entry in log_history if 'train_loss' in entry]\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(steps, train_losses)\n",
        "        plt.title('Training Loss Over Time')\n",
        "        plt.xlabel('Steps')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "        plt.savefig('training_loss.png')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Training metrics plotted and saved as 'training_loss.png'\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Training log file not found. Run training first.\")\n",
        "\n",
        "def compare_model_outputs(original_model, finetuned_model, tokenizer, prompts):\n",
        "    comparisons = []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        # Generate from both models\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            orig_outputs = original_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "            ft_outputs = finetuned_model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
        "\n",
        "        orig_text = tokenizer.decode(orig_outputs[0], skip_special_tokens=True)\n",
        "        ft_text = tokenizer.decode(ft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        comparisons.append({\n",
        "            'prompt': prompt,\n",
        "            'original': orig_text[len(prompt):],\n",
        "            'fine_tuned': ft_text[len(prompt):],\n",
        "        })\n",
        "\n",
        "    return comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b894a8e6ef7a4c1b88436b6a618bed61",
            "0c2a9f49e86f4341a92a9fb4446665ae",
            "e1f1fdf60fc241abb9462b360d3ddeff",
            "1c15d353a0e441afb5dbdb4c0b267b61",
            "b282e6e8d0784dc58b401540cc1cf8f6",
            "a9c36d65eba04868ab97b0a21e038c35",
            "acf3fe8338d64d7ea46a29e9475483a4",
            "04993def425c4289986cad57cd16b4f9",
            "df01adc6971249179f2689ccc4dcac23",
            "959a5bd114a1477ea18b7b2eb6eb6034",
            "8b79572fff704d48b8211670d7ee9fb5",
            "e6709c40fe6a4401a81c8ed936bb0727",
            "ee7dfb47cc9342a88543178ef23d580e",
            "0b1d674da1964c4babc25e691cc07254",
            "e06cbf3ad0494905a96af3cb9408a63d",
            "105746d6109f4a51b2c0122fa87985c5",
            "0fbafaea6c5a47098c757d608c0a4f2a",
            "a5428fd3c1534df582d79da3524e309b",
            "4546e78ecfd7439b9ecd6ac2f737b042",
            "e79e6b8929144768b73bb13436232128",
            "db736e21700248a2bc101cffa89ccbfa",
            "19d2d0c6af0c4edeb983d570594d735b",
            "f66eb11e28c74412b395547499bea909",
            "e987d59b51024411b93b3ebe13b7195a",
            "63192803adfb45bd9c0e3cdb7da30897",
            "dd6f3393ca6c42b58e26f811a080f091",
            "f7fcd47ed5454bf0a7db0eea17a1a104",
            "715fa9f223f54f00a02813484d8fa480",
            "b69aeb52a81147448bb8d46cd68f3a13",
            "4eaed705ebd24de2a09529f8d615b8ec",
            "14b876c74f5546e2a6ac3d7537a2e206",
            "d3605d009abb476d8941cff5bd2210df",
            "0865656e27eb4cbc8ae7bf3567cb2bc3"
          ]
        },
        "id": "iRnD6CETLolt",
        "outputId": "e144286e-b968-4738-9539-6138c4dc7f66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Meta LLaMA 3.2-1B Financial Pre-training Pipeline\n",
            "==================================================\n",
            "⚠️  Warning: HF_TOKEN environment variable not set.\n",
            "You may need to set your Hugging Face token to access the model:\n",
            "export HF_TOKEN=your_token_here\n",
            "Or login using: huggingface-cli login\n",
            "\n",
            "Loading Meta LLaMA 3.2-1B tokenizer and model...\n",
            "Model loaded: meta-llama/Llama-3.2-1B\n",
            "Model parameters: 1,235,814,400\n",
            "Model size: ~1.2B parameters\n",
            "Loading financial documents dataset...\n",
            "Trying to load Financial Twitter sentiment data...\n",
            "❌ Failed to load zeroshot/twitter-financial-news-sentiment: Invalid pattern: '**' can only be an entire path component\n",
            "Trying to load Financial PhraseBank...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b894a8e6ef7a4c1b88436b6a618bed61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.04k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6709c40fe6a4401a81c8ed936bb0727",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.88k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Failed to load financial_phrasebank: Config name is missing.\n",
            "Please pick one among the available configs: ['sentences_allagree', 'sentences_75agree', 'sentences_66agree', 'sentences_50agree']\n",
            "Example of usage:\n",
            "\t`load_dataset('financial_phrasebank', 'sentences_allagree')`\n",
            "Trying to load Finance Alpaca dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f66eb11e28c74412b395547499bea909",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Failed to load gbharti/finance-alpaca: Invalid pattern: '**' can only be an entire path component\n",
            "⚠️  No datasets loaded successfully. Creating synthetic financial text...\n",
            "Creating synthetic financial documents...\n",
            "Total documents after cleaning: 5000\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 15.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized 5000 documents\n",
            "Starting continued pre-training...\n",
            "Total tokens in dataset: 111,528\n",
            "Target tokens: 1,000,000,000\n",
            "Epochs needed: 8966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Error occurred: API key must be 40 characters long, yours was 4\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "API key must be 40 characters long, yours was 4",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-39c93e3607c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;31m# Additional utility functions for monitoring and analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-39c93e3607c9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Continue pre-training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinue_pretraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000_000_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# Run benchmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-39c93e3607c9>\u001b[0m in \u001b[0;36mcontinue_pretraining\u001b[0;34m(self, dataset, target_tokens)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training started...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Save the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2462\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2463\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2464\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    859\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should never get here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_telemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m         \u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_warnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_run_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36mmaybe_login\u001b[0;34m(self, init_settings)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         wandb_login._login(\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey_is_pre_configured\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtry_save_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_global_anonymous_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mtry_save_api_key\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mapikey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mapikey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteNetrcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mwrite_key\u001b[0;34m(settings, key, api)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;34m\"API key must be 40 characters long, yours was {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: API key must be 40 characters long, yours was 4"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Meta LLaMA 3.2-1B Financial Pre-training Pipeline\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Check for HuggingFace token\n",
        "    import os\n",
        "    if not os.getenv('HF_TOKEN'):\n",
        "        print(\"⚠️  Warning: HF_TOKEN environment variable not set.\")\n",
        "        print(\"You may need to set your Hugging Face token to access the model:\")\n",
        "        print(\"export HF_TOKEN=your_token_here\")\n",
        "        print(\"Or login using: huggingface-cli login\")\n",
        "        print()\n",
        "\n",
        "    try:\n",
        "        # Initialize pipeline\n",
        "        pipeline = FinancialPretrainingPipeline()\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        pipeline.load_model_and_tokenizer()\n",
        "\n",
        "        # Load financial dataset\n",
        "        financial_texts = pipeline.load_financial_dataset()\n",
        "\n",
        "        # Tokenize dataset\n",
        "        tokenized_dataset = pipeline.tokenize_dataset(financial_texts)\n",
        "\n",
        "        # Continue pre-training\n",
        "        pipeline.continue_pretraining(tokenized_dataset, target_tokens=1_000_000_000)\n",
        "\n",
        "        # Run benchmarks\n",
        "        benchmark_results = pipeline.run_benchmark_tests()\n",
        "\n",
        "        # Calculate perplexity (using subset of financial texts as test)\n",
        "        test_texts = financial_texts[:1000]  # Use first 1000 for testing\n",
        "        perplexity_results = pipeline.evaluate_perplexity(test_texts)\n",
        "\n",
        "        # Analyze results\n",
        "        pipeline.analyze_results(benchmark_results, perplexity_results)\n",
        "\n",
        "        print(\"\\nPipeline completed successfully! 🎉\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error occurred: {str(e)}\")\n",
        "        if \"401\" in str(e) or \"access\" in str(e).lower():\n",
        "            print(\"\\n🔐 This might be an authentication issue.\")\n",
        "            print(\"Make sure you have:\")\n",
        "            print(\"1. Requested access to meta-llama/Llama-3.2-1B on Hugging Face\")\n",
        "            print(\"2. Set your HF_TOKEN environment variable\")\n",
        "            print(\"3. Or run: huggingface-cli login\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_WdWdrnMa-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04993def425c4289986cad57cd16b4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0865656e27eb4cbc8ae7bf3567cb2bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b1d674da1964c4babc25e691cc07254": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4546e78ecfd7439b9ecd6ac2f737b042",
            "max": 8878,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e79e6b8929144768b73bb13436232128",
            "value": 8878
          }
        },
        "0c2a9f49e86f4341a92a9fb4446665ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c36d65eba04868ab97b0a21e038c35",
            "placeholder": "​",
            "style": "IPY_MODEL_acf3fe8338d64d7ea46a29e9475483a4",
            "value": "Downloading builder script: 100%"
          }
        },
        "0fbafaea6c5a47098c757d608c0a4f2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105746d6109f4a51b2c0122fa87985c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14b876c74f5546e2a6ac3d7537a2e206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19d2d0c6af0c4edeb983d570594d735b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c15d353a0e441afb5dbdb4c0b267b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_959a5bd114a1477ea18b7b2eb6eb6034",
            "placeholder": "​",
            "style": "IPY_MODEL_8b79572fff704d48b8211670d7ee9fb5",
            "value": " 6.04k/6.04k [00:00&lt;00:00, 470kB/s]"
          }
        },
        "4546e78ecfd7439b9ecd6ac2f737b042": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eaed705ebd24de2a09529f8d615b8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63192803adfb45bd9c0e3cdb7da30897": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eaed705ebd24de2a09529f8d615b8ec",
            "max": 831,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14b876c74f5546e2a6ac3d7537a2e206",
            "value": 831
          }
        },
        "715fa9f223f54f00a02813484d8fa480": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b79572fff704d48b8211670d7ee9fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "959a5bd114a1477ea18b7b2eb6eb6034": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5428fd3c1534df582d79da3524e309b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c36d65eba04868ab97b0a21e038c35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf3fe8338d64d7ea46a29e9475483a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b282e6e8d0784dc58b401540cc1cf8f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b69aeb52a81147448bb8d46cd68f3a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b894a8e6ef7a4c1b88436b6a618bed61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c2a9f49e86f4341a92a9fb4446665ae",
              "IPY_MODEL_e1f1fdf60fc241abb9462b360d3ddeff",
              "IPY_MODEL_1c15d353a0e441afb5dbdb4c0b267b61"
            ],
            "layout": "IPY_MODEL_b282e6e8d0784dc58b401540cc1cf8f6"
          }
        },
        "d3605d009abb476d8941cff5bd2210df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db736e21700248a2bc101cffa89ccbfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd6f3393ca6c42b58e26f811a080f091": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3605d009abb476d8941cff5bd2210df",
            "placeholder": "​",
            "style": "IPY_MODEL_0865656e27eb4cbc8ae7bf3567cb2bc3",
            "value": " 831/831 [00:00&lt;00:00, 74.6kB/s]"
          }
        },
        "df01adc6971249179f2689ccc4dcac23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e06cbf3ad0494905a96af3cb9408a63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db736e21700248a2bc101cffa89ccbfa",
            "placeholder": "​",
            "style": "IPY_MODEL_19d2d0c6af0c4edeb983d570594d735b",
            "value": " 8.88k/8.88k [00:00&lt;00:00, 815kB/s]"
          }
        },
        "e1f1fdf60fc241abb9462b360d3ddeff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04993def425c4289986cad57cd16b4f9",
            "max": 6036,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df01adc6971249179f2689ccc4dcac23",
            "value": 6036
          }
        },
        "e6709c40fe6a4401a81c8ed936bb0727": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee7dfb47cc9342a88543178ef23d580e",
              "IPY_MODEL_0b1d674da1964c4babc25e691cc07254",
              "IPY_MODEL_e06cbf3ad0494905a96af3cb9408a63d"
            ],
            "layout": "IPY_MODEL_105746d6109f4a51b2c0122fa87985c5"
          }
        },
        "e79e6b8929144768b73bb13436232128": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e987d59b51024411b93b3ebe13b7195a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715fa9f223f54f00a02813484d8fa480",
            "placeholder": "​",
            "style": "IPY_MODEL_b69aeb52a81147448bb8d46cd68f3a13",
            "value": "Downloading readme: 100%"
          }
        },
        "ee7dfb47cc9342a88543178ef23d580e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fbafaea6c5a47098c757d608c0a4f2a",
            "placeholder": "​",
            "style": "IPY_MODEL_a5428fd3c1534df582d79da3524e309b",
            "value": "Downloading readme: 100%"
          }
        },
        "f66eb11e28c74412b395547499bea909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e987d59b51024411b93b3ebe13b7195a",
              "IPY_MODEL_63192803adfb45bd9c0e3cdb7da30897",
              "IPY_MODEL_dd6f3393ca6c42b58e26f811a080f091"
            ],
            "layout": "IPY_MODEL_f7fcd47ed5454bf0a7db0eea17a1a104"
          }
        },
        "f7fcd47ed5454bf0a7db0eea17a1a104": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
