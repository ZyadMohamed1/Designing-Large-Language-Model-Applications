{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01540a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55383bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicalFallaciesEvaluator:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        os.environ['OPENAI_API_SECRET_KEY'] = api_key\n",
    "        \n",
    "        # Models to evaluate\n",
    "        self.openai_models = {\n",
    "            'gpt-4o': 'gpt-4o',\n",
    "            'gpt-4o-mini': 'gpt-4o-mini', \n",
    "            'o1-preview': 'o1-preview',\n",
    "            'o1-mini': 'o1-mini',\n",
    "        }\n",
    "        \n",
    "        # Open source models for comparison\n",
    "        self.open_source_models = {\n",
    "            'llama-2-70b': 'meta-llama/Llama-2-70b-chat-hf',\n",
    "            'llama-3-70b': 'meta-llama/Meta-Llama-3-70B-Instruct',\n",
    "        }\n",
    "        \n",
    "        # Logical fallacy tasks\n",
    "        self.benchmark_tasks = [\n",
    "            'bigbench_formal_fallacies_syllogisms_negation',\n",
    "            'bigbench_logical_deduction_three_objects',\n",
    "            'bigbench_logical_deduction_five_objects',\n",
    "            'bigbench_logical_deduction_seven_objects',\n",
    "        ]\n",
    "        \n",
    "        # Custom fallacy tasks we'll create\n",
    "        self.custom_tasks = [\n",
    "            'custom_ad_hominem',\n",
    "            'custom_straw_man',\n",
    "            'custom_false_dichotomy',\n",
    "            'custom_circular_reasoning',\n",
    "        ]\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def run_evaluation(self, model_name, model_id, tasks):\n",
    "        results = {}\n",
    "        \n",
    "        for task in tasks:\n",
    "            print(f\"Evaluating {model_name} on {task}...\")\n",
    "            \n",
    "            try:\n",
    "                # Construct the command\n",
    "                cmd = [\n",
    "                    'python', 'main.py',\n",
    "                    'lm_eval',\n",
    "                    '--model', 'openai-completions',\n",
    "                    '--model_args', f'model={model_id}',\n",
    "                    '--tasks', task,\n",
    "                    '--output_path', f'results/{model_name}_{task}.json',\n",
    "                    '--log_samples'\n",
    "                ]\n",
    "                \n",
    "                # Run the evaluation\n",
    "                result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    # Parse results\n",
    "                    with open(f'results/{model_name}_{task}.json', 'r') as f:\n",
    "                        task_results = json.load(f)\n",
    "                    \n",
    "                    results[task] = {\n",
    "                        'accuracy': task_results.get('results', {}).get(task, {}).get('acc', 0),\n",
    "                        'acc_norm': task_results.get('results', {}).get(task, {}).get('acc_norm', 0),\n",
    "                        'samples': task_results.get('samples', [])\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"Error evaluating {model_name} on {task}: {result.stderr}\")\n",
    "                    results[task] = {'accuracy': 0, 'acc_norm': 0, 'error': result.stderr}\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Exception evaluating {model_name} on {task}: {e}\")\n",
    "                results[task] = {'accuracy': 0, 'acc_norm': 0, 'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_custom_fallacy_tasks(self):\n",
    "        \n",
    "        # Ad Hominem fallacy examples\n",
    "        ad_hominem_examples = [\n",
    "            {\n",
    "                \"input\": \"John argues that we should increase funding for education. But John dropped out of high school, so his argument is invalid.\",\n",
    "                \"target\": \"ad_hominem\",\n",
    "                \"explanation\": \"Attacks the person making the argument rather than the argument itself\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"Sarah says we need stricter environmental regulations. But she drives an SUV, so she's clearly wrong.\",\n",
    "                \"target\": \"ad_hominem\", \n",
    "                \"explanation\": \"Dismisses the argument based on perceived hypocrisy rather than merit\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Straw Man fallacy examples\n",
    "        straw_man_examples = [\n",
    "            {\n",
    "                \"input\": \"Person A: 'We should have some gun control measures.' Person B: 'So you want to take away all guns and leave us defenseless!'\",\n",
    "                \"target\": \"straw_man\",\n",
    "                \"explanation\": \"Misrepresents the original argument to make it easier to attack\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create task files (simplified - in practice you'd create proper JSONL files)\n",
    "        custom_tasks = {\n",
    "            'ad_hominem': ad_hominem_examples,\n",
    "            'straw_man': straw_man_examples,\n",
    "        }\n",
    "        \n",
    "        return custom_tasks\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \n",
    "        # Create results directory\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        \n",
    "        # Evaluate OpenAI models\n",
    "        for model_name, model_id in self.openai_models.items():\n",
    "            print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "            self.results[model_name] = self.run_evaluation(\n",
    "                model_name, model_id, self.benchmark_tasks\n",
    "            )\n",
    "        \n",
    "        # Note: For open source models, you'd need different evaluation setup\n",
    "        # This is a placeholder for the structure\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \n",
    "        # Convert results to DataFrame for easier analysis\n",
    "        data = []\n",
    "        for model, tasks in self.results.items():\n",
    "            for task, metrics in tasks.items():\n",
    "                if 'error' not in metrics:\n",
    "                    data.append({\n",
    "                        'model': model,\n",
    "                        'task': task,\n",
    "                        'accuracy': metrics['accuracy'],\n",
    "                        'acc_norm': metrics['acc_norm']\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Calculate average performance per model\n",
    "        model_avg = df.groupby('model').agg({\n",
    "            'accuracy': 'mean',\n",
    "            'acc_norm': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        print(\"\\n=== Model Performance Summary ===\")\n",
    "        print(model_avg)\n",
    "        \n",
    "        # Calculate performance per task\n",
    "        task_performance = df.pivot_table(\n",
    "            index='task', \n",
    "            columns='model', \n",
    "            values='accuracy'\n",
    "        ).round(3)\n",
    "        \n",
    "        print(\"\\n=== Task Performance Breakdown ===\")\n",
    "        print(task_performance)\n",
    "        \n",
    "        return df, model_avg, task_performance\n",
    "    \n",
    "    def create_visualizations(self, df):\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Overall model comparison\n",
    "        model_avg = df.groupby('model')['accuracy'].mean().sort_values(ascending=True)\n",
    "        axes[0,0].barh(model_avg.index, model_avg.values)\n",
    "        axes[0,0].set_title('Average Accuracy by Model')\n",
    "        axes[0,0].set_xlabel('Accuracy')\n",
    "        \n",
    "        # 2. Performance by task\n",
    "        task_pivot = df.pivot_table(index='model', columns='task', values='accuracy')\n",
    "        sns.heatmap(task_pivot, annot=True, cmap='YlOrRd', ax=axes[0,1])\n",
    "        axes[0,1].set_title('Accuracy Heatmap by Model and Task')\n",
    "        \n",
    "        # 3. Model comparison boxplot\n",
    "        sns.boxplot(data=df, x='model', y='accuracy', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Accuracy Distribution by Model')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 4. Task difficulty ranking\n",
    "        task_avg = df.groupby('task')['accuracy'].mean().sort_values(ascending=True)\n",
    "        axes[1,1].barh(task_avg.index, task_avg.values)\n",
    "        axes[1,1].set_title('Average Task Difficulty (Lower = Harder)')\n",
    "        axes[1,1].set_xlabel('Average Accuracy Across Models')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('logical_fallacies_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Initialize evaluator (you'll need to provide your OpenAI API key)\n",
    "    api_key = os.getenv('OPENAI_API_SECRET_KEY')\n",
    "    if not api_key:\n",
    "        print(\"Please set your OPENAI_API_SECRET_KEY environment variable\")\n",
    "        return\n",
    "    \n",
    "    evaluator = LogicalFallaciesEvaluator(api_key)\n",
    "    \n",
    "    # Run evaluations\n",
    "    print(\"Starting model evaluations...\")\n",
    "    results = evaluator.evaluate_all_models()\n",
    "    \n",
    "    # Analyze results\n",
    "    df, model_avg, task_performance = evaluator.analyze_results()\n",
    "    \n",
    "    # Create visualizations\n",
    "    evaluator.create_visualizations(df)\n",
    "    \n",
    "\n",
    "    print(\"\\nEvaluation complete! Check 'logical_fallacies_comparison.png' and 'logical_fallacies_report.md'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeecaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
