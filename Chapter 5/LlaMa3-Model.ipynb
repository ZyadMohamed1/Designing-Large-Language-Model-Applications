{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c500ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import PyPDF2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c545ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLlamaQA:\n",
    "    def __init__(self):\n",
    "        self.documents = \"\"\n",
    "        self.ollama_url = \"http://localhost:11434\"\n",
    "    \n",
    "    # Step 3: Load PDF files\n",
    "    def load_pdf(self, file_path):\n",
    "        text = \"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "    # Step 4: Load TXT files\n",
    "    def load_txt(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    # Step 5: Clean and extract important text\n",
    "    def clean_text(self, text):\n",
    "        # Remove extra whitespace and empty lines\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        clean_text = '\\n'.join(lines)\n",
    "        \n",
    "        # Keep only first 4000 characters to fit in context\n",
    "        if len(clean_text) > 4000:\n",
    "            clean_text = clean_text[:4000] + \"...\"\n",
    "        \n",
    "        return clean_text\n",
    "    \n",
    "    # Step 6: Upload multiple files\n",
    "    def upload_files(self, file_paths):\n",
    "        all_text = \"\"\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            print(f\"Loading: {file_path}\")\n",
    "            \n",
    "            if file_path.endswith('.pdf'):\n",
    "                text = self.load_pdf(file_path)\n",
    "            elif file_path.endswith('.txt'):\n",
    "                text = self.load_txt(file_path)\n",
    "            else:\n",
    "                print(f\"Unsupported file type: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Clean the text\n",
    "            clean_text = self.clean_text(text)\n",
    "            all_text += f\"\\n\\n=== {os.path.basename(file_path)} ===\\n{clean_text}\"\n",
    "        \n",
    "        self.documents = all_text\n",
    "        print(f\"✅ Loaded {len(file_paths)} files\")\n",
    "        return True\n",
    "    \n",
    "     # Step 7: Ask questions to Llama 3\n",
    "    def ask_question(self, question):\n",
    "        if not self.documents:\n",
    "            return \"No documents loaded!\"\n",
    "        \n",
    "        # Create simple prompt\n",
    "        prompt = f\"\"\"Based on the following documents, answer this question: {question}\n",
    "\n",
    "Documents:\n",
    "{self.documents}\n",
    "\n",
    "Answer based only on the information in the documents:\"\"\"\n",
    "        \n",
    "        # Send to Llama 3\n",
    "        try:\n",
    "            # First check what models are available\n",
    "            models_response = requests.get(f\"{self.ollama_url}/api/tags\")\n",
    "            if models_response.status_code == 200:\n",
    "                available_models = [m['name'] for m in models_response.json()['models']]\n",
    "                print(f\"Available models: {available_models}\")\n",
    "                \n",
    "                # Try to find llama3 variant\n",
    "                model_name = \"llama3\"\n",
    "                if \"llama3\" not in available_models:\n",
    "                    for model in available_models:\n",
    "                        if \"llama\" in model.lower():\n",
    "                            model_name = model\n",
    "                            print(f\"Using model: {model_name}\")\n",
    "                            break\n",
    "                    else:\n",
    "                        return \"❌ No Llama model found. Run: ollama pull llama3\"\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.ollama_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()['response']\n",
    "            else:\n",
    "                return f\"Error: {response.status_code}\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "109ee97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create the system\n",
    "    qa = SimpleLlamaQA()\n",
    "    \n",
    "    # Example files (replace with your file paths)\n",
    "    files = [\n",
    "        \"test.txt\"\n",
    "    ]\n",
    "    \n",
    "    # Upload files\n",
    "    qa.upload_files(files)\n",
    "    \n",
    "    # Ask questions\n",
    "    questions = [\n",
    "        \"What is the name of the person?\",\n",
    "        \"What does this person have?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\nQ: {question}\")\n",
    "        answer = qa.ask_question(question)\n",
    "        print(f\"A: {answer}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fe5d4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: test.txt\n",
      "✅ Loaded 1 files\n",
      "\n",
      "Q: What is the name of the person?\n",
      "Available models: ['llama3:latest']\n",
      "Using model: llama3:latest\n",
      "A: According to the document \"test.txt\", the name of the person is Mike.\n",
      "--------------------------------------------------\n",
      "\n",
      "Q: What does this person have?\n",
      "Available models: ['llama3:latest']\n",
      "Using model: llama3:latest\n",
      "A: According to the document \"test.txt\", Mike has a big brain.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create QA system\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
