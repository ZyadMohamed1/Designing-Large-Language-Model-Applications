{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ae7751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba2e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import guidance\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4144c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Make sure you have installed the required packages:\n",
      "pip install guidance transformers torch beautifulsoup4 requests accelerate\n",
      "\n",
      "Note: You'll need access to Llama3 model (requires Hugging Face authentication)\n",
      "Run: huggingface-cli login\n",
      "\n",
      "================================================================================\n",
      "Setting up Llama3 model...\n",
      "Loading tokenizer and model...\n",
      "Error in main execution: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "401 Client Error. (Request ID: Root=1-6844ae4c-3c884eaa71e906ff5edfe732;3a847a96-d05b-4220-9058-b7c6284cefe6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_http.py\", line 409, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\hub.py\", line 470, in cached_files\n",
      "    hf_hub_download(\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 1008, in hf_hub_download\n",
      "    return _hf_hub_download_to_cache_dir(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 1115, in _hf_hub_download_to_cache_dir\n",
      "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 1645, in _raise_on_head_call_error\n",
      "    raise head_call_error\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 1533, in _get_metadata_or_catch_error\n",
      "    metadata = get_hf_file_metadata(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 1450, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "        ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 286, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py\", line 310, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\utils\\_http.py\", line 426, in hf_raise_for_status\n",
      "    raise _format(GatedRepoError, message, response) from e\n",
      "huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-6844ae4c-3c884eaa71e906ff5edfe732;3a847a96-d05b-4220-9058-b7c6284cefe6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Temp\\ipykernel_15556\\3534257956.py\", line 139, in main\n",
      "    model, tokenizer = setup_model()\n",
      "                       ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Temp\\ipykernel_15556\\3534257956.py\", line 10, in setup_model\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 970, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1153, in from_pretrained\n",
      "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\", line 595, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py\", line 654, in _get_config_dict\n",
      "    resolved_config_file = cached_file(\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\hub.py\", line 312, in cached_file\n",
      "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\hub.py\", line 533, in cached_files\n",
      "    raise OSError(\n",
      "OSError: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "401 Client Error. (Request ID: Root=1-6844ae4c-3c884eaa71e906ff5edfe732;3a847a96-d05b-4220-9058-b7c6284cefe6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def setup_model():\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    \n",
    "    print(\"Loading tokenizer and model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # Set up guidance with the model\n",
    "    Llama3 = guidance.models.Transformers(model, tokenizer=tokenizer)\n",
    "    return model, tokenizer\n",
    "\n",
    "def fetch_wikipedia_content(url):\n",
    "    print(\"Fetching Wikipedia content...\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract main content paragraphs\n",
    "    content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "    paragraphs = content_div.find_all('p')\n",
    "    \n",
    "    # Clean and combine paragraphs\n",
    "    text = \"\"\n",
    "    for p in paragraphs[:10]:  # Limit to first 10 paragraphs for processing\n",
    "        clean_text = re.sub(r'\\[.*?\\]', '', p.get_text())  # Remove citations\n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()  # Clean whitespace\n",
    "        if len(clean_text) > 20:  # Only include substantial paragraphs\n",
    "            text += clean_text + \" \"\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def create_ner_template():\n",
    "    ner_template = guidance('''\n",
    "{{#system~}}\n",
    "You are an expert at Named Entity Recognition (NER). Your task is to identify and tag named entities in text.\n",
    "\n",
    "Tag the following entity types:\n",
    "- PER: Person names\n",
    "- ORG: Organizations, companies, institutions\n",
    "- LOC: Locations, places, countries, cities\n",
    "- NUM: Numbers (including years, quantities)\n",
    "- DATE: Dates and time expressions\n",
    "- EVENT: Events, competitions, games\n",
    "\n",
    "Format your output exactly like this example:\n",
    "Input: \"Padma sold 23 umbrellas in Guatemala\"\n",
    "Output: *Padma: PER* *sold:* *23: NUM* *umbrellas:* *in:* *Guatemala: LOC*\n",
    "\n",
    "Rules:\n",
    "1. Put entity tags after colons (word: TAG)\n",
    "2. Surround tagged words with asterisks (*word: TAG*)\n",
    "3. Leave untagged words as *word:* (with colon but no tag)\n",
    "4. Tag ALL words in the sentence\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Please perform Named Entity Recognition on this text:\n",
    "\"{{text}}\"\n",
    "\n",
    "Provide the tagged output in the specified format:\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "{{gen 'tagged_output' max_tokens=500 temperature=0.1}}\n",
    "{{~/assistant}}\n",
    "''')\n",
    "    return ner_template\n",
    "\n",
    "def process_text_chunks(text, chunk_size=200):\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        if len(current_chunk + sentence) < chunk_size:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def run_ner_analysis(text_chunks, ner_template, max_chunks=5):\n",
    "    \"\"\"Run NER analysis on text chunks\"\"\"\n",
    "    print(f\"Processing {min(len(text_chunks), max_chunks)} text chunks...\")\n",
    "    results = []\n",
    "    \n",
    "    for i, chunk in enumerate(text_chunks[:max_chunks]):\n",
    "        print(f\"Processing chunk {i+1}/{min(len(text_chunks), max_chunks)}\")\n",
    "        print(f\"Chunk: {chunk[:100]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Run the guidance template\n",
    "            result = ner_template(text=chunk)\n",
    "            tagged_output = result['tagged_output'].strip()\n",
    "            \n",
    "            results.append({\n",
    "                'original': chunk,\n",
    "                'tagged': tagged_output\n",
    "            })\n",
    "            \n",
    "            print(f\"Tagged: {tagged_output[:100]}...\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {e}\")\n",
    "            results.append({\n",
    "                'original': chunk,\n",
    "                'tagged': f\"Error: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \n",
    "    try:\n",
    "        # Setup\n",
    "        print(\"Setting up Llama3 model...\")\n",
    "        model, tokenizer = setup_model()\n",
    "        \n",
    "        # Fetch Wikipedia content\n",
    "        wiki_url = \"https://en.wikipedia.org/wiki/Summer_Olympic_Games\"\n",
    "        text = fetch_wikipedia_content(wiki_url)\n",
    "        print(f\"Fetched {len(text)} characters from Wikipedia\")\n",
    "        \n",
    "        # Create NER template\n",
    "        ner_template = create_ner_template()\n",
    "        \n",
    "        # Process text\n",
    "        text_chunks = process_text_chunks(text)\n",
    "        print(f\"Split text into {len(text_chunks)} chunks\")\n",
    "        \n",
    "        # Run NER analysis\n",
    "        results = run_ner_analysis(text_chunks, ner_template)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NAMED ENTITY RECOGNITION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nCHUNK {i}:\")\n",
    "            print(f\"Original: {result['original']}\")\n",
    "            print(f\"Tagged:   {result['tagged']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Save results to file\n",
    "        with open('ner_results.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Named Entity Recognition Results\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            for i, result in enumerate(results, 1):\n",
    "                f.write(f\"CHUNK {i}:\\n\")\n",
    "                f.write(f\"Original: {result['original']}\\n\")\n",
    "                f.write(f\"Tagged: {result['tagged']}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        print(f\"\\nResults saved to 'ner_results.txt'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages first\n",
    "    print(\"Make sure you have installed the required packages:\")\n",
    "    print(\"pip install guidance transformers torch beautifulsoup4 requests accelerate\")\n",
    "    print(\"\\nNote: You'll need access to Llama3 model (requires Hugging Face authentication)\")\n",
    "    print(\"Run: huggingface-cli login\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866347f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
