{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8fd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "from typing import List, Dict, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a38388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAAssistantWithRankVicuna:\n",
    "    def __init__(self, documents: List[str]):\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Initialize embedding model for initial retrieval\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize cross-encoder for reranking (RankVicuna-style)\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        \n",
    "        # Initialize QA model\n",
    "        self.qa_model = pipeline('question-answering', \n",
    "                               model='distilbert-base-cased-distilled-squad',\n",
    "                               return_scores=True)\n",
    "        \n",
    "        # Build FAISS index for fast similarity search\n",
    "        self.build_index()\n",
    "    \n",
    "    def build_index(self):\n",
    "        print(\"Building document index...\")\n",
    "        embeddings = self.embedding_model.encode(self.documents)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"Index built with {len(self.documents)} documents\")\n",
    "    \n",
    "    def retrieve_initial_candidates(self, query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        candidates = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.documents):  # Valid index\n",
    "                candidates.append((self.documents[idx], float(score)))\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def rerank_with_rankvicuna(self, query: str, candidates: List[Tuple[str, float]], \n",
    "                              top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        # Prepare query-document pairs for reranking\n",
    "        query_doc_pairs = [[query, doc] for doc, _ in candidates]\n",
    "        \n",
    "        # Get reranking scores using CrossEncoder\n",
    "        rerank_scores = self.reranker.predict(query_doc_pairs)\n",
    "        \n",
    "        # Combine documents with new scores\n",
    "        reranked = [(candidates[i][0], float(score)) \n",
    "                   for i, score in enumerate(rerank_scores)]\n",
    "        \n",
    "        # Sort by reranking score (descending)\n",
    "        reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return reranked[:top_k]\n",
    "    \n",
    "    def generate_answer_default_prompt(self, query: str, context: str) -> Dict:\n",
    "        try:\n",
    "            result = self.qa_model(question=query, context=context)\n",
    "            return {\n",
    "                'answer': result['answer'],\n",
    "                'confidence': result['score'],\n",
    "                'prompt_type': 'default'\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'answer': \"I couldn't find a relevant answer in the provided context.\",\n",
    "                'confidence': 0.0,\n",
    "                'prompt_type': 'default'\n",
    "            }\n",
    "    \n",
    "    def generate_answer_custom_prompt(self, query: str, context: str) -> Dict:\n",
    "        # Custom prompt template - more structured\n",
    "        custom_context = f\"\"\"\n",
    "        Context Information:\n",
    "        {context}\n",
    "        \n",
    "        Instructions: Based on the above context, provide a comprehensive and accurate answer to the question below. \n",
    "        If the context doesn't contain enough information, clearly state what information is missing.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # For this example, we'll use the same QA model but with modified context\n",
    "            # In practice, you might use a different model that accepts custom prompts\n",
    "            result = self.qa_model(question=query, context=custom_context)\n",
    "            return {\n",
    "                'answer': result['answer'],\n",
    "                'confidence': result['score'],\n",
    "                'prompt_type': 'custom'\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'answer': \"I couldn't find a relevant answer in the provided context.\",\n",
    "                'confidence': 0.0,\n",
    "                'prompt_type': 'custom'\n",
    "            }\n",
    "    \n",
    "    def answer_question(self, query: str, use_custom_prompt: bool = False) -> Dict:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Initial retrieval\n",
    "        print(f\"Retrieving candidates for: '{query}'\")\n",
    "        candidates = self.retrieve_initial_candidates(query, k=10)\n",
    "        \n",
    "        if not candidates:\n",
    "            return {\n",
    "                'answer': \"No relevant documents found.\",\n",
    "                'confidence': 0.0,\n",
    "                'retrieval_time': time.time() - start_time,\n",
    "                'documents_used': 0\n",
    "            }\n",
    "        \n",
    "        # Step 2: Reranking with RankVicuna\n",
    "        print(\"Reranking candidates...\")\n",
    "        reranked_docs = self.rerank_with_rankvicuna(query, candidates, top_k=3)\n",
    "        \n",
    "        # Step 3: Combine top documents for context\n",
    "        combined_context = \"\\n\\n\".join([doc for doc, _ in reranked_docs])\n",
    "        \n",
    "        # Step 4: Generate answer with chosen prompt template\n",
    "        if use_custom_prompt:\n",
    "            result = self.generate_answer_custom_prompt(query, combined_context)\n",
    "        else:\n",
    "            result = self.generate_answer_default_prompt(query, combined_context)\n",
    "        \n",
    "        # Add metadata\n",
    "        result.update({\n",
    "            'retrieval_time': time.time() - start_time,\n",
    "            'documents_used': len(reranked_docs),\n",
    "            'reranking_scores': [score for _, score in reranked_docs],\n",
    "            'top_documents': [doc[:100] + \"...\" for doc, _ in reranked_docs]\n",
    "        })\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aed9857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and performance comparison\n",
    "def main():\n",
    "    # Sample documents (replace with your dataset)\n",
    "    sample_documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991.\",\n",
    "        \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "        \"Natural language processing (NLP) is a branch of AI that helps computers understand, interpret and manipulate human language.\",\n",
    "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "        \"Computer vision is a field of AI that trains computers to interpret and understand visual information from the world.\",\n",
    "        \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by performing actions in an environment.\",\n",
    "        \"Data science combines statistics, mathematics, and computer science to analyze and interpret complex data.\",\n",
    "        \"Cloud computing provides on-demand access to computing resources over the internet without direct active management.\",\n",
    "        \"Cybersecurity involves protecting computer systems, networks, and data from digital attacks and unauthorized access.\",\n",
    "        \"Blockchain is a distributed ledger technology that maintains a continuously growing list of records linked using cryptography.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize QA Assistant\n",
    "    qa_assistant = QAAssistantWithRankVicuna(sample_documents)\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"What is machine learning?\",\n",
    "        \"Who created Python?\",\n",
    "        \"What is deep learning?\",\n",
    "        \"How does blockchain work?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Performance Comparison: Default vs Custom Prompt ===\\n\")\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Test with default prompt\n",
    "        result_default = qa_assistant.answer_question(question, use_custom_prompt=False)\n",
    "        print(f\"Default Prompt Answer: {result_default['answer']}\")\n",
    "        print(f\"Confidence: {result_default['confidence']:.3f}\")\n",
    "        \n",
    "        # Test with custom prompt\n",
    "        result_custom = qa_assistant.answer_question(question, use_custom_prompt=True)\n",
    "        print(f\"Custom Prompt Answer: {result_custom['answer']}\")\n",
    "        print(f\"Confidence: {result_custom['confidence']:.3f}\")\n",
    "        \n",
    "        print(f\"Retrieval Time: {result_default['retrieval_time']:.3f}s\")\n",
    "        print(f\"Documents Used: {result_default['documents_used']}\")\n",
    "        print(f\"Reranking Scores: {[f'{s:.3f}' for s in result_default['reranking_scores']]}\")\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02194364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building document index...\n",
      "Index built with 10 documents\n",
      "=== Performance Comparison: Default vs Custom Prompt ===\n",
      "\n",
      "Question: What is machine learning?\n",
      "--------------------------------------------------\n",
      "Retrieving candidates for: 'What is machine learning?'\n",
      "Reranking candidates...\n",
      "Default Prompt Answer: a subset of artificial intelligence\n",
      "Confidence: 0.285\n",
      "Retrieving candidates for: 'What is machine learning?'\n",
      "Reranking candidates...\n",
      "Custom Prompt Answer: a subset of artificial intelligence\n",
      "Confidence: 0.301\n",
      "Retrieval Time: 0.242s\n",
      "Documents Used: 3\n",
      "Reranking Scores: ['11.272', '4.944', '-3.607']\n",
      "\n",
      "============================================================\n",
      "\n",
      "Question: Who created Python?\n",
      "--------------------------------------------------\n",
      "Retrieving candidates for: 'Who created Python?'\n",
      "Reranking candidates...\n",
      "Default Prompt Answer: Guido van Rossum\n",
      "Confidence: 0.997\n",
      "Retrieving candidates for: 'Who created Python?'\n",
      "Reranking candidates...\n",
      "Custom Prompt Answer: Guido van Rossum\n",
      "Confidence: 0.996\n",
      "Retrieval Time: 0.110s\n",
      "Documents Used: 3\n",
      "Reranking Scores: ['9.501', '-10.634', '-10.916']\n",
      "\n",
      "============================================================\n",
      "\n",
      "Question: What is deep learning?\n",
      "--------------------------------------------------\n",
      "Retrieving candidates for: 'What is deep learning?'\n",
      "Reranking candidates...\n",
      "Default Prompt Answer: uses neural networks with multiple layers to model and understand complex patterns in data\n",
      "Confidence: 0.663\n",
      "Retrieving candidates for: 'What is deep learning?'\n",
      "Reranking candidates...\n",
      "Custom Prompt Answer: uses neural networks with multiple layers to model and understand complex patterns in data\n",
      "Confidence: 0.501\n",
      "Retrieval Time: 0.091s\n",
      "Documents Used: 3\n",
      "Reranking Scores: ['7.550', '-2.783', '-3.572']\n",
      "\n",
      "============================================================\n",
      "\n",
      "Question: How does blockchain work?\n",
      "--------------------------------------------------\n",
      "Retrieving candidates for: 'How does blockchain work?'\n",
      "Reranking candidates...\n",
      "Default Prompt Answer: maintains a continuously growing list of records linked using cryptography\n",
      "Confidence: 0.149\n",
      "Retrieving candidates for: 'How does blockchain work?'\n",
      "Reranking candidates...\n",
      "Custom Prompt Answer: How\n",
      "Confidence: 0.666\n",
      "Retrieval Time: 0.105s\n",
      "Documents Used: 3\n",
      "Reranking Scores: ['6.151', '-10.487', '-10.608']\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0201de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
