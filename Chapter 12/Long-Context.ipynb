{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa61967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c47d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongContextTest:\n",
    "    def __init__(self, wikipedia_url: str, ollama_url: str = \"http://localhost:11434\"):\n",
    "        self.wikipedia_url = wikipedia_url\n",
    "        self.ollama_url = ollama_url\n",
    "        self.base_text = self._extract_wikipedia_text()\n",
    "        self.sentences = self._split_into_sentences(self.base_text)\n",
    "        \n",
    "    def _extract_wikipedia_text(self) -> str:\n",
    "        try:\n",
    "            print(f\"Fetching Wikipedia page: {self.wikipedia_url}\")\n",
    "            response = requests.get(self.wikipedia_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'sup', 'table', 'div.navbox', 'div.infobox']):\n",
    "                element.decompose()\n",
    "            \n",
    "            # Extract main content (usually in div with id=\"mw-content-text\")\n",
    "            content = soup.find('div', {'id': 'mw-content-text'})\n",
    "            if not content:\n",
    "                content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "            if not content:\n",
    "                content = soup\n",
    "            \n",
    "            # Get all paragraphs\n",
    "            paragraphs = content.find_all('p')\n",
    "            text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "            \n",
    "            # Clean up text\n",
    "            text = re.sub(r'\\[.*?\\]', '', text)  # Remove citation brackets\n",
    "            text = re.sub(r'\\s+', ' ', text)     # Normalize whitespace\n",
    "            \n",
    "            print(f\"Extracted {len(text)} characters from Wikipedia\")\n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Wikipedia page: {e}\")\n",
    "            return \"\"\n",
    "        \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        sentences = re.split(r'[.!?]\\s+', text)\n",
    "        return [s.strip() + '.' for s in sentences if len(s.strip()) > 10]\n",
    "    \n",
    "    def _approximate_tokens(self, text: str) -> int:\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def _get_text_chunk(self, target_tokens: int) -> str:\n",
    "        target_chars = target_tokens * 4\n",
    "        current_chars = 0\n",
    "        result = []\n",
    "        \n",
    "        for sentence in self.sentences:\n",
    "            if current_chars + len(sentence) > target_chars and result:\n",
    "                break\n",
    "            result.append(sentence)\n",
    "            current_chars += len(sentence)\n",
    "        \n",
    "        return ' '.join(result)\n",
    "    \n",
    "    def create_test_questions(self) -> List[Dict]:\n",
    "        questions = [\n",
    "            {\n",
    "                \"question\": \"How many daily passengers use the rail system in Greater Tokyo?\",\n",
    "                \"needle\": \"40 million passengers (counted twice if transferring between operators) use the rail system daily (14.6 billion annually)\",\n",
    "                \"expected_answer\": \"40 million\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the busiest train station in the world by passenger throughput?\",\n",
    "                \"needle\": \"Shinjuku Station is the busiest train station in the world by passenger throughput.\",\n",
    "                \"expected_answer\": \"Shinjuku Station\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"How many rail lines does JR East operate within Greater Tokyo?\",\n",
    "                \"needle\": \"In total, JR alone operates 23 lines within the Greater Tokyo area.\",\n",
    "                \"expected_answer\": \"23 lines\"\n",
    "            }\n",
    "        ]\n",
    "        return questions\n",
    "    \n",
    "    def call_ollama(self, prompt: str, model: str = \"llama3\") -> str:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.ollama_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0.1}  # Low temperature for consistency\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            return response.json().get(\"response\", \"\")\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def create_context(self, needle: str, context_tokens: int) -> str:\n",
    "        if context_tokens <= 400:  # Just needle + 200 before/after\n",
    "            before_tokens = after_tokens = 200\n",
    "        else:\n",
    "            # Distribute extra tokens\n",
    "            extra_tokens = context_tokens - 400\n",
    "            before_tokens = 200 + extra_tokens // 2\n",
    "            after_tokens = 200 + extra_tokens - extra_tokens // 2\n",
    "        \n",
    "        before_text = self._get_text_chunk(before_tokens)\n",
    "        after_text = self._get_text_chunk(after_tokens)\n",
    "        \n",
    "        context = f\"{before_text} {needle} {after_text}\"\n",
    "        return context\n",
    "    \n",
    "    def test_single_question(self, question: Dict, context_tokens: int, trials: int = 10) -> Dict:\n",
    "        context = self.create_context(question[\"needle\"], context_tokens)\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following text about Tokyo's transport system, answer the question briefly and accurately.\n",
    "\n",
    "                    Text: {context}\n",
    "\n",
    "                    Question: {question[\"question\"]}\n",
    "\n",
    "                    Answer:\"\"\"\n",
    "        \n",
    "        correct_answers = 0\n",
    "        responses = []\n",
    "        \n",
    "        for i in range(trials):\n",
    "            response = self.call_ollama(prompt)\n",
    "            responses.append(response)\n",
    "            \n",
    "            # Simple check if expected answer is in response\n",
    "            if question[\"expected_answer\"].lower() in response.lower():\n",
    "                correct_answers += 1\n",
    "            \n",
    "            time.sleep(0.5)  # Small delay between requests\n",
    "        \n",
    "        accuracy = correct_answers / trials\n",
    "        actual_tokens = self._approximate_tokens(context)\n",
    "        \n",
    "        return {\n",
    "            \"context_tokens\": actual_tokens,\n",
    "            \"target_tokens\": context_tokens,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_answers\": correct_answers,\n",
    "            \"total_trials\": trials,\n",
    "            \"sample_responses\": responses[:3]  # Keep first 3 responses as samples\n",
    "        }\n",
    "    \n",
    "    def run_full_test(self, max_context_tokens: int = 8000, step_size: int = 400) -> Dict:\n",
    "        questions = self.create_test_questions()\n",
    "        results = {}\n",
    "        \n",
    "        print(f\"Running long-context efficacy test...\")\n",
    "        print(f\"Questions: {len(questions)}\")\n",
    "        print(f\"Max context: {max_context_tokens} tokens\")\n",
    "        print(f\"Step size: {step_size} tokens\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, question in enumerate(questions):\n",
    "            print(f\"\\nTesting Question {i+1}: {question['question'][:60]}...\")\n",
    "            question_results = []\n",
    "            \n",
    "            context_sizes = range(400, max_context_tokens + 1, step_size)\n",
    "            \n",
    "            for context_tokens in context_sizes:\n",
    "                print(f\"  Context size: {context_tokens} tokens\", end=\" \")\n",
    "                \n",
    "                result = self.test_single_question(question, context_tokens)\n",
    "                question_results.append(result)\n",
    "                \n",
    "                print(f\"- Accuracy: {result['accuracy']:.1%}\")\n",
    "                \n",
    "                # Early stopping if we can't fit more context\n",
    "                if result[\"context_tokens\"] >= max_context_tokens * 0.9:\n",
    "                    break\n",
    "            \n",
    "            results[f\"question_{i+1}\"] = {\n",
    "                \"question\": question[\"question\"],\n",
    "                \"expected_answer\": question[\"expected_answer\"],\n",
    "                \"results\": question_results\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results: Dict):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LONG CONTEXT EFFICACY TEST RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for q_key, q_data in results.items():\n",
    "            print(f\"\\n{q_key.upper()}: {q_data['question']}\")\n",
    "            print(f\"Expected: {q_data['expected_answer']}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for result in q_data[\"results\"]:\n",
    "                tokens = result[\"context_tokens\"]\n",
    "                accuracy = result[\"accuracy\"]\n",
    "                print(f\"  {tokens:5d} tokens: {accuracy:5.1%} accuracy\")\n",
    "        \n",
    "        # Overall accuracy trend\n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(\"ACCURACY TRENDS\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Calculate average accuracy at each context length\n",
    "        context_sizes = set()\n",
    "        for q_data in results.values():\n",
    "            for result in q_data[\"results\"]:\n",
    "                context_sizes.add(result[\"context_tokens\"])\n",
    "        \n",
    "        for size in sorted(context_sizes):\n",
    "            accuracies = []\n",
    "            for q_data in results.values():\n",
    "                for result in q_data[\"results\"]:\n",
    "                    if result[\"context_tokens\"] == size:\n",
    "                        accuracies.append(result[\"accuracy\"])\n",
    "            \n",
    "            if accuracies:\n",
    "                avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "                print(f\"  {size:5d} tokens: {avg_accuracy:5.1%} average accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c67b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Wikipedia URL for Tokyo transport\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Transport_in_Greater_Tokyo\"\n",
    "    \n",
    "    # Initialize and run test\n",
    "    test = LongContextTest(wikipedia_url)\n",
    "    \n",
    "    if not test.base_text:\n",
    "        print(\"Failed to extract text from Wikipedia. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully loaded {len(test.base_text)} characters of text\")\n",
    "    \n",
    "    results = test.run_full_test(max_context_tokens=4000, step_size=400)\n",
    "    results = test.run_full_test(max_context_tokens=4000, step_size=400)\n",
    "    \n",
    "    # Print results\n",
    "    test.print_summary(results)\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(\"long_context_test_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to: long_context_test_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf97c18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Wikipedia page: https://en.wikipedia.org/wiki/Transport_in_Greater_Tokyo\n",
      "Extracted 13472 characters from Wikipedia\n",
      "Successfully loaded 13472 characters of text\n",
      "Running long-context efficacy test...\n",
      "Questions: 3\n",
      "Max context: 4000 tokens\n",
      "Step size: 400 tokens\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Question 1: How many daily passengers use the rail system in Greater Tok...\n",
      "  Context size: 400 tokens - Accuracy: 100.0%\n",
      "  Context size: 800 tokens - Accuracy: 100.0%\n",
      "  Context size: 1200 tokens - Accuracy: 100.0%\n",
      "  Context size: 1600 tokens - Accuracy: 100.0%\n",
      "  Context size: 2000 tokens - Accuracy: 100.0%\n",
      "  Context size: 2400 tokens - Accuracy: 100.0%\n",
      "  Context size: 2800 tokens - Accuracy: 100.0%\n",
      "  Context size: 3200 tokens - Accuracy: 100.0%\n",
      "  Context size: 3600 tokens - Accuracy: 100.0%\n",
      "  Context size: 4000 tokens - Accuracy: 100.0%\n",
      "\n",
      "Testing Question 2: What is the busiest train station in the world by passenger ...\n",
      "  Context size: 400 tokens - Accuracy: 100.0%\n",
      "  Context size: 800 tokens - Accuracy: 100.0%\n",
      "  Context size: 1200 tokens - Accuracy: 100.0%\n",
      "  Context size: 1600 tokens - Accuracy: 100.0%\n",
      "  Context size: 2000 tokens - Accuracy: 100.0%\n",
      "  Context size: 2400 tokens - Accuracy: 100.0%\n",
      "  Context size: 2800 tokens - Accuracy: 100.0%\n",
      "  Context size: 3200 tokens - Accuracy: 100.0%\n",
      "  Context size: 3600 tokens - Accuracy: 100.0%\n",
      "  Context size: 4000 tokens - Accuracy: 100.0%\n",
      "\n",
      "Testing Question 3: How many rail lines does JR East operate within Greater Toky...\n",
      "  Context size: 400 tokens - Accuracy: 100.0%\n",
      "  Context size: 800 tokens - Accuracy: 100.0%\n",
      "  Context size: 1200 tokens - Accuracy: 100.0%\n",
      "  Context size: 1600 tokens - Accuracy: 100.0%\n",
      "  Context size: 2000 tokens - Accuracy: 100.0%\n",
      "  Context size: 2400 tokens - Accuracy: 100.0%\n",
      "  Context size: 2800 tokens - Accuracy: 100.0%\n",
      "  Context size: 3200 tokens - Accuracy: 100.0%\n",
      "  Context size: 3600 tokens - Accuracy: 100.0%\n",
      "  Context size: 4000 tokens - Accuracy: 100.0%\n",
      "Running long-context efficacy test...\n",
      "Questions: 3\n",
      "Max context: 4000 tokens\n",
      "Step size: 400 tokens\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing Question 1: How many daily passengers use the rail system in Greater Tok...\n",
      "  Context size: 400 tokens - Accuracy: 100.0%\n",
      "  Context size: 800 tokens - Accuracy: 100.0%\n",
      "  Context size: 1200 tokens - Accuracy: 100.0%\n",
      "  Context size: 1600 tokens - Accuracy: 100.0%\n",
      "  Context size: 2000 tokens - Accuracy: 100.0%\n",
      "  Context size: 2400 tokens - Accuracy: 100.0%\n",
      "  Context size: 2800 tokens - Accuracy: 100.0%\n",
      "  Context size: 3200 tokens - Accuracy: 100.0%\n",
      "  Context size: 3600 tokens - Accuracy: 100.0%\n",
      "  Context size: 4000 tokens - Accuracy: 100.0%\n",
      "\n",
      "Testing Question 2: What is the busiest train station in the world by passenger ...\n",
      "  Context size: 400 tokens - Accuracy: 100.0%\n",
      "  Context size: 800 tokens - Accuracy: 100.0%\n",
      "  Context size: 1200 tokens - Accuracy: 100.0%\n",
      "  Context size: 1600 tokens - Accuracy: 100.0%\n",
      "  Context size: 2000 tokens - Accuracy: 100.0%\n",
      "  Context size: 2400 tokens - Accuracy: 100.0%\n",
      "  Context size: 2800 tokens - Accuracy: 100.0%\n",
      "  Context size: 3200 tokens - Accuracy: 100.0%\n",
      "  Context size: 3600 tokens - Accuracy: 100.0%\n",
      "  Context size: 4000 tokens - Accuracy: 100.0%\n",
      "\n",
      "Testing Question 3: How many rail lines does JR East operate within Greater Toky...\n",
      "  Context size: 400 tokens - Accuracy: 100.0%\n",
      "  Context size: 800 tokens - Accuracy: 100.0%\n",
      "  Context size: 1200 tokens - Accuracy: 100.0%\n",
      "  Context size: 1600 tokens - Accuracy: 100.0%\n",
      "  Context size: 2000 tokens - Accuracy: 100.0%\n",
      "  Context size: 2400 tokens - Accuracy: 100.0%\n",
      "  Context size: 2800 tokens - Accuracy: 100.0%\n",
      "  Context size: 3200 tokens - Accuracy: 100.0%\n",
      "  Context size: 3600 tokens - Accuracy: 100.0%\n",
      "  Context size: 4000 tokens - Accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "LONG CONTEXT EFFICACY TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "QUESTION_1: How many daily passengers use the rail system in Greater Tokyo?\n",
      "Expected: 40 million\n",
      "----------------------------------------\n",
      "    253 tokens: 100.0% accuracy\n",
      "    826 tokens: 100.0% accuracy\n",
      "   1228 tokens: 100.0% accuracy\n",
      "   1635 tokens: 100.0% accuracy\n",
      "   1959 tokens: 100.0% accuracy\n",
      "   2388 tokens: 100.0% accuracy\n",
      "   2832 tokens: 100.0% accuracy\n",
      "   3187 tokens: 100.0% accuracy\n",
      "   3513 tokens: 100.0% accuracy\n",
      "   3996 tokens: 100.0% accuracy\n",
      "\n",
      "QUESTION_2: What is the busiest train station in the world by passenger throughput?\n",
      "Expected: Shinjuku Station\n",
      "----------------------------------------\n",
      "    243 tokens: 100.0% accuracy\n",
      "    816 tokens: 100.0% accuracy\n",
      "   1218 tokens: 100.0% accuracy\n",
      "   1625 tokens: 100.0% accuracy\n",
      "   1949 tokens: 100.0% accuracy\n",
      "   2378 tokens: 100.0% accuracy\n",
      "   2822 tokens: 100.0% accuracy\n",
      "   3178 tokens: 100.0% accuracy\n",
      "   3503 tokens: 100.0% accuracy\n",
      "   3987 tokens: 100.0% accuracy\n",
      "\n",
      "QUESTION_3: How many rail lines does JR East operate within Greater Tokyo?\n",
      "Expected: 23 lines\n",
      "----------------------------------------\n",
      "    239 tokens: 100.0% accuracy\n",
      "    812 tokens: 100.0% accuracy\n",
      "   1214 tokens: 100.0% accuracy\n",
      "   1621 tokens: 100.0% accuracy\n",
      "   1945 tokens: 100.0% accuracy\n",
      "   2374 tokens: 100.0% accuracy\n",
      "   2818 tokens: 100.0% accuracy\n",
      "   3174 tokens: 100.0% accuracy\n",
      "   3499 tokens: 100.0% accuracy\n",
      "   3983 tokens: 100.0% accuracy\n",
      "\n",
      "========================================\n",
      "ACCURACY TRENDS\n",
      "========================================\n",
      "    239 tokens: 100.0% average accuracy\n",
      "    243 tokens: 100.0% average accuracy\n",
      "    253 tokens: 100.0% average accuracy\n",
      "    812 tokens: 100.0% average accuracy\n",
      "    816 tokens: 100.0% average accuracy\n",
      "    826 tokens: 100.0% average accuracy\n",
      "   1214 tokens: 100.0% average accuracy\n",
      "   1218 tokens: 100.0% average accuracy\n",
      "   1228 tokens: 100.0% average accuracy\n",
      "   1621 tokens: 100.0% average accuracy\n",
      "   1625 tokens: 100.0% average accuracy\n",
      "   1635 tokens: 100.0% average accuracy\n",
      "   1945 tokens: 100.0% average accuracy\n",
      "   1949 tokens: 100.0% average accuracy\n",
      "   1959 tokens: 100.0% average accuracy\n",
      "   2374 tokens: 100.0% average accuracy\n",
      "   2378 tokens: 100.0% average accuracy\n",
      "   2388 tokens: 100.0% average accuracy\n",
      "   2818 tokens: 100.0% average accuracy\n",
      "   2822 tokens: 100.0% average accuracy\n",
      "   2832 tokens: 100.0% average accuracy\n",
      "   3174 tokens: 100.0% average accuracy\n",
      "   3178 tokens: 100.0% average accuracy\n",
      "   3187 tokens: 100.0% average accuracy\n",
      "   3499 tokens: 100.0% average accuracy\n",
      "   3503 tokens: 100.0% average accuracy\n",
      "   3513 tokens: 100.0% average accuracy\n",
      "   3983 tokens: 100.0% average accuracy\n",
      "   3987 tokens: 100.0% average accuracy\n",
      "   3996 tokens: 100.0% average accuracy\n",
      "\n",
      "Results saved to: long_context_test_results.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce261e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
