{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88bb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import math\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f06dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQLM:\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.model_name = model_name\n",
    "        self.ollama_url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    def get_perplexity(self, text: str) -> float:\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": text,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0,\n",
    "                \"num_predict\": 5  # Generate a few tokens to get likelihood\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.ollama_url, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                # Use response metrics as proxy for perplexity\n",
    "                response_length = len(result.get('response', ''))\n",
    "                prompt_length = len(text.split())\n",
    "                \n",
    "                # Simple perplexity approximation\n",
    "                # Lower values = better fit = more relevant\n",
    "                perplexity = prompt_length / max(1, response_length + 1)\n",
    "                return perplexity\n",
    "            else:\n",
    "                return float('inf')\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Ollama: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def score_document(self, query: str, document: str) -> float:\n",
    "        # Create prompt for the LLM to evaluate relevance\n",
    "        prompt = f\"Given this query: '{query}'\\nHow relevant is this document: '{document}'\\nRelevance score (0-10):\"\n",
    "        \n",
    "        # Get perplexity/score from the model\n",
    "        perplexity = self.get_perplexity(prompt)\n",
    "        \n",
    "        # Also use simple text overlap as backup\n",
    "        query_words = set(query.lower().split())\n",
    "        doc_words = set(document.lower().split())\n",
    "        overlap = len(query_words.intersection(doc_words))\n",
    "        \n",
    "        if len(query_words) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Combine LLM score with text overlap\n",
    "        overlap_score = overlap / len(query_words)\n",
    "        \n",
    "        # Convert perplexity to relevance score (lower perplexity = more relevant)\n",
    "        llm_score = 1.0 / (1.0 + perplexity) if perplexity != float('inf') else 0.0\n",
    "        \n",
    "        # Weighted combination\n",
    "        final_score = 0.7 * overlap_score + 0.3 * llm_score\n",
    "        \n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d824cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qlm():\n",
    "    \n",
    "    # Small test dataset\n",
    "    query = \"machine learning algorithms\"\n",
    "    \n",
    "    documents = [\n",
    "        \"Machine learning algorithms are computational methods that learn patterns from data to make predictions.\",\n",
    "        \"Cooking recipes often involve following step-by-step instructions to prepare delicious meals.\",\n",
    "        \"Deep learning is a subset of machine learning that uses neural networks with multiple layers.\",\n",
    "        \"The weather today is sunny with a chance of rain in the afternoon.\",\n",
    "        \"Supervised learning algorithms require labeled data to train predictive models.\",\n",
    "        \"Basketball is a popular sport played with two teams of five players each.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Query: '{query}'\\n\")\n",
    "    print(\"Testing QLM Document Ranking:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize QLM\n",
    "    qlm = SimpleQLM()\n",
    "    \n",
    "    # Score documents\n",
    "    scored_docs = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        score = qlm.score_document(query, doc)\n",
    "        scored_docs.append((score, i, doc))\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    scored_docs.sort(reverse=True)\n",
    "    \n",
    "    # Display results\n",
    "    for rank, (score, doc_id, doc) in enumerate(scored_docs, 1):\n",
    "        relevance = \"üî• HIGHLY RELEVANT\" if score > 0.5 else \"‚úÖ RELEVANT\" if score > 0.2 else \"‚ùå NOT RELEVANT\"\n",
    "        print(f\"Rank {rank}: Score = {score:.3f} {relevance}\")\n",
    "        print(f\"Doc {doc_id}: {doc[:80]}{'...' if len(doc) > 80 else ''}\")\n",
    "        print()\n",
    "    \n",
    "    return scored_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d88450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_effectiveness(scored_docs, ground_truth_relevant=[0, 2, 4]):\n",
    "    print(\"Effectiveness Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_indices = [doc_id for _, doc_id, _ in scored_docs[:3]]\n",
    "    \n",
    "    # Calculate precision@3\n",
    "    relevant_in_top3 = len(set(top_3_indices).intersection(set(ground_truth_relevant)))\n",
    "    precision_at_3 = relevant_in_top3 / 3\n",
    "    \n",
    "    # Calculate recall@3  \n",
    "    recall_at_3 = relevant_in_top3 / len(ground_truth_relevant)\n",
    "    \n",
    "    print(f\"Ground truth relevant docs: {ground_truth_relevant}\")\n",
    "    print(f\"Top 3 predicted docs: {top_3_indices}\")\n",
    "    print(f\"Precision@3: {precision_at_3:.3f}\")\n",
    "    print(f\"Recall@3: {recall_at_3:.3f}\")\n",
    "    \n",
    "    if precision_at_3 > 0.6:\n",
    "        print(\"‚úÖ QLM shows good effectiveness!\")\n",
    "    elif precision_at_3 > 0.3:\n",
    "        print(\"‚ö†Ô∏è  QLM shows moderate effectiveness\")\n",
    "    else:\n",
    "        print(\"‚ùå QLM needs improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a43d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple QLM Test with Llama3\n",
      "========================================\n",
      "Query: 'machine learning algorithms'\n",
      "\n",
      "Testing QLM Document Ranking:\n",
      "==================================================\n",
      "Rank 1: Score = 0.825 üî• HIGHLY RELEVANT\n",
      "Doc 0: Machine learning algorithms are computational methods that learn patterns from d...\n",
      "\n",
      "Rank 2: Score = 0.603 üî• HIGHLY RELEVANT\n",
      "Doc 4: Supervised learning algorithms require labeled data to train predictive models.\n",
      "\n",
      "Rank 3: Score = 0.589 üî• HIGHLY RELEVANT\n",
      "Doc 2: Deep learning is a subset of machine learning that uses neural networks with mul...\n",
      "\n",
      "Rank 4: Score = 0.135 ‚ùå NOT RELEVANT\n",
      "Doc 3: The weather today is sunny with a chance of rain in the afternoon.\n",
      "\n",
      "Rank 5: Score = 0.130 ‚ùå NOT RELEVANT\n",
      "Doc 1: Cooking recipes often involve following step-by-step instructions to prepare del...\n",
      "\n",
      "Rank 6: Score = 0.124 ‚ùå NOT RELEVANT\n",
      "Doc 5: Basketball is a popular sport played with two teams of five players each.\n",
      "\n",
      "Effectiveness Analysis:\n",
      "==============================\n",
      "Ground truth relevant docs: [0, 2, 4]\n",
      "Top 3 predicted docs: [0, 4, 2]\n",
      "Precision@3: 1.000\n",
      "Recall@3: 1.000\n",
      "‚úÖ QLM shows good effectiveness!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Simple QLM Test with Llama3\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Run the test\n",
    "    results = test_qlm()\n",
    "    \n",
    "    # Evaluate effectiveness\n",
    "    # Documents 0, 2, 4 are relevant to \"machine learning algorithms\"\n",
    "    evaluate_effectiveness(results, ground_truth_relevant=[0, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58e6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
