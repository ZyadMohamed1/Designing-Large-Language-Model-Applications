{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6e66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef63ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228120586d4742e686f25dc79c2d3fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\models--BAAI--llm-embedder. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb41a2f20154404b80d3d7ff90820675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a792f11894b949cb9ad2b059e5419286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d843199ee3c84cdfbe43571f51471c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22d4504c3d8431cb59fdf92f7862710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c646f5c57145089e838b4cc8b1e719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the LLM-Embedder model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/llm-embedder\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/llm-embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, instruction=\"\"):\n",
    "    # Add instruction prefix if provided (useful for queries)\n",
    "    if instruction:\n",
    "        texts = [f\"{instruction} {text}\" for text in texts]\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use mean pooling of last hidden states\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be7001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document embeddings...\n",
      "tensor([[-0.4256, -0.5326,  0.9291,  ..., -0.5737,  0.6766, -0.1039],\n",
      "        [-0.7919, -0.7674,  0.6042,  ..., -0.1216,  0.0868,  0.5076],\n",
      "        [-0.2909, -1.0550,  0.7306,  ..., -0.2639,  0.4091,  0.1456],\n",
      "        ...,\n",
      "        [-0.1506, -0.4367,  1.0117,  ..., -0.2462,  0.0858,  0.3055],\n",
      "        [-0.2443, -0.1187,  0.9840,  ..., -0.1865, -0.1917,  0.2820],\n",
      "        [-0.8901, -0.4655,  1.1325,  ..., -0.8435,  0.5171,  0.0556]])\n",
      "torch.Size([9, 768])\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base (documents)\n",
    "documents = [\n",
    "    \"Real Madrid and Barcelona clashed in another thrilling edition of El Clásico.\",\n",
    "    \"The match was filled with intensity, showcasing world-class football from both sides.\",\n",
    "    \"Real Madrid took an early lead with a clinical finish from Jude Bellingham.\",\n",
    "    \"Barcelona responded quickly with a brilliant goal by Robert Lewandowski.\",\n",
    "    \"The midfield battle was fierce, with both teams pressing high and forcing turnovers.\",\n",
    "    \"Vinícius Jr. caused constant problems for Barcelona’s defense with his pace and dribbling.\",\n",
    "    \"A late goal from Federico Valverde sealed the win for Real Madrid.\",\n",
    "    \"The Santiago Bernabéu erupted as Madrid secured a vital victory in La Liga.\",\n",
    "    \"The win pushed Real Madrid to the top of the table, asserting dominance in the title race.\"\n",
    "]\n",
    "\n",
    "# Create embeddings for documents\n",
    "print(\"Creating document embeddings...\")\n",
    "doc_embeddings = get_embeddings(documents)\n",
    "print(doc_embeddings)\n",
    "print(doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4807c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_answer(query, top_k=2):\n",
    "    \n",
    "    # Get query embedding with instruction\n",
    "    query_embedding = get_embeddings([query], instruction=\"Represent this sentence for searching relevant passages:\")\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = cosine_similarity(query_embedding.numpy(), doc_embeddings.numpy())[0]\n",
    "    \n",
    "    # Get top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        doc = documents[idx]\n",
    "        score = similarities[idx]\n",
    "        print(f\"Retrieved Document {i+1} (similarity: {score:.3f}):\")\n",
    "        print(f\"  {doc}\")\n",
    "        retrieved_docs.append(doc)\n",
    "    \n",
    "    # Create context for generation (in a real RAG system, this would go to an LLM)\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    print(f\"\\nCombined Context for Generation:\")\n",
    "    print(f\"  {context}\")\n",
    "    \n",
    "    return retrieved_docs, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1cef76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Who lead the first goal for real madrid?\n",
      "==================================================\n",
      "Retrieved Document 1 (similarity: 0.846):\n",
      "  Real Madrid took an early lead with a clinical finish from Jude Bellingham.\n",
      "Retrieved Document 2 (similarity: 0.823):\n",
      "  A late goal from Federico Valverde sealed the win for Real Madrid.\n",
      "\n",
      "Combined Context for Generation:\n",
      "  Real Madrid took an early lead with a clinical finish from Jude Bellingham.\n",
      "A late goal from Federico Valverde sealed the win for Real Madrid.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Query: Who scored the win goal\n",
      "==================================================\n",
      "Retrieved Document 1 (similarity: 0.828):\n",
      "  A late goal from Federico Valverde sealed the win for Real Madrid.\n",
      "Retrieved Document 2 (similarity: 0.798):\n",
      "  Barcelona responded quickly with a brilliant goal by Robert Lewandowski.\n",
      "\n",
      "Combined Context for Generation:\n",
      "  A late goal from Federico Valverde sealed the win for Real Madrid.\n",
      "Barcelona responded quickly with a brilliant goal by Robert Lewandowski.\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"Who lead the first goal for real madrid?\",\n",
    "        \"Who scored the win goal\",\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        retrieve_and_answer(query)\n",
    "        print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb863e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
