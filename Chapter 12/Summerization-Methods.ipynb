{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c4e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86077fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAFocusedSummarizer:    \n",
    "    def __init__(self):\n",
    "        # Keywords that indicate important information for QA\n",
    "        self.qa_keywords = [\n",
    "            'what', 'when', 'where', 'who', 'why', 'how', 'which',\n",
    "            'because', 'due to', 'caused by', 'resulted in', 'led to',\n",
    "            'definition', 'means', 'refers to', 'defined as'\n",
    "        ]\n",
    "    \n",
    "    def extractive_summary(self, document: str, query: str, max_sentences: int = 3) -> str:\n",
    "        sentences = self._split_sentences(document)\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        # Score sentences based on query relevance\n",
    "        scored_sentences = []\n",
    "        for sentence in sentences:\n",
    "            score = self._calculate_relevance_score(sentence, query_words)\n",
    "            scored_sentences.append((sentence, score))\n",
    "        \n",
    "        # Select top sentences\n",
    "        top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:max_sentences]\n",
    "        return ' '.join([sent[0] for sent in top_sentences])\n",
    "    \n",
    "    def abstractive_summary(self, document: str, query: str) -> str:\n",
    "        sentences = self._split_sentences(document)\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        # Extract key facts and entities\n",
    "        key_facts = self._extract_key_facts(sentences, query_words)\n",
    "        entities = self._extract_entities(document)\n",
    "        \n",
    "        # Generate focused summary\n",
    "        summary_parts = []\n",
    "        \n",
    "        # Add direct answer if found\n",
    "        direct_answer = self._find_direct_answer(sentences, query_words)\n",
    "        if direct_answer:\n",
    "            summary_parts.append(direct_answer)\n",
    "        \n",
    "        # Add supporting context\n",
    "        context = self._extract_supporting_context(key_facts, entities)\n",
    "        if context:\n",
    "            summary_parts.append(context)\n",
    "        \n",
    "        return ' '.join(summary_parts)\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def _calculate_relevance_score(self, sentence: str, query_words: set) -> float:\n",
    "        sentence_words = set(sentence.lower().split())\n",
    "        \n",
    "        # Base score from word overlap\n",
    "        overlap = len(query_words.intersection(sentence_words))\n",
    "        base_score = overlap / len(query_words) if query_words else 0\n",
    "        \n",
    "        # Bonus for QA keywords\n",
    "        qa_bonus = sum(1 for word in self.qa_keywords if word in sentence.lower()) * 0.1\n",
    "        \n",
    "        # Bonus for sentence length (avoid very short sentences)\n",
    "        length_bonus = min(len(sentence.split()) / 20, 0.2)\n",
    "        \n",
    "        return base_score + qa_bonus + length_bonus\n",
    "    \n",
    "    def _extract_key_facts(self, sentences: List[str], query_words: set) -> List[str]:\n",
    "        facts = []\n",
    "        for sentence in sentences:\n",
    "            if self._calculate_relevance_score(sentence, query_words) > 0.3:\n",
    "                facts.append(sentence)\n",
    "        return facts[:2]  # Top 2 facts\n",
    "    \n",
    "    def _extract_entities(self, text: str) -> List[str]:\n",
    "        words = text.split()\n",
    "        entities = [word for word in words if word[0].isupper() and len(word) > 2]\n",
    "        return list(set(entities))[:5]  # Top 5 unique entities\n",
    "    \n",
    "    def _find_direct_answer(self, sentences: List[str], query_words: set) -> str:\n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            # Look for definition patterns\n",
    "            if any(pattern in sentence_lower for pattern in ['is defined as', 'means', 'refers to']):\n",
    "                return sentence\n",
    "        return \"\"\n",
    "    \n",
    "    def _extract_supporting_context(self, facts: List[str], entities: List[str]) -> str:\n",
    "        if not facts:\n",
    "            return \"\"\n",
    "        \n",
    "        # Combine most relevant fact with entities\n",
    "        main_fact = facts[0] if facts else \"\"\n",
    "        entity_context = f\"Key entities mentioned: {', '.join(entities[:3])}\" if entities else \"\"\n",
    "        \n",
    "        return f\"{main_fact} {entity_context}\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72cf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericSummarizer:\n",
    "    \n",
    "    def summarize(self, document: str, max_sentences: int = 3) -> str:\n",
    "        sentences = self._split_sentences(document)\n",
    "        \n",
    "        # Score sentences based on general importance\n",
    "        scored_sentences = []\n",
    "        for sentence in sentences:\n",
    "            score = self._calculate_generic_score(sentence, sentences)\n",
    "            scored_sentences.append((sentence, score))\n",
    "        \n",
    "        # Select top sentences\n",
    "        top_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)[:max_sentences]\n",
    "        return ' '.join([sent[0] for sent in top_sentences])\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    def _calculate_generic_score(self, sentence: str, all_sentences: List[str]) -> float:\n",
    "        words = sentence.lower().split()\n",
    "        \n",
    "        # Word frequency across document\n",
    "        all_words = ' '.join(all_sentences).lower().split()\n",
    "        word_freq = Counter(all_words)\n",
    "        \n",
    "        # Score based on word frequencies\n",
    "        freq_score = sum(word_freq[word] for word in words) / len(words)\n",
    "        \n",
    "        # Position bonus (first and last sentences often important)\n",
    "        position_bonus = 0.1 if sentence in all_sentences[:2] or sentence in all_sentences[-2:] else 0\n",
    "        \n",
    "        # Length bonus\n",
    "        length_bonus = min(len(words) / 15, 0.2)\n",
    "        \n",
    "        return freq_score + position_bonus + length_bonus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abe47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON OF SUMMARIZATION APPROACHES ===\n",
      "\n",
      "Query: What is deep learning?\n",
      "\n",
      "Original Document Length: 101 words\n",
      "\n",
      "1. QA-Focused Extractive Summary:\n",
      "Deep learning is a specialized form of machine learning \n",
      "    that uses neural networks with multiple layers Machine learning is a subset of artificial intelligence that focuses on algorithms \n",
      "    that can learn from data Backpropagation is the algorithm used to update these weights\n",
      "\n",
      "Length: 42 words\n",
      "Noise Removal: Focuses on query-relevant sentences\n",
      "\n",
      "2. QA-Focused Abstractive Summary:\n",
      "Machine learning is a subset of artificial intelligence that focuses on algorithms \n",
      "    that can learn from data Key entities mentioned: Neural, Convolutional, Backpropagation\n",
      "\n",
      "Length: 23 words\n",
      "Noise Removal: Synthesizes answer-specific information\n",
      "\n",
      "3. Generic Summary (ChatGPT-like):\n",
      "Deep learning is a specialized form of machine learning \n",
      "    that uses neural networks with multiple layers The process \n",
      "    of training involves feeding data to the network and adjusting weights Recurrent neural networks \n",
      "    are designed for sequential data like text and time series\n",
      "\n",
      "Length: 42 words\n",
      "Noise Removal: General importance, may include irrelevant info\n",
      "\n",
      "=== ANALYSIS ===\n",
      "QA-Focused Summaries:\n",
      "✓ Target specific information relevant to the query\n",
      "✓ Remove noise unrelated to the question\n",
      "✓ Maintain context needed for accurate answers\n",
      "\n",
      "Generic Summaries:\n",
      "• May include important but irrelevant information\n",
      "• Don't prioritize query-specific content\n",
      "• Good for general understanding, poor for specific QA\n"
     ]
    }
   ],
   "source": [
    "# Example usage and comparison\n",
    "def compare_summarizers():\n",
    "    # Sample document and query\n",
    "    document = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that focuses on algorithms \n",
    "    that can learn from data. Deep learning is a specialized form of machine learning \n",
    "    that uses neural networks with multiple layers. Neural networks are inspired by \n",
    "    the human brain and consist of interconnected nodes called neurons. The process \n",
    "    of training involves feeding data to the network and adjusting weights. \n",
    "    Backpropagation is the algorithm used to update these weights. Convolutional \n",
    "    neural networks are particularly effective for image recognition tasks. \n",
    "    They use filters to detect features in images. Recurrent neural networks \n",
    "    are designed for sequential data like text and time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"What is deep learning?\"\n",
    "    \n",
    "    # Initialize summarizers\n",
    "    qa_summarizer = QAFocusedSummarizer()\n",
    "    generic_summarizer = GenericSummarizer()\n",
    "    \n",
    "    print(\"=== COMPARISON OF SUMMARIZATION APPROACHES ===\\n\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(f\"Original Document Length: {len(document.split())} words\\n\")\n",
    "    \n",
    "    # QA-Focused Extractive Summary\n",
    "    extractive_summary = qa_summarizer.extractive_summary(document, query)\n",
    "    print(\"1. QA-Focused Extractive Summary:\")\n",
    "    print(f\"{extractive_summary}\\n\")\n",
    "    print(f\"Length: {len(extractive_summary.split())} words\")\n",
    "    print(f\"Noise Removal: Focuses on query-relevant sentences\\n\")\n",
    "    \n",
    "    # QA-Focused Abstractive Summary  \n",
    "    abstractive_summary = qa_summarizer.abstractive_summary(document, query)\n",
    "    print(\"2. QA-Focused Abstractive Summary:\")\n",
    "    print(f\"{abstractive_summary}\\n\")\n",
    "    print(f\"Length: {len(abstractive_summary.split())} words\")\n",
    "    print(f\"Noise Removal: Synthesizes answer-specific information\\n\")\n",
    "    \n",
    "    # Generic Summary (ChatGPT-like)\n",
    "    generic_summary = generic_summarizer.summarize(document)\n",
    "    print(\"3. Generic Summary (ChatGPT-like):\")\n",
    "    print(f\"{generic_summary}\\n\")\n",
    "    print(f\"Length: {len(generic_summary.split())} words\")\n",
    "    print(f\"Noise Removal: General importance, may include irrelevant info\\n\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"=== ANALYSIS ===\")\n",
    "    print(\"QA-Focused Summaries:\")\n",
    "    print(\"✓ Target specific information relevant to the query\")\n",
    "    print(\"✓ Remove noise unrelated to the question\")\n",
    "    print(\"✓ Maintain context needed for accurate answers\")\n",
    "    print()\n",
    "    print(\"Generic Summaries:\")\n",
    "    print(\"• May include important but irrelevant information\")\n",
    "    print(\"• Don't prioritize query-specific content\")\n",
    "    print(\"• Good for general understanding, poor for specific QA\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_summarizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16eb04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
