{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b501697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import statistics\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dca7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidenceAssessment:\n",
    "    def __init__(self, model_name=\"llama3\"):\n",
    "        self.model_name = model_name\n",
    "        self.ollama_url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    def query_llama(self, prompt, temperature=0.7):\n",
    "        \"\"\"Send query to Ollama\"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        }\n",
    "        response = requests.post(self.ollama_url, json=payload)\n",
    "        return response.json()[\"response\"].strip()\n",
    "    \n",
    "    def method1_direct_confidence(self, question):\n",
    "        prompt = f\"\"\"Question: {question}\n",
    "        \n",
    "                Please answer the question and then rate your confidence from 0-100 (where 100 is completely certain).\n",
    "                Format: Answer: [your answer]\n",
    "                Confidence: [0-100]\"\"\"\n",
    "        \n",
    "        response = self.query_llama(prompt)\n",
    "        \n",
    "        # Extract confidence score\n",
    "        confidence_match = re.search(r'Confidence:\\s*(\\d+)', response, re.IGNORECASE)\n",
    "        confidence = int(confidence_match.group(1)) if confidence_match else 50\n",
    "        \n",
    "        return response, confidence\n",
    "    \n",
    "    def method2_margin_sampling(self, question, n_samples=5):\n",
    "        answers = []\n",
    "        temperatures = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        \n",
    "        for temp in temperatures[:n_samples]:\n",
    "            prompt = f\"Question: {question}\\nAnswer briefly:\"\n",
    "            answer = self.query_llama(prompt, temperature=temp)\n",
    "            answers.append(answer.lower().strip())\n",
    "        \n",
    "        # Calculate consistency (inverse of diversity)\n",
    "        unique_answers = len(set(answers))\n",
    "        consistency = ((n_samples - unique_answers + 1) / n_samples) * 100\n",
    "        \n",
    "        return answers[0], consistency\n",
    "    \n",
    "    def method3_self_consistency(self, question, n_samples=5):\n",
    "        prompt = f\"Question: {question}\\nAnswer briefly:\"\n",
    "        answers = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            answer = self.query_llama(prompt, temperature=0.8)\n",
    "            answers.append(answer.lower().strip())\n",
    "        \n",
    "        # Find most common answer and calculate confidence\n",
    "        from collections import Counter\n",
    "        answer_counts = Counter(answers)\n",
    "        most_common_answer, count = answer_counts.most_common(1)[0]\n",
    "        confidence = (count / n_samples) * 100\n",
    "        \n",
    "        return most_common_answer, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90fe0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_confidence_methods():\n",
    "    \n",
    "    # Test questions - mix of easy facts and potentially tricky ones\n",
    "    test_questions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"In what year did World War II end?\",\n",
    "        \"What is the largest planet in our solar system?\",\n",
    "        \"Who painted the Mona Lisa?\",\n",
    "        \"What is the chemical symbol for gold?\",\n",
    "        \"In what year was the Berlin Wall torn down?\",\n",
    "        \"What is the deepest ocean trench on Earth?\",\n",
    "        \"Who was the first person to walk on the moon?\"\n",
    "    ]\n",
    "    \n",
    "    assessor = ConfidenceAssessment()\n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing Confidence Assessment Methods\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Method 1: Direct confidence\n",
    "            answer1, conf1 = assessor.method1_direct_confidence(question)\n",
    "            print(answer1)\n",
    "            print(f\"Direct Confidence: {conf1}%\")\n",
    "            \n",
    "            # Method 2: Margin sampling\n",
    "            answer2, conf2 = assessor.method2_margin_sampling(question)\n",
    "            print(answer2)\n",
    "            print(f\"Margin Sampling: {conf2:.1f}%\")\n",
    "            \n",
    "            # Method 3: Self-consistency\n",
    "            answer3, conf3 = assessor.method3_self_consistency(question)\n",
    "            print(answer3)\n",
    "            print(f\"Self-Consistency: {conf3:.1f}%\")\n",
    "            \n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'direct_conf': conf1,\n",
    "                'margin_conf': conf2,\n",
    "                'self_cons_conf': conf3,\n",
    "                'answers': [answer1, answer2, answer3]\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {e}\")\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results:\n",
    "        direct_scores = [r['direct_conf'] for r in results]\n",
    "        margin_scores = [r['margin_conf'] for r in results]\n",
    "        self_cons_scores = [r['self_cons_conf'] for r in results]\n",
    "        \n",
    "        print(f\"Average Confidence Scores:\")\n",
    "        print(f\"Direct Confidence: {statistics.mean(direct_scores):.1f}%\")\n",
    "        print(f\"Margin Sampling: {statistics.mean(margin_scores):.1f}%\")\n",
    "        print(f\"Self-Consistency: {statistics.mean(self_cons_scores):.1f}%\")\n",
    "        \n",
    "        print(f\"\\nStandard Deviation:\")\n",
    "        print(f\"Direct Confidence: {statistics.stdev(direct_scores):.1f}\")\n",
    "        print(f\"Margin Sampling: {statistics.stdev(margin_scores):.1f}\")\n",
    "        print(f\"Self-Consistency: {statistics.stdev(self_cons_scores):.1f}\")\n",
    "        \n",
    "        print(\"\\nObservations:\")\n",
    "        print(\"- Lower std dev suggests more calibrated confidence\")\n",
    "        print(\"- Compare how methods handle easy vs hard questions\")\n",
    "        print(\"- Self-consistency often most reliable for factual questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee3e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Confidence Assessment Methods\n",
      "==================================================\n",
      "\n",
      "1. What is the capital of France?\n",
      "----------------------------------------\n",
      "Answer: Paris\n",
      "\n",
      "Confidence: 100\n",
      "Direct Confidence: 100%\n",
      "paris.\n",
      "Margin Sampling: 100.0%\n",
      "paris.\n",
      "Self-Consistency: 80.0%\n",
      "\n",
      "2. In what year did World War II end?\n",
      "----------------------------------------\n",
      "Answer: 1945\n",
      "\n",
      "Confidence: 100\n",
      "Direct Confidence: 100%\n",
      "world war ii ended in 1945.\n",
      "Margin Sampling: 60.0%\n",
      "world war ii ended on september 2, 1945.\n",
      "Self-Consistency: 40.0%\n",
      "\n",
      "3. What is the largest planet in our solar system?\n",
      "----------------------------------------\n",
      "Answer: Jupiter\n",
      "\n",
      "Confidence: 100\n",
      "Direct Confidence: 100%\n",
      "jupiter.\n",
      "Margin Sampling: 80.0%\n",
      "jupiter.\n",
      "Self-Consistency: 40.0%\n",
      "\n",
      "4. Who painted the Mona Lisa?\n",
      "----------------------------------------\n",
      "Answer: Leonardo da Vinci\n",
      "\n",
      "Confidence: 99\n",
      "Direct Confidence: 99%\n",
      "leonardo da vinci.\n",
      "Margin Sampling: 100.0%\n",
      "leonardo da vinci.\n",
      "Self-Consistency: 100.0%\n",
      "\n",
      "5. What is the chemical symbol for gold?\n",
      "----------------------------------------\n",
      "Answer: Au\n",
      "\n",
      "Confidence: 100\n",
      "Direct Confidence: 100%\n",
      "the chemical symbol for gold is au.\n",
      "Margin Sampling: 80.0%\n",
      "the chemical symbol for gold is au.\n",
      "Self-Consistency: 60.0%\n",
      "\n",
      "6. In what year was the Berlin Wall torn down?\n",
      "----------------------------------------\n",
      "Answer: 1989\n",
      "\n",
      "Confidence: 99\n",
      "Direct Confidence: 99%\n",
      "the berlin wall was torn down in 1989.\n",
      "Margin Sampling: 80.0%\n",
      "the berlin wall was torn down in 1989.\n",
      "Self-Consistency: 100.0%\n",
      "\n",
      "7. What is the deepest ocean trench on Earth?\n",
      "----------------------------------------\n",
      "Answer: The Mariana Trench.\n",
      "\n",
      "Confidence: 90\n",
      "\n",
      "I'm quite confident about this answer, as the Mariana Trench is widely recognized as the deepest ocean trench on Earth. It has a maximum depth of approximately 36,000 feet (10,973 meters), and it's located in the Pacific Ocean, to the east of the Mariana Islands. However, I wouldn't rate my confidence at 100%, as there might be some new discoveries or revisions that could potentially change this answer.\n",
      "Direct Confidence: 90%\n",
      "the mariana trench, located in the pacific ocean, is the deepest ocean trench on earth, with a maximum depth of approximately 36,000 feet (10,973 meters).\n",
      "Margin Sampling: 60.0%\n",
      "the deepest ocean trench on earth is the mariana trench, which has a maximum depth of approximately 36,000 feet (10,973 meters).\n",
      "Self-Consistency: 40.0%\n",
      "\n",
      "8. Who was the first person to walk on the moon?\n",
      "----------------------------------------\n",
      "Answer: Neil Armstrong\n",
      "\n",
      "Confidence: 100\n",
      "Direct Confidence: 100%\n",
      "neil armstrong was the first person to walk on the moon. he stepped out of the lunar module eagle and onto the moon's surface on july 20, 1969, during the apollo 11 mission.\n",
      "Margin Sampling: 60.0%\n",
      "the first person to walk on the moon was neil armstrong, an american astronaut who stepped out of the lunar module eagle onto the moon's surface on july 20, 1969, during the apollo 11 mission.\n",
      "Self-Consistency: 20.0%\n",
      "\n",
      "==================================================\n",
      "SUMMARY ANALYSIS\n",
      "==================================================\n",
      "Average Confidence Scores:\n",
      "Direct Confidence: 98.5%\n",
      "Margin Sampling: 77.5%\n",
      "Self-Consistency: 60.0%\n",
      "\n",
      "Standard Deviation:\n",
      "Direct Confidence: 3.5\n",
      "Margin Sampling: 16.7\n",
      "Self-Consistency: 30.2\n",
      "\n",
      "Observations:\n",
      "- Lower std dev suggests more calibrated confidence\n",
      "- Compare how methods handle easy vs hard questions\n",
      "- Self-consistency often most reliable for factual questions\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_confidence_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b29f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
