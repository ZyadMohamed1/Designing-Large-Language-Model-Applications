{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6771f01",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8336d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from langdetect import detect, detect_langs, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f688ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C4 realnewslike dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3826ae6856754778bbefefccaedffb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77dbf171a20448bb4d6803aff651681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the realnewslike subset of C4\n",
    "print(\"Loading C4 realnewslike dataset...\")\n",
    "dataset = datasets.load_dataset(\"allenai/c4\", \"realnewslike\", split=\"train\", streaming=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba18bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_languages_with_langdetect(text):\n",
    "    languages_found = []\n",
    "    \n",
    "    try:\n",
    "        # Split text into chunks to analyze different parts\n",
    "        chunks = [text[i:i+1000] for i in range(0, len(text), 500)][:5]  # Max 5 chunks\n",
    "        \n",
    "        detected_languages = set()\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if len(chunk.strip()) < 50:  # Skip very short chunks\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Get primary language\n",
    "                primary_lang = detect(chunk)\n",
    "                if primary_lang != 'en':\n",
    "                    detected_languages.add(primary_lang)\n",
    "                \n",
    "                # Get confidence scores for multiple languages\n",
    "                lang_probs = detect_langs(chunk)\n",
    "                for lang_prob in lang_probs:\n",
    "                    if lang_prob.lang != 'en' and lang_prob.prob > 0.3:  # Confidence threshold\n",
    "                        detected_languages.add(lang_prob.lang)\n",
    "                        \n",
    "            except LangDetectException:\n",
    "                continue\n",
    "        \n",
    "        # Convert to list with dummy confidence scores\n",
    "        languages_found = [(lang, 1.0) for lang in detected_languages]\n",
    "        \n",
    "    except LangDetectException:\n",
    "        # Fallback: try to detect on full text\n",
    "        try:\n",
    "            detected = detect(text)\n",
    "            if detected != 'en':\n",
    "                languages_found = [(detected, 1.0)]\n",
    "        except LangDetectException:\n",
    "            pass\n",
    "    \n",
    "    return languages_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99010f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_context(text, max_chars=500):\n",
    "    # Look for common contexts where non-English appears\n",
    "    contexts = []\n",
    "    \n",
    "    # Check for quotations\n",
    "    if '\"' in text or '\"' in text or '\"' in text:\n",
    "        contexts.append(\"quoted_text\")\n",
    "    \n",
    "    # Check for URLs/domains\n",
    "    if re.search(r'https?://[^\\s]+|www\\.[^\\s]+|\\.[a-z]{2,3}/', text):\n",
    "        contexts.append(\"web_content\")\n",
    "    \n",
    "    # Check for names/proper nouns (multiple capitalized words)\n",
    "    if len(re.findall(r'\\b[A-Z][a-z]+\\s+[A-Z][a-z]+', text)) > 2:\n",
    "        contexts.append(\"proper_nouns\")\n",
    "    \n",
    "    # Check for mixed language (English + other)\n",
    "    english_words = len(re.findall(r'\\b(the|and|or|but|in|on|at|to|for|of|with|by)\\b', text, re.IGNORECASE))\n",
    "    if english_words > 5:\n",
    "        contexts.append(\"mixed_language\")\n",
    "    \n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48263785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing C4 samples for non-English content...\n",
      "Processed 100 samples...\n",
      "Processed 200 samples...\n",
      "Processed 300 samples...\n",
      "Processed 400 samples...\n",
      "Processed 500 samples...\n",
      "Processed 600 samples...\n",
      "Processed 700 samples...\n",
      "Processed 800 samples...\n",
      "Processed 900 samples...\n",
      "Processed 1000 samples...\n",
      "\n",
      "=== ANALYSIS RESULTS ===\n",
      "Processed 1000 samples\n",
      "Found 3 samples with non-English content\n"
     ]
    }
   ],
   "source": [
    "# Analyze samples\n",
    "print(\"\\nAnalyzing C4 samples for non-English content...\")\n",
    "non_english_examples = []\n",
    "context_counts = Counter()\n",
    "language_counts = Counter()\n",
    "\n",
    "# Process samples\n",
    "sample_count = 0\n",
    "for example in dataset:\n",
    "    if sample_count >= 1000:  # Limit for demonstration\n",
    "        break\n",
    "    \n",
    "    text = example['text']\n",
    "    languages = detect_languages_with_langdetect(text)\n",
    "    \n",
    "    if languages:\n",
    "        contexts = analyze_context(text)\n",
    "        \n",
    "        # Store example\n",
    "        non_english_examples.append({\n",
    "            'text': text[:300] + \"...\" if len(text) > 300 else text,\n",
    "            'languages': languages,\n",
    "            'contexts': contexts,\n",
    "            'url': example.get('url', 'N/A')\n",
    "        })\n",
    "        \n",
    "        # Update counters\n",
    "        for lang, score in languages:\n",
    "            language_counts[lang] += 1\n",
    "        for context in contexts:\n",
    "            context_counts[context] += 1\n",
    "    \n",
    "    sample_count += 1\n",
    "    if sample_count % 100 == 0:\n",
    "        print(f\"Processed {sample_count} samples...\")\n",
    "\n",
    "# Results\n",
    "print(f\"\\n=== ANALYSIS RESULTS ===\")\n",
    "print(f\"Processed {sample_count} samples\")\n",
    "print(f\"Found {len(non_english_examples)} samples with non-English content\")\n",
    "\n",
    "# Language code mapping for better display\n",
    "LANGUAGE_NAMES = {\n",
    "    'es': 'Spanish', 'fr': 'French', 'de': 'German', 'it': 'Italian',\n",
    "    'pt': 'Portuguese', 'ru': 'Russian', 'ja': 'Japanese', 'ko': 'Korean',\n",
    "    'zh-cn': 'Chinese', 'ar': 'Arabic', 'hi': 'Hindi', 'nl': 'Dutch',\n",
    "    'sv': 'Swedish', 'da': 'Danish', 'no': 'Norwegian', 'fi': 'Finnish',\n",
    "    'pl': 'Polish', 'cs': 'Czech', 'hu': 'Hungarian', 'ro': 'Romanian',\n",
    "    'tr': 'Turkish', 'el': 'Greek', 'he': 'Hebrew', 'th': 'Thai',\n",
    "    'vi': 'Vietnamese', 'id': 'Indonesian', 'ms': 'Malay', 'tl': 'Filipino'\n",
    "}\n",
    "\n",
    "def get_language_name(code):\n",
    "    return LANGUAGE_NAMES.get(code, code.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57203da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LANGUAGE DISTRIBUTION ===\n",
      "Italian (it): 1 instances\n",
      "AF (af): 1 instances\n",
      "SW (sw): 1 instances\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== LANGUAGE DISTRIBUTION ===\")\n",
    "for lang, count in language_counts.most_common():\n",
    "    lang_name = get_language_name(lang)\n",
    "    print(f\"{lang_name} ({lang}): {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4205563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONTEXT ANALYSIS ===\n",
      "Mixed Language: 3 instances\n",
      "Proper Nouns: 2 instances\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== CONTEXT ANALYSIS ===\")\n",
    "for context, count in context_counts.most_common():\n",
    "    print(f\"{context.replace('_', ' ').title()}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091b4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE NON-ENGLISH FRAGMENTS ===\n",
      "\n",
      "--- ITALIAN (it) EXAMPLE ---\n",
      "Contexts: proper_nouns, mixed_language\n",
      "URL: https://www.dcourier.com/news/2017/jan/01/births-announcements-part-ii-january-1-2016/\n",
      "Text: Seren Rayne Frank Sutherland, a six lb., eight oz., girl, was born Saturday, Dec. 3, 2016, at Yavapai Regional Medical Center to Donell Sutherland and Adam Frank of Prescott.\n",
      "Alexander Velasco, a six lb., 12 oz., boy, was born Wednesday, Dec. 7, 2016, at Yavapai Regional Medical Center to Erika Avit...\n",
      "\n",
      "--- AF (af) EXAMPLE ---\n",
      "Contexts: mixed_language\n",
      "URL: https://independentjobs.independent.co.uk/job/9739075/audio-visual-project-manager-events-industry/\n",
      "Text: A live events and bespoke creative technical solutions company based in Loughborough is seeking a technical project manager to join their growing production team.\n",
      "You may currently be working be working as an audio-visual/AV project manager or technical production manager in live events or you may b...\n",
      "\n",
      "--- SW (sw) EXAMPLE ---\n",
      "Contexts: proper_nouns, mixed_language\n",
      "URL: http://www.maravipost.com/malawis-zbs-chief-kazako-flat-house-sealed-over-nonpayment-of-rentals/\n",
      "Text: LILONGWE-(MaraviPost)-An angry landlord on Wednesday sealed a flat house at Area 6 in the capital Lilongwe for Zodiak Broadcasting Corporation Managing Director Gospel Kazako for non payment of rentals.\n",
      "According to information gathered reveals that Kazako has been giving excuses to the owner of the...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== EXAMPLE NON-ENGLISH FRAGMENTS ===\")\n",
    "# Show a few examples from each language\n",
    "shown_languages = set()\n",
    "for example in non_english_examples[:10]:\n",
    "    for lang, score in example['languages']:\n",
    "        if lang not in shown_languages:\n",
    "            lang_name = get_language_name(lang)\n",
    "            print(f\"\\n--- {lang_name.upper()} ({lang}) EXAMPLE ---\")\n",
    "            print(f\"Contexts: {', '.join(example['contexts'])}\")\n",
    "            print(f\"URL: {example['url']}\")\n",
    "            print(f\"Text: {example['text']}\")\n",
    "            shown_languages.add(lang)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af99421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM LEARNING POTENTIAL ANALYSIS ===\n",
      "Average fragment length: 303 characters\n",
      "Coherent fragments (2+ sentences): 3/3 (100.0%)\n",
      "Mixed language content: 3/3 (100.0%)\n",
      "\n",
      "=== LEARNING ASSESSMENT ===\n",
      "✓ GOOD: Fragments are long enough and coherent enough for learning\n",
      "✓ GOOD: Mixed language content provides translation context\n",
      "\n",
      "=== CONCLUSIONS ===\n",
      "Based on this analysis of C4's 'English' dataset:\n",
      "1. Non-English content DOES persist despite filtering\n",
      "2. Common contexts: web content, quoted text, proper nouns, mixed language\n",
      "3. Learning potential depends on:\n",
      "   - Fragment coherence: 100.0%\n",
      "   - Mixed language context: 100.0%\n",
      "   - Average length: 303 chars\n",
      "4. ✓ An LLM could potentially learn some vocabulary and patterns\n",
      "5. ✓ But would likely have limited fluency due to fragmented exposure\n"
     ]
    }
   ],
   "source": [
    "# Analysis for LLM learning potential\n",
    "print(f\"\\n=== LLM LEARNING POTENTIAL ANALYSIS ===\")\n",
    "\n",
    "def analyze_learning_potential(examples):\n",
    "    \n",
    "    fragment_lengths = []\n",
    "    coherent_fragments = 0\n",
    "    mixed_content = 0\n",
    "    \n",
    "    for example in examples:\n",
    "        text = example['text']\n",
    "        fragment_lengths.append(len(text))\n",
    "        \n",
    "        # Check if fragment is coherent (has sentence structure)\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        complete_sentences = [s for s in sentences if len(s.strip()) > 20]\n",
    "        \n",
    "        if len(complete_sentences) >= 2:\n",
    "            coherent_fragments += 1\n",
    "            \n",
    "        if 'mixed_language' in example['contexts']:\n",
    "            mixed_content += 1\n",
    "    \n",
    "    avg_length = sum(fragment_lengths) / len(fragment_lengths) if fragment_lengths else 0\n",
    "    \n",
    "    print(f\"Average fragment length: {avg_length:.0f} characters\")\n",
    "    print(f\"Coherent fragments (2+ sentences): {coherent_fragments}/{len(examples)} ({coherent_fragments/len(examples)*100:.1f}%)\")\n",
    "    print(f\"Mixed language content: {mixed_content}/{len(examples)} ({mixed_content/len(examples)*100:.1f}%)\")\n",
    "    \n",
    "    # Learning assessment\n",
    "    print(f\"\\n=== LEARNING ASSESSMENT ===\")\n",
    "    if avg_length > 100 and coherent_fragments > len(examples) * 0.3:\n",
    "        print(\"✓ GOOD: Fragments are long enough and coherent enough for learning\")\n",
    "    else:\n",
    "        print(\"✗ LIMITED: Fragments may be too short or incoherent for effective learning\")\n",
    "        \n",
    "    if mixed_content > len(examples) * 0.5:\n",
    "        print(\"✓ GOOD: Mixed language content provides translation context\")\n",
    "    else:\n",
    "        print(\"✗ LIMITED: Little mixed-language content for translation learning\")\n",
    "    \n",
    "    return {\n",
    "        'avg_length': avg_length,\n",
    "        'coherent_ratio': coherent_fragments / len(examples),\n",
    "        'mixed_ratio': mixed_content / len(examples)\n",
    "    }\n",
    "\n",
    "if non_english_examples:\n",
    "    stats = analyze_learning_potential(non_english_examples)\n",
    "    \n",
    "    print(f\"\\n=== CONCLUSIONS ===\")\n",
    "    print(\"Based on this analysis of C4's 'English' dataset:\")\n",
    "    print(\"1. Non-English content DOES persist despite filtering\")\n",
    "    print(\"2. Common contexts: web content, quoted text, proper nouns, mixed language\")\n",
    "    print(\"3. Learning potential depends on:\")\n",
    "    print(f\"   - Fragment coherence: {stats['coherent_ratio']:.1%}\")\n",
    "    print(f\"   - Mixed language context: {stats['mixed_ratio']:.1%}\")\n",
    "    print(f\"   - Average length: {stats['avg_length']:.0f} chars\")\n",
    "    \n",
    "    if stats['coherent_ratio'] > 0.2 and stats['avg_length'] > 50:\n",
    "        print(\"4. ✓ An LLM could potentially learn some vocabulary and patterns\")\n",
    "        print(\"5. ✓ But would likely have limited fluency due to fragmented exposure\")\n",
    "    else:\n",
    "        print(\"4. ✗ Learning would be very limited due to fragmented, short content\")\n",
    "        \n",
    "else:\n",
    "    print(\"No non-English content found in the sampled data.\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"1. The langdetect filtering was very effective for this subset\")\n",
    "    print(\"2. The sample size was too small - try increasing from 1000\")\n",
    "    print(\"3. The confidence threshold (0.3) was too high\")\n",
    "    print(\"Try: increasing sample size, lowering confidence threshold, or using different C4 subsets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
