{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b7f865",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae93f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (0.32.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d39102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9661c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C4 realnewslike dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535b153ef6b841699854d5be8e7c500e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\datasets--allenai--c4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89369e1c6a54ead8db7ffa7934eea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3561abf52ef4f5caebb6ca5650f9769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IterableDataset({\n",
      "    features: ['text', 'timestamp', 'url'],\n",
      "    num_shards: 512\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the realnewslike subset of C4\n",
    "print(\"Loading C4 realnewslike dataset...\")\n",
    "realnewslike = load_dataset(\"allenai/c4\", \"realnewslike\", \n",
    "                           streaming=True, split=\"train\")\n",
    "print(realnewslike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f8036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents...\n",
      "Processed 0 documents...\n",
      "Processed 1000 documents...\n",
      "Processed 2000 documents...\n",
      "Processed 3000 documents...\n",
      "Processed 4000 documents...\n",
      "Processed 5000 documents...\n",
      "Processed 6000 documents...\n",
      "Processed 7000 documents...\n",
      "Processed 8000 documents...\n",
      "Processed 9000 documents...\n",
      "Processed 10000 documents...\n",
      "Processed 11000 documents...\n",
      "Processed 12000 documents...\n",
      "Processed 13000 documents...\n",
      "Processed 14000 documents...\n",
      "Processed 15000 documents...\n",
      "Processed 16000 documents...\n",
      "Processed 17000 documents...\n",
      "Processed 18000 documents...\n",
      "Processed 19000 documents...\n",
      "Processed 20000 documents...\n",
      "Processed 21000 documents...\n",
      "Processed 22000 documents...\n",
      "Processed 23000 documents...\n",
      "Processed 24000 documents...\n",
      "Processed 25000 documents...\n",
      "Processed 26000 documents...\n",
      "Processed 27000 documents...\n",
      "Processed 28000 documents...\n",
      "Processed 29000 documents...\n",
      "Processed 30000 documents...\n",
      "Processed 31000 documents...\n",
      "Processed 32000 documents...\n",
      "Processed 33000 documents...\n",
      "Processed 34000 documents...\n",
      "Processed 35000 documents...\n",
      "Processed 36000 documents...\n",
      "Processed 37000 documents...\n",
      "Processed 38000 documents...\n",
      "Processed 39000 documents...\n",
      "Processed 40000 documents...\n",
      "Processed 41000 documents...\n",
      "Processed 42000 documents...\n",
      "Processed 43000 documents...\n",
      "Processed 44000 documents...\n",
      "Processed 45000 documents...\n",
      "Processed 46000 documents...\n",
      "Processed 47000 documents...\n",
      "Processed 48000 documents...\n",
      "Processed 49000 documents...\n",
      "Processed 50000 documents...\n",
      "\n",
      "Processing complete! Found 203785 unique words.\n",
      "\n",
      "Top 50 most frequent words:\n",
      "--------------------------------------------------\n",
      "said                    91502\n",
      "who                     56224\n",
      "one                     50130\n",
      "which                   48360\n",
      "new                     47293\n",
      "also                    43971\n",
      "out                     40323\n",
      "there                   35599\n",
      "what                    35152\n",
      "people                  33821\n",
      "time                    32124\n",
      "first                   31349\n",
      "two                     30483\n",
      "like                    29662\n",
      "year                    29270\n",
      "last                    26255\n",
      "years                   25945\n",
      "get                     22375\n",
      "while                   22185\n",
      "many                    20962\n",
      "because                 20168\n",
      "even                    19730\n",
      "state                   19725\n",
      "make                    19591\n",
      "through                 18769\n",
      "before                  18426\n",
      "back                    18120\n",
      "then                    17515\n",
      "work                    17377\n",
      "well                    17170\n",
      "way                     16912\n",
      "three                   16576\n",
      "made                    16193\n",
      "during                  15933\n",
      "says                    15885\n",
      "million                 15850\n",
      "much                    15755\n",
      "world                   15593\n",
      "since                   15546\n",
      "against                 15475\n",
      "government              15437\n",
      "take                    15326\n",
      "day                     15214\n",
      "see                     15170\n",
      "still                   15128\n",
      "home                    15035\n",
      "company                 14806\n",
      "down                    14785\n",
      "president               14731\n",
      "city                    14689\n",
      "\n",
      "============================================================\n",
      "TOPIC ANALYSIS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define common stop words to remove\n",
    "stop_words = {\n",
    "    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "    'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "    'should', 'may', 'might', 'must', 'can', 'shall', 'this', 'that',\n",
    "    'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "    'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its',\n",
    "    'our', 'their', 'from', 'up', 'about', 'into', 'over', 'after',\n",
    "    'as', 'so', 'if', 'than', 'when', 'where', 'why', 'how', 'all',\n",
    "    'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "    'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'than', 'too',\n",
    "    'very', 'just', 'now'\n",
    "}\n",
    "\n",
    "# Initialize word counter\n",
    "word_counter = Counter()\n",
    "\n",
    "# Process documents\n",
    "print(\"Processing documents...\")\n",
    "for i, example in enumerate(realnewslike):\n",
    "    # Extract text and convert to lowercase\n",
    "    text = example[\"text\"].lower()\n",
    "    \n",
    "    # Split by whitespace to get words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter words: remove stop words and keep only alphabetic words\n",
    "    filtered_words = [\n",
    "        word.strip('.,!?\";:()[]{}') for word in words \n",
    "        if word.strip('.,!?\";:()[]{}').isalpha() \n",
    "        and word.strip('.,!?\";:()[]{}') not in stop_words\n",
    "        and len(word.strip('.,!?\";:()[]{}')) > 2  # Remove very short words\n",
    "    ]\n",
    "    \n",
    "    # Update counter\n",
    "    word_counter.update(filtered_words)\n",
    "    \n",
    "    # Print progress every 1000 documents\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processed {i} documents...\")\n",
    "    \n",
    "    # Limit processing for demonstration (remove this for full dataset)\n",
    "    if i == 50000:  # Process 50,000 documents\n",
    "        break\n",
    "\n",
    "print(f\"\\nProcessing complete! Found {len(word_counter)} unique words.\")\n",
    "\n",
    "# Display most common words\n",
    "print(\"\\nTop 50 most frequent words:\")\n",
    "print(\"-\" * 50)\n",
    "for word, count in word_counter.most_common(50):\n",
    "    print(f\"{word:<20} {count:>8}\")\n",
    "\n",
    "# Analyze topics by looking at word categories\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOPIC ANALYSIS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1b5dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology words total count: 44615\n",
      "Politics words total count: 76658\n",
      "Sports words total count: 67532\n",
      "Business/Economy words total count: 95290\n",
      "Health words total count: 27180\n",
      "Entertainment words total count: 37771\n",
      "\n",
      "Total words processed: 11,842,240\n",
      "\n",
      "Topic representation (as percentage of total words):\n",
      "Technology: 0.38%\n",
      "Politics: 0.65%\n",
      "Sports: 0.57%\n",
      "Business/Economy: 0.80%\n",
      "Health: 0.23%\n",
      "Entertainment: 0.32%\n",
      "\n",
      "============================================================\n",
      "OBSERVATIONS ON TOPIC REPRESENTATION\n",
      "============================================================\n",
      "Potentially underrepresented topics:\n",
      "science           2760 (0.023%)\n",
      "research          6334 (0.053%)\n",
      "education         5570 (0.047%)\n",
      "environment       2614 (0.022%)\n",
      "climate           2365 (0.020%)\n",
      "art               3233 (0.027%)\n",
      "culture           2148 (0.018%)\n",
      "history           5291 (0.045%)\n",
      "philosophy         455 (0.004%)\n",
      "literature         404 (0.003%)\n",
      "book              4736 (0.040%)\n",
      "academic           964 (0.008%)\n",
      "university        8021 (0.068%)\n",
      "\n",
      "Note: This analysis is based on a sample of the dataset.\n",
      "For complete analysis, remove the iteration limit and process the full dataset.\n"
     ]
    }
   ],
   "source": [
    "# Technology words\n",
    "tech_words = ['technology', 'computer', 'internet', 'digital', 'software', \n",
    "              'app', 'smartphone', 'online', 'website', 'data', 'ai', \n",
    "              'artificial', 'intelligence', 'tech', 'cyber', 'bitcoin']\n",
    "tech_count = sum(word_counter[word] for word in tech_words)\n",
    "\n",
    "# Politics words\n",
    "politics_words = ['government', 'president', 'political', 'election', 'vote',\n",
    "                  'congress', 'senate', 'democrat', 'republican', 'policy',\n",
    "                  'law', 'legislation', 'campaign', 'politician']\n",
    "politics_count = sum(word_counter[word] for word in politics_words)\n",
    "\n",
    "# Sports words\n",
    "sports_words = ['game', 'team', 'player', 'season', 'football', 'basketball',\n",
    "                'baseball', 'soccer', 'sports', 'championship', 'coach',\n",
    "                'score', 'win', 'league']\n",
    "sports_count = sum(word_counter[word] for word in sports_words)\n",
    "\n",
    "# Business/Economy words\n",
    "business_words = ['business', 'company', 'market', 'economy', 'economic',\n",
    "                  'financial', 'money', 'price', 'cost', 'profit', 'investment',\n",
    "                  'stock', 'bank', 'trade', 'industry']\n",
    "business_count = sum(word_counter[word] for word in business_words)\n",
    "\n",
    "# Health words\n",
    "health_words = ['health', 'medical', 'hospital', 'doctor', 'patient',\n",
    "                'treatment', 'medicine', 'disease', 'virus', 'vaccine',\n",
    "                'covid', 'pandemic', 'healthcare']\n",
    "health_count = sum(word_counter[word] for word in health_words)\n",
    "\n",
    "# Entertainment words  \n",
    "entertainment_words = ['movie', 'film', 'music', 'show', 'entertainment',\n",
    "                      'celebrity', 'actor', 'singer', 'hollywood', 'television',\n",
    "                      'tv', 'series', 'netflix']\n",
    "entertainment_count = sum(word_counter[word] for word in entertainment_words)\n",
    "\n",
    "print(f\"Technology words total count: {tech_count}\")\n",
    "print(f\"Politics words total count: {politics_count}\")\n",
    "print(f\"Sports words total count: {sports_count}\")\n",
    "print(f\"Business/Economy words total count: {business_count}\")\n",
    "print(f\"Health words total count: {health_count}\")\n",
    "print(f\"Entertainment words total count: {entertainment_count}\")\n",
    "\n",
    "# Calculate total processed words\n",
    "total_words = sum(word_counter.values())\n",
    "print(f\"\\nTotal words processed: {total_words:,}\")\n",
    "\n",
    "print(\"\\nTopic representation (as percentage of total words):\")\n",
    "print(f\"Technology: {(tech_count/total_words)*100:.2f}%\")\n",
    "print(f\"Politics: {(politics_count/total_words)*100:.2f}%\")\n",
    "print(f\"Sports: {(sports_count/total_words)*100:.2f}%\")\n",
    "print(f\"Business/Economy: {(business_count/total_words)*100:.2f}%\")\n",
    "print(f\"Health: {(health_count/total_words)*100:.2f}%\")\n",
    "print(f\"Entertainment: {(entertainment_count/total_words)*100:.2f}%\")\n",
    "\n",
    "# Look for potentially underrepresented topics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OBSERVATIONS ON TOPIC REPRESENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "underrepresented_words = ['science', 'research', 'education', 'environment',\n",
    "                         'climate', 'art', 'culture', 'history', 'philosophy',\n",
    "                         'literature', 'book', 'academic', 'university']\n",
    "\n",
    "print(\"Potentially underrepresented topics:\")\n",
    "for word in underrepresented_words:\n",
    "    count = word_counter[word]\n",
    "    percentage = (count/total_words)*100 if total_words > 0 else 0\n",
    "    print(f\"{word:<15} {count:>6} ({percentage:.3f}%)\")\n",
    "\n",
    "print(\"\\nNote: This analysis is based on a sample of the dataset.\")\n",
    "print(\"For complete analysis, remove the iteration limit and process the full dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6834a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
