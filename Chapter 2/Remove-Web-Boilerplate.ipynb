{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a930188c",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f15d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import html\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c220de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample HTML content\n",
    "SAMPLE_HTML = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>BBC News - Wikipedia</title>\n",
    "    <meta name=\"description\" content=\"BBC News article\">\n",
    "</head>\n",
    "<body>\n",
    "    <nav>\n",
    "        <ul>\n",
    "            <li><a href=\"/home\">Home</a></li>\n",
    "            <li><a href=\"/news\">News</a></li>\n",
    "        </ul>\n",
    "    </nav>\n",
    "    \n",
    "    <div class=\"main-content\">\n",
    "        <h1>BBC News</h1>\n",
    "        \n",
    "        <p>BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.</p>\n",
    "        \n",
    "        <div class=\"infobox\">\n",
    "            <table>\n",
    "                <tr><td>Founded</td><td>1922</td></tr>\n",
    "                <tr><td>Headquarters</td><td>Broadcasting House, London</td></tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <h2>History</h2>\n",
    "        \n",
    "        <p>The British Broadcasting Company broadcast its first radio bulletin from radio station 2LO on 14 November 1922. Wishing to avoid competition, newspaper publishers persuaded the government to ban the BBC from broadcasting news before 7 p.m., and to force it to use wire service copy instead of reporting on its own.</p>\n",
    "        \n",
    "        <p>The BBC gradually gained the right to edit the copy and, in 1934, created its own news operation. However, it could not broadcast news before 6 p.m. until World War II.</p>\n",
    "        \n",
    "        <div class=\"sidebar\">\n",
    "            <h3>Related Articles</h3>\n",
    "            <ul>\n",
    "                <li><a href=\"/bbc\">BBC</a></li>\n",
    "                <li><a href=\"/news\">News Broadcasting</a></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Television News</h2>\n",
    "        \n",
    "        <p>Television news, although physically separate from its radio counterpart, was still firmly under radio news' control in the 1950s. Correspondents provided reports for both outlets, and the first televised bulletin, shown on 5 July 1954 on the then BBC television service and presented by Richard Baker, involved his providing narration off-screen while stills were shown.</p>\n",
    "        \n",
    "    </div>\n",
    "    \n",
    "    <footer>\n",
    "        <p>© 2025 Wikipedia. All rights reserved.</p>\n",
    "        <div class=\"footer-links\">\n",
    "            <a href=\"/privacy\">Privacy Policy</a>\n",
    "            <a href=\"/terms\">Terms of Service</a>\n",
    "        </div>\n",
    "    </footer>\n",
    "    \n",
    "    <script>\n",
    "        // Analytics tracking code\n",
    "        console.log(\"Page loaded\");\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67274d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextExtractor:\n",
    "    \n",
    "    def __init__(self, html_content):\n",
    "        self.html_content = html_content\n",
    "        self.soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    def basic_beautifulsoup_extraction(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"1. BASIC BEAUTIFULSOUP EXTRACTION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Simple get_text() approach\n",
    "        raw_text = self.soup.get_text()\n",
    "        print(\"Raw text extraction:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(raw_text[:500] + \"...\" if len(raw_text) > 500 else raw_text)\n",
    "        print(\"\\nISSUES IDENTIFIED:\")\n",
    "        print(\"- Includes navigation menu text\")\n",
    "        print(\"- Includes footer content\") \n",
    "        print(\"- Includes sidebar content\")\n",
    "        print(\"- Poor spacing and formatting\")\n",
    "        print(\"- No content prioritization\")\n",
    "        \n",
    "        return raw_text\n",
    "    \n",
    "    def improved_beautifulsoup_extraction(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"2. IMPROVED BEAUTIFULSOUP EXTRACTION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        soup_copy = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        \n",
    "        # Remove navigation, footer, sidebar, scripts, style\n",
    "        unwanted_tags = ['nav', 'footer', 'script', 'style', 'aside']\n",
    "        unwanted_classes = ['sidebar', 'footer-links', 'infobox']\n",
    "        \n",
    "        for tag in unwanted_tags:\n",
    "            for element in soup_copy.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        for class_name in unwanted_classes:\n",
    "            for element in soup_copy.find_all(class_=class_name):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Focus on main content\n",
    "        main_content = soup_copy.find('div', class_='main-content')\n",
    "        if main_content:\n",
    "            # Extract text with better spacing\n",
    "            text = main_content.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            text = soup_copy.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up excessive whitespace\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        print(\"Improved extraction:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(text)\n",
    "        print(\"\\nIMPROVEMENTS:\")\n",
    "        print(\"✓ Removed navigation and footer\")\n",
    "        print(\"✓ Removed sidebar content\")\n",
    "        print(\"✓ Better text spacing\")\n",
    "        print(\"✓ Focused on main content area\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def justext_like_approach(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"3. JUSTEXT-LIKE HEURISTIC APPROACH\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Justext uses language models and statistical features\n",
    "        # We'll simulate some basic heuristics\n",
    "        \n",
    "        soup_copy = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        \n",
    "        # Score paragraphs based on various features\n",
    "        paragraphs = []\n",
    "        \n",
    "        for p in soup_copy.find_all(['p', 'div']):\n",
    "            if not p.get_text(strip=True):\n",
    "                continue\n",
    "                \n",
    "            text = p.get_text(strip=True)\n",
    "            \n",
    "            # Calculate heuristic scores\n",
    "            score = 0\n",
    "            \n",
    "            # Length score (prefer medium-length paragraphs)\n",
    "            length = len(text)\n",
    "            if 50 <= length <= 500:\n",
    "                score += 2\n",
    "            elif length > 500:\n",
    "                score += 1\n",
    "            \n",
    "            # Link density (prefer low link density)\n",
    "            links = p.find_all('a')\n",
    "            link_text_length = sum(len(a.get_text()) for a in links)\n",
    "            link_density = link_text_length / length if length > 0 else 1\n",
    "            \n",
    "            if link_density < 0.3:\n",
    "                score += 2\n",
    "            elif link_density < 0.5:\n",
    "                score += 1\n",
    "            \n",
    "            # Position score (prefer content in main areas)\n",
    "            if p.find_parent('nav') or p.find_parent('footer'):\n",
    "                score -= 3\n",
    "            if 'main' in str(p.get('class', [])).lower():\n",
    "                score += 2\n",
    "            \n",
    "            # Punctuation score (real sentences have punctuation)\n",
    "            if re.search(r'[.!?]', text):\n",
    "                score += 1\n",
    "            \n",
    "            # Word count (prefer substantial content)\n",
    "            word_count = len(text.split())\n",
    "            if word_count > 10:\n",
    "                score += 1\n",
    "            \n",
    "            paragraphs.append((score, text, p))\n",
    "        \n",
    "        # Select paragraphs with positive scores\n",
    "        good_paragraphs = [text for score, text, elem in paragraphs if score > 0]\n",
    "        \n",
    "        result = '\\n\\n'.join(good_paragraphs)\n",
    "        \n",
    "        print(\"Heuristic-based extraction:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(result)\n",
    "        print(f\"\\nHEURISTICS APPLIED:\")\n",
    "        print(f\"- Scored {len(paragraphs)} text blocks\")\n",
    "        print(f\"- Selected {len(good_paragraphs)} high-quality paragraphs\")\n",
    "        print(\"- Length-based filtering\")\n",
    "        print(\"- Link density analysis\")\n",
    "        print(\"- Position-based scoring\")\n",
    "        print(\"- Punctuation analysis\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def readability_like_approach(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"4. READABILITY-LIKE ALGORITHM\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        soup_copy = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        \n",
    "        # Score elements based on Readability-like heuristics\n",
    "        scored_elements = []\n",
    "        \n",
    "        for elem in soup_copy.find_all(['div', 'article', 'section', 'main']):\n",
    "            if not elem.get_text(strip=True):\n",
    "                continue\n",
    "            \n",
    "            score = 0\n",
    "            text = elem.get_text(strip=True)\n",
    "            \n",
    "            # Positive scoring\n",
    "            if elem.name in ['article', 'main']:\n",
    "                score += 25\n",
    "            \n",
    "            # Class and ID based scoring\n",
    "            class_id_text = ' '.join([\n",
    "                ' '.join(elem.get('class', [])),\n",
    "                elem.get('id', '')\n",
    "            ]).lower()\n",
    "            \n",
    "            positive_keywords = ['content', 'main', 'article', 'entry', 'post']\n",
    "            negative_keywords = ['nav', 'sidebar', 'footer', 'ad', 'comment', 'widget']\n",
    "            \n",
    "            for keyword in positive_keywords:\n",
    "                if keyword in class_id_text:\n",
    "                    score += 25\n",
    "            \n",
    "            for keyword in negative_keywords:\n",
    "                if keyword in class_id_text:\n",
    "                    score -= 25\n",
    "            \n",
    "            # Content-based scoring\n",
    "            p_count = len(elem.find_all('p'))\n",
    "            score += min(p_count * 5, 50)  # Cap at 50\n",
    "            \n",
    "            # Text length scoring\n",
    "            if len(text) > 200:\n",
    "                score += 20\n",
    "            \n",
    "            # Link density penalty\n",
    "            links = elem.find_all('a')\n",
    "            link_text = ' '.join(a.get_text() for a in links)\n",
    "            if len(text) > 0:\n",
    "                link_density = len(link_text) / len(text)\n",
    "                score -= int(link_density * 50)\n",
    "            \n",
    "            scored_elements.append((score, elem, text))\n",
    "        \n",
    "        # Get the highest scoring element\n",
    "        if scored_elements:\n",
    "            best_score, best_elem, best_text = max(scored_elements, key=lambda x: x[0])\n",
    "            \n",
    "            # Extract clean text from the best element\n",
    "            for unwanted in best_elem.find_all(['script', 'style', 'nav', 'footer']):\n",
    "                unwanted.decompose()\n",
    "            \n",
    "            result = best_elem.get_text(separator='\\n', strip=True)\n",
    "            result = re.sub(r'\\n\\s*\\n', '\\n\\n', result)\n",
    "            \n",
    "            print(\"Readability-style extraction:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(result)\n",
    "            print(f\"\\nALGORITHM RESULTS:\")\n",
    "            print(f\"- Best element score: {best_score}\")\n",
    "            print(f\"- Selected element: {best_elem.name}\")\n",
    "            print(f\"- Class/ID: {best_elem.get('class', [])} / {best_elem.get('id', 'None')}\")\n",
    "            print(\"- Applied content scoring heuristics\")\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        return \"No suitable content found\"\n",
    "    \n",
    "    def newspaper_like_approach(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"5. NEWSPAPER3K-LIKE APPROACH\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        soup_copy = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        \n",
    "        # Extract article metadata\n",
    "        title = soup_copy.find('h1')\n",
    "        title_text = title.get_text(strip=True) if title else \"No title found\"\n",
    "        \n",
    "        # Look for article-like content structures\n",
    "        article_candidates = []\n",
    "        \n",
    "        # Find content containers\n",
    "        containers = soup_copy.find_all(['article', 'div', 'section'], \n",
    "                                      class_=re.compile(r'(content|main|article|entry)', re.I))\n",
    "        \n",
    "        if not containers:\n",
    "            # Fallback to divs with substantial content\n",
    "            containers = [div for div in soup_copy.find_all('div') \n",
    "                         if len(div.get_text(strip=True)) > 200]\n",
    "        \n",
    "        for container in containers:\n",
    "            # Remove noise\n",
    "            for noise in container.find_all(['nav', 'footer', 'aside', 'script', 'style']):\n",
    "                noise.decompose()\n",
    "            \n",
    "            # Extract paragraphs\n",
    "            paragraphs = container.find_all('p')\n",
    "            if len(paragraphs) >= 2:  # Articles typically have multiple paragraphs\n",
    "                text_blocks = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "                if text_blocks:\n",
    "                    article_candidates.append('\\n\\n'.join(text_blocks))\n",
    "        \n",
    "        # Select the longest article candidate\n",
    "        if article_candidates:\n",
    "            article_text = max(article_candidates, key=len)\n",
    "        else:\n",
    "            article_text = \"No article content identified\"\n",
    "        \n",
    "        print(f\"Title: {title_text}\")\n",
    "        print(\"\\nArticle text:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(article_text)\n",
    "        print(f\"\\nNEWSPAPER3K-STYLE FEATURES:\")\n",
    "        print(f\"- Title extraction: {'✓' if title else '✗'}\")\n",
    "        print(f\"- Found {len(article_candidates)} article candidates\")\n",
    "        print(\"- Paragraph-based content extraction\")\n",
    "        print(\"- Container detection by class names\")\n",
    "        \n",
    "        return {\"title\": title_text, \"text\": article_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_heuristics_needed():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ADDITIONAL HEURISTICS NEEDED FOR PRODUCTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    heuristics = [\n",
    "        {\n",
    "            \"category\": \"Language Detection\",\n",
    "            \"techniques\": [\n",
    "                \"Language model-based text classification\",\n",
    "                \"Character frequency analysis\", \n",
    "                \"Stop word detection\",\n",
    "                \"N-gram analysis\"\n",
    "            ],\n",
    "            \"why\": \"To filter content in the target language and improve accuracy\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Content Structure Analysis\", \n",
    "            \"techniques\": [\n",
    "                \"DOM tree depth analysis\",\n",
    "                \"Sibling element similarity\",\n",
    "                \"CSS class pattern matching\",\n",
    "                \"HTML5 semantic element detection\"\n",
    "            ],\n",
    "            \"why\": \"Modern websites use consistent CSS patterns for content\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Text Quality Metrics\",\n",
    "            \"techniques\": [\n",
    "                \"Sentence boundary detection\",\n",
    "                \"Punctuation density analysis\",\n",
    "                \"Capitalization patterns\",\n",
    "                \"Word frequency distribution\",\n",
    "                \"Text coherence scoring\"\n",
    "            ],\n",
    "            \"why\": \"To distinguish real content from navigation/boilerplate text\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Visual Layout Heuristics\",\n",
    "            \"techniques\": [\n",
    "                \"Font size analysis (via CSS)\",\n",
    "                \"Text block positioning\",\n",
    "                \"White space analysis\", \n",
    "                \"Reading flow patterns\"\n",
    "            ],\n",
    "            \"why\": \"Important content typically has distinct visual styling\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Site-Specific Rules\",\n",
    "            \"techniques\": [\n",
    "                \"Domain-specific extraction rules\",\n",
    "                \"CMS pattern recognition\",\n",
    "                \"Publisher-specific templates\",\n",
    "                \"Microdata/JSON-LD parsing\"\n",
    "            ],\n",
    "            \"why\": \"Many sites follow consistent patterns that can be learned\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Machine Learning Approaches\",\n",
    "            \"techniques\": [\n",
    "                \"Content vs. boilerplate classification\",\n",
    "                \"Feature engineering from DOM properties\",\n",
    "                \"Neural content extraction models\",\n",
    "                \"Transfer learning from labeled data\"\n",
    "            ],\n",
    "            \"why\": \"Data-driven approaches can generalize better than hand-crafted rules\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for h in heuristics:\n",
    "        print(f\"\\n{h['category']}:\")\n",
    "        print(f\"  Why needed: {h['why']}\")\n",
    "        print(\"  Techniques:\")\n",
    "        for technique in h['techniques']:\n",
    "            print(f\"    - {technique}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_library_performance():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LIBRARY COMPARISON & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    libraries = [\n",
    "        {\n",
    "            \"name\": \"Justext\",\n",
    "            \"pros\": [\"Language-aware\", \"Statistical approach\", \"Good at removing boilerplate\", \"Handles multiple languages\"],\n",
    "            \"cons\": [\"Requires language specification\", \"May be too aggressive\", \"Less control over rules\"],\n",
    "            \"best_for\": \"Multi-language news sites, blogs with lots of navigation\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"newspaper3k\", \n",
    "            \"pros\": [\"Article-focused\", \"Extracts metadata\", \"Handles news sites well\", \"Easy to use\"],\n",
    "            \"cons\": [\"News-specific\", \"Less customizable\", \"May miss non-article content\"],\n",
    "            \"best_for\": \"News articles, blog posts, journalistic content\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Trafilatura\",\n",
    "            \"pros\": [\"Fast\", \"High precision\", \"Handles many site types\", \"Good recall\"],\n",
    "            \"cons\": [\"Less metadata extraction\", \"Newer library\", \"May need fine-tuning\"],\n",
    "            \"best_for\": \"General web scraping, academic content, diverse site types\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Readability/readabilipy\",\n",
    "            \"pros\": [\"Based on Mozilla algorithm\", \"Handles complex layouts\", \"Good for articles\"],\n",
    "            \"cons\": [\"May be slower\", \"Less control\", \"Originally designed for reading view\"],\n",
    "            \"best_for\": \"Article reading, content curation, reading applications\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"BeautifulSoup + Custom Rules\",\n",
    "            \"pros\": [\"Full control\", \"Site-specific optimization\", \"Highly customizable\"],\n",
    "            \"cons\": [\"Requires manual tuning\", \"Site-specific\", \"Time-intensive\"],\n",
    "            \"best_for\": \"Specific sites, custom requirements, maximum control needed\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for lib in libraries:\n",
    "        print(f\"\\n{lib['name']}:\")\n",
    "        print(f\"  Best for: {lib['best_for']}\")\n",
    "        print(\"  Pros:\", ', '.join(lib['pros']))\n",
    "        print(\"  Cons:\", ', '.join(lib['cons']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59027b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_world_example_analysis():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"REAL-WORLD EXAMPLE: BBC NEWS WIKIPEDIA PAGE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Raw content from the fetched BBC News Wikipedia page\n",
    "    raw_sample = '''BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage. The service has over 5,500 journalists working across its output including in 50 foreign news bureaus where more than 250 foreign correspondents are stationed.'''\n",
    "    \n",
    "    print(\"EXTRACTED CONTENT (after processing):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(raw_sample)\n",
    "    \n",
    "    print(\"\\nWHY THIS EXTRACTION WORKED:\")\n",
    "    print(\"✓ Focused on main article content\") \n",
    "    print(\"✓ Removed navigation and metadata\")\n",
    "    print(\"✓ Preserved paragraph structure\")\n",
    "    print(\"✓ Filtered out Wikipedia-specific elements\")\n",
    "    \n",
    "    print(\"\\nCHALLENGES ENCOUNTERED:\")\n",
    "    print(\"• Complex nested HTML structure\")\n",
    "    print(\"• Multiple content sections mixed with metadata\")\n",
    "    print(\"• Navigation links embedded within content\")\n",
    "    print(\"• Table data mixed with article text\")\n",
    "    print(\"• Citation markers and edit links throughout\")\n",
    "    \n",
    "    return raw_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_recommendations():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PRODUCTION RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    recommendations = [\n",
    "        {\n",
    "            \"approach\": \"Multi-Library Pipeline\",\n",
    "            \"description\": \"Use 2-3 libraries in sequence for better results\",\n",
    "            \"example\": \"Trafilatura → newspaper3k → custom cleanup\",\n",
    "            \"when\": \"High-volume, diverse content sources\"\n",
    "        },\n",
    "        {\n",
    "            \"approach\": \"Site-Specific Rules\",\n",
    "            \"description\": \"Develop custom extraction rules for major sources\",\n",
    "            \"example\": \"BBC: target .story-content, CNN: target .article-body\",\n",
    "            \"when\": \"Limited number of high-priority sources\"\n",
    "        },\n",
    "        {\n",
    "            \"approach\": \"Machine Learning Classification\",\n",
    "            \"description\": \"Train models to classify content vs. boilerplate\",\n",
    "            \"example\": \"Features: text length, link density, CSS classes\",\n",
    "            \"when\": \"Large-scale, automated processing\"\n",
    "        },\n",
    "        {\n",
    "            \"approach\": \"Hybrid Heuristic System\",\n",
    "            \"description\": \"Combine rule-based and statistical approaches\",\n",
    "            \"example\": \"Content scoring + language detection + DOM analysis\",\n",
    "            \"when\": \"Balance between accuracy and maintainability\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(f\"\\n{rec['approach']}:\")\n",
    "        print(f\"  Description: {rec['description']}\")\n",
    "        print(f\"  Example: {rec['example']}\")\n",
    "        print(f\"  Best for: {rec['when']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984fd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEB TEXT EXTRACTION ANALYSIS\n",
      "============================================================\n",
      "Analysis of text extraction techniques using real-world examples\n",
      "============================================================\n",
      "1. BASIC BEAUTIFULSOUP EXTRACTION\n",
      "============================================================\n",
      "Raw text extraction:\n",
      "------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BBC News - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "News\n",
      "\n",
      "\n",
      "\n",
      "BBC News\n",
      "BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.\n",
      "\n",
      "\n",
      "Founded1922\n",
      "HeadquartersBroadcasting House, London\n",
      "\n",
      "\n",
      "History\n",
      "The British Broad...\n",
      "\n",
      "ISSUES IDENTIFIED:\n",
      "- Includes navigation menu text\n",
      "- Includes footer content\n",
      "- Includes sidebar content\n",
      "- Poor spacing and formatting\n",
      "- No content prioritization\n",
      "\n",
      "============================================================\n",
      "2. IMPROVED BEAUTIFULSOUP EXTRACTION\n",
      "============================================================\n",
      "Improved extraction:\n",
      "------------------------------\n",
      "BBC News\n",
      "BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.\n",
      "History\n",
      "The British Broadcasting Company broadcast its first radio bulletin from radio station 2LO on 14 November 1922. Wishing to avoid competition, newspaper publishers persuaded the government to ban the BBC from broadcasting news before 7 p.m., and to force it to use wire service copy instead of reporting on its own.\n",
      "The BBC gradually gained the right to edit the copy and, in 1934, created its own news operation. However, it could not broadcast news before 6 p.m. until World War II.\n",
      "Television News\n",
      "Television news, although physically separate from its radio counterpart, was still firmly under radio news' control in the 1950s. Correspondents provided reports for both outlets, and the first televised bulletin, shown on 5 July 1954 on the then BBC television service and presented by Richard Baker, involved his providing narration off-screen while stills were shown.\n",
      "\n",
      "IMPROVEMENTS:\n",
      "✓ Removed navigation and footer\n",
      "✓ Removed sidebar content\n",
      "✓ Better text spacing\n",
      "✓ Focused on main content area\n",
      "\n",
      "============================================================\n",
      "3. JUSTEXT-LIKE HEURISTIC APPROACH\n",
      "============================================================\n",
      "Heuristic-based extraction:\n",
      "------------------------------\n",
      "BBC NewsBBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.Founded1922HeadquartersBroadcasting House, LondonHistoryThe British Broadcasting Company broadcast its first radio bulletin from radio station 2LO on 14 November 1922. Wishing to avoid competition, newspaper publishers persuaded the government to ban the BBC from broadcasting news before 7 p.m., and to force it to use wire service copy instead of reporting on its own.The BBC gradually gained the right to edit the copy and, in 1934, created its own news operation. However, it could not broadcast news before 6 p.m. until World War II.Related ArticlesBBCNews BroadcastingTelevision NewsTelevision news, although physically separate from its radio counterpart, was still firmly under radio news' control in the 1950s. Correspondents provided reports for both outlets, and the first televised bulletin, shown on 5 July 1954 on the then BBC television service and presented by Richard Baker, involved his providing narration off-screen while stills were shown.\n",
      "\n",
      "BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.\n",
      "\n",
      "Founded1922HeadquartersBroadcasting House, London\n",
      "\n",
      "The British Broadcasting Company broadcast its first radio bulletin from radio station 2LO on 14 November 1922. Wishing to avoid competition, newspaper publishers persuaded the government to ban the BBC from broadcasting news before 7 p.m., and to force it to use wire service copy instead of reporting on its own.\n",
      "\n",
      "The BBC gradually gained the right to edit the copy and, in 1934, created its own news operation. However, it could not broadcast news before 6 p.m. until World War II.\n",
      "\n",
      "Television news, although physically separate from its radio counterpart, was still firmly under radio news' control in the 1950s. Correspondents provided reports for both outlets, and the first televised bulletin, shown on 5 July 1954 on the then BBC television service and presented by Richard Baker, involved his providing narration off-screen while stills were shown.\n",
      "\n",
      "HEURISTICS APPLIED:\n",
      "- Scored 9 text blocks\n",
      "- Selected 6 high-quality paragraphs\n",
      "- Length-based filtering\n",
      "- Link density analysis\n",
      "- Position-based scoring\n",
      "- Punctuation analysis\n",
      "\n",
      "============================================================\n",
      "4. READABILITY-LIKE ALGORITHM\n",
      "============================================================\n",
      "Readability-style extraction:\n",
      "------------------------------\n",
      "BBC News\n",
      "BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.\n",
      "Founded\n",
      "1922\n",
      "Headquarters\n",
      "Broadcasting House, London\n",
      "History\n",
      "The British Broadcasting Company broadcast its first radio bulletin from radio station 2LO on 14 November 1922. Wishing to avoid competition, newspaper publishers persuaded the government to ban the BBC from broadcasting news before 7 p.m., and to force it to use wire service copy instead of reporting on its own.\n",
      "The BBC gradually gained the right to edit the copy and, in 1934, created its own news operation. However, it could not broadcast news before 6 p.m. until World War II.\n",
      "Related Articles\n",
      "BBC\n",
      "News Broadcasting\n",
      "Television News\n",
      "Television news, although physically separate from its radio counterpart, was still firmly under radio news' control in the 1950s. Correspondents provided reports for both outlets, and the first televised bulletin, shown on 5 July 1954 on the then BBC television service and presented by Richard Baker, involved his providing narration off-screen while stills were shown.\n",
      "\n",
      "ALGORITHM RESULTS:\n",
      "- Best element score: 90\n",
      "- Selected element: div\n",
      "- Class/ID: ['main-content'] / None\n",
      "- Applied content scoring heuristics\n",
      "\n",
      "============================================================\n",
      "5. NEWSPAPER3K-LIKE APPROACH\n",
      "============================================================\n",
      "Title: BBC News\n",
      "\n",
      "Article text:\n",
      "------------------------------\n",
      "BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.\n",
      "\n",
      "The British Broadcasting Company broadcast its first radio bulletin from radio station 2LO on 14 November 1922. Wishing to avoid competition, newspaper publishers persuaded the government to ban the BBC from broadcasting news before 7 p.m., and to force it to use wire service copy instead of reporting on its own.\n",
      "\n",
      "The BBC gradually gained the right to edit the copy and, in 1934, created its own news operation. However, it could not broadcast news before 6 p.m. until World War II.\n",
      "\n",
      "Television news, although physically separate from its radio counterpart, was still firmly under radio news' control in the 1950s. Correspondents provided reports for both outlets, and the first televised bulletin, shown on 5 July 1954 on the then BBC television service and presented by Richard Baker, involved his providing narration off-screen while stills were shown.\n",
      "\n",
      "NEWSPAPER3K-STYLE FEATURES:\n",
      "- Title extraction: ✓\n",
      "- Found 1 article candidates\n",
      "- Paragraph-based content extraction\n",
      "- Container detection by class names\n",
      "\n",
      "============================================================\n",
      "REAL-WORLD EXAMPLE: BBC NEWS WIKIPEDIA PAGE\n",
      "============================================================\n",
      "EXTRACTED CONTENT (after processing):\n",
      "----------------------------------------\n",
      "BBC News is an operational business division of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage. The service has over 5,500 journalists working across its output including in 50 foreign news bureaus where more than 250 foreign correspondents are stationed.\n",
      "\n",
      "WHY THIS EXTRACTION WORKED:\n",
      "✓ Focused on main article content\n",
      "✓ Removed navigation and metadata\n",
      "✓ Preserved paragraph structure\n",
      "✓ Filtered out Wikipedia-specific elements\n",
      "\n",
      "CHALLENGES ENCOUNTERED:\n",
      "• Complex nested HTML structure\n",
      "• Multiple content sections mixed with metadata\n",
      "• Navigation links embedded within content\n",
      "• Table data mixed with article text\n",
      "• Citation markers and edit links throughout\n",
      "\n",
      "============================================================\n",
      "ADDITIONAL HEURISTICS NEEDED FOR PRODUCTION\n",
      "============================================================\n",
      "\n",
      "Language Detection:\n",
      "  Why needed: To filter content in the target language and improve accuracy\n",
      "  Techniques:\n",
      "    - Language model-based text classification\n",
      "    - Character frequency analysis\n",
      "    - Stop word detection\n",
      "    - N-gram analysis\n",
      "\n",
      "Content Structure Analysis:\n",
      "  Why needed: Modern websites use consistent CSS patterns for content\n",
      "  Techniques:\n",
      "    - DOM tree depth analysis\n",
      "    - Sibling element similarity\n",
      "    - CSS class pattern matching\n",
      "    - HTML5 semantic element detection\n",
      "\n",
      "Text Quality Metrics:\n",
      "  Why needed: To distinguish real content from navigation/boilerplate text\n",
      "  Techniques:\n",
      "    - Sentence boundary detection\n",
      "    - Punctuation density analysis\n",
      "    - Capitalization patterns\n",
      "    - Word frequency distribution\n",
      "    - Text coherence scoring\n",
      "\n",
      "Visual Layout Heuristics:\n",
      "  Why needed: Important content typically has distinct visual styling\n",
      "  Techniques:\n",
      "    - Font size analysis (via CSS)\n",
      "    - Text block positioning\n",
      "    - White space analysis\n",
      "    - Reading flow patterns\n",
      "\n",
      "Site-Specific Rules:\n",
      "  Why needed: Many sites follow consistent patterns that can be learned\n",
      "  Techniques:\n",
      "    - Domain-specific extraction rules\n",
      "    - CMS pattern recognition\n",
      "    - Publisher-specific templates\n",
      "    - Microdata/JSON-LD parsing\n",
      "\n",
      "Machine Learning Approaches:\n",
      "  Why needed: Data-driven approaches can generalize better than hand-crafted rules\n",
      "  Techniques:\n",
      "    - Content vs. boilerplate classification\n",
      "    - Feature engineering from DOM properties\n",
      "    - Neural content extraction models\n",
      "    - Transfer learning from labeled data\n",
      "\n",
      "============================================================\n",
      "LIBRARY COMPARISON & RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "Justext:\n",
      "  Best for: Multi-language news sites, blogs with lots of navigation\n",
      "  Pros: Language-aware, Statistical approach, Good at removing boilerplate, Handles multiple languages\n",
      "  Cons: Requires language specification, May be too aggressive, Less control over rules\n",
      "\n",
      "newspaper3k:\n",
      "  Best for: News articles, blog posts, journalistic content\n",
      "  Pros: Article-focused, Extracts metadata, Handles news sites well, Easy to use\n",
      "  Cons: News-specific, Less customizable, May miss non-article content\n",
      "\n",
      "Trafilatura:\n",
      "  Best for: General web scraping, academic content, diverse site types\n",
      "  Pros: Fast, High precision, Handles many site types, Good recall\n",
      "  Cons: Less metadata extraction, Newer library, May need fine-tuning\n",
      "\n",
      "Readability/readabilipy:\n",
      "  Best for: Article reading, content curation, reading applications\n",
      "  Pros: Based on Mozilla algorithm, Handles complex layouts, Good for articles\n",
      "  Cons: May be slower, Less control, Originally designed for reading view\n",
      "\n",
      "BeautifulSoup + Custom Rules:\n",
      "  Best for: Specific sites, custom requirements, maximum control needed\n",
      "  Pros: Full control, Site-specific optimization, Highly customizable\n",
      "  Cons: Requires manual tuning, Site-specific, Time-intensive\n",
      "\n",
      "============================================================\n",
      "PRODUCTION RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "Multi-Library Pipeline:\n",
      "  Description: Use 2-3 libraries in sequence for better results\n",
      "  Example: Trafilatura → newspaper3k → custom cleanup\n",
      "  Best for: High-volume, diverse content sources\n",
      "\n",
      "Site-Specific Rules:\n",
      "  Description: Develop custom extraction rules for major sources\n",
      "  Example: BBC: target .story-content, CNN: target .article-body\n",
      "  Best for: Limited number of high-priority sources\n",
      "\n",
      "Machine Learning Classification:\n",
      "  Description: Train models to classify content vs. boilerplate\n",
      "  Example: Features: text length, link density, CSS classes\n",
      "  Best for: Large-scale, automated processing\n",
      "\n",
      "Hybrid Heuristic System:\n",
      "  Description: Combine rule-based and statistical approaches\n",
      "  Example: Content scoring + language detection + DOM analysis\n",
      "  Best for: Balance between accuracy and maintainability\n",
      "\n",
      "============================================================\n",
      "KEY FINDINGS & CONCLUSION\n",
      "============================================================\n",
      "✓ No single library handles all cases perfectly\n",
      "✓ Basic extraction is rarely sufficient for production\n",
      "✓ Heuristic approaches provide significant improvement\n",
      "✓ Site-specific rules often yield the best results\n",
      "✓ Content quality varies dramatically across the web\n",
      "✓ Language detection is crucial for multilingual sites\n",
      "\n",
      "RECOMMENDED APPROACH:\n",
      "1. Start with Trafilatura or newspaper3k for baseline\n",
      "2. Add custom heuristics for specific content types\n",
      "3. Implement fallback strategies for edge cases\n",
      "4. Monitor extraction quality and iterate\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"WEB TEXT EXTRACTION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Analysis of text extraction techniques using real-world examples\")\n",
    "    \n",
    "    extractor = TextExtractor(SAMPLE_HTML)\n",
    "    \n",
    "    # Run all extraction methods\n",
    "    basic_result = extractor.basic_beautifulsoup_extraction()\n",
    "    improved_result = extractor.improved_beautifulsoup_extraction()\n",
    "    justext_result = extractor.justext_like_approach()\n",
    "    readability_result = extractor.readability_like_approach()\n",
    "    newspaper_result = extractor.newspaper_like_approach()\n",
    "    \n",
    "    # Real-world analysis\n",
    "    real_world_example_analysis()\n",
    "    \n",
    "    # Analysis and recommendations\n",
    "    additional_heuristics_needed()\n",
    "    compare_library_performance()\n",
    "    production_recommendations()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"KEY FINDINGS & CONCLUSION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"✓ No single library handles all cases perfectly\")\n",
    "    print(\"✓ Basic extraction is rarely sufficient for production\")\n",
    "    print(\"✓ Heuristic approaches provide significant improvement\")\n",
    "    print(\"✓ Site-specific rules often yield the best results\")\n",
    "    print(\"✓ Content quality varies dramatically across the web\")\n",
    "    print(\"✓ Language detection is crucial for multilingual sites\")\n",
    "    print(\"\\nRECOMMENDED APPROACH:\")\n",
    "    print(\"1. Start with Trafilatura or newspaper3k for baseline\")\n",
    "    print(\"2. Add custom heuristics for specific content types\")\n",
    "    print(\"3. Implement fallback strategies for edge cases\")\n",
    "    print(\"4. Monitor extraction quality and iterate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
