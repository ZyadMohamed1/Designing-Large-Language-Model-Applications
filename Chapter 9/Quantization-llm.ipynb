{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b69500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bitsandbytes) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bitsandbytes) (2.2.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zyad3\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-win_amd64.whl (66.5 MB)\n",
      "   ---------------------------------------- 0.0/66.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/66.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.3/66.5 MB 3.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 2.1/66.5 MB 3.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 2.9/66.5 MB 3.8 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 3.7/66.5 MB 3.8 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 4.5/66.5 MB 3.8 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 5.2/66.5 MB 3.8 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 6.0/66.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 7.1/66.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 7.9/66.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 8.7/66.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 9.4/66.5 MB 3.8 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 10.2/66.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 11.0/66.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 11.8/66.5 MB 3.8 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 12.6/66.5 MB 3.8 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 13.4/66.5 MB 3.8 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 14.2/66.5 MB 3.8 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 14.9/66.5 MB 3.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 15.7/66.5 MB 3.8 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 16.5/66.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 17.3/66.5 MB 3.8 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 18.1/66.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 18.9/66.5 MB 3.7 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 19.7/66.5 MB 3.8 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 20.4/66.5 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 21.2/66.5 MB 3.7 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 22.0/66.5 MB 3.7 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 22.8/66.5 MB 3.7 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 23.6/66.5 MB 3.7 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 24.4/66.5 MB 3.7 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 25.2/66.5 MB 3.7 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 26.0/66.5 MB 3.7 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 26.7/66.5 MB 3.7 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 27.5/66.5 MB 3.7 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 28.3/66.5 MB 3.7 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 29.1/66.5 MB 3.7 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 29.9/66.5 MB 3.7 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 30.7/66.5 MB 3.7 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 31.5/66.5 MB 3.7 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 32.0/66.5 MB 3.7 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 32.8/66.5 MB 3.7 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 33.6/66.5 MB 3.7 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 34.3/66.5 MB 3.7 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 35.1/66.5 MB 3.7 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 35.9/66.5 MB 3.7 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 36.7/66.5 MB 3.7 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 37.5/66.5 MB 3.7 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 38.3/66.5 MB 3.7 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 39.1/66.5 MB 3.7 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 39.8/66.5 MB 3.7 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 40.6/66.5 MB 3.7 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 41.4/66.5 MB 3.7 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 42.2/66.5 MB 3.7 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 43.0/66.5 MB 3.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 43.8/66.5 MB 3.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 44.6/66.5 MB 3.7 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 45.4/66.5 MB 3.7 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 46.1/66.5 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 46.9/66.5 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 47.4/66.5 MB 3.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 48.2/66.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 49.0/66.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 49.5/66.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 50.3/66.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 51.1/66.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 51.9/66.5 MB 3.7 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 52.7/66.5 MB 3.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 53.2/66.5 MB 3.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 54.0/66.5 MB 3.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 54.5/66.5 MB 3.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 55.3/66.5 MB 3.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 56.1/66.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 56.6/66.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 57.4/66.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 58.2/66.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 58.7/66.5 MB 3.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 59.5/66.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 60.3/66.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 60.8/66.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 61.6/66.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 62.4/66.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 62.9/66.5 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 63.7/66.5 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 64.5/66.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.0/66.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.8/66.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  66.3/66.5 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 66.5/66.5 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103bd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "DATASET_NAME = \"squad\"  # Using SQuAD for question answering\n",
    "NUM_SAMPLES = 50  # Keep small for quick testing\n",
    "MAX_NEW_TOKENS = 100\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b176d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBenchmark:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "    def get_model_size_mb(self, model) -> float:\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        return (param_size + buffer_size) / (1024 ** 2)\n",
    "    \n",
    "    def load_model_with_precision(self, precision: str):\n",
    "        print(f\"\\n🔄 Loading model in {precision} precision...\")\n",
    "        \n",
    "        if precision == \"float32\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    "            )\n",
    "        elif precision == \"float16\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    "            )\n",
    "        elif precision == \"bfloat16\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    "            )\n",
    "        elif precision == \"int8\":\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_threshold=6.0\n",
    "            )\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\" if DEVICE == \"cuda\" else None\n",
    "            )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_dataset(self) -> List[str]:\n",
    "        print(\"📚 Loading SQuAD dataset...\")\n",
    "        dataset = load_dataset(DATASET_NAME, split=\"validation\")\n",
    "        \n",
    "        prompts = []\n",
    "        for i, example in enumerate(dataset):\n",
    "            if i >= NUM_SAMPLES:\n",
    "                break\n",
    "            \n",
    "            prompt = f\"Context: {example['context']}\\nQuestion: {example['question']}\\nAnswer:\"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def benchmark_inference_time(self, model, prompts: List[str]) -> float:\n",
    "        print(\"⏱️  Measuring inference time...\")\n",
    "        \n",
    "        times = []\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts[:10]:  # Use subset for timing\n",
    "                inputs = self.tokenizer(\n",
    "                    prompt, \n",
    "                    return_tensors=\"pt\", \n",
    "                    truncation=True, \n",
    "                    max_length=512\n",
    "                )\n",
    "                \n",
    "                if DEVICE == \"cuda\":\n",
    "                    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "        \n",
    "        return np.mean(times)\n",
    "    \n",
    "    def evaluate_quality(self, model, prompts: List[str]) -> Dict:\n",
    "        print(\"📊 Evaluating model quality...\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        valid_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in prompts[:20]:  # Use subset for evaluation\n",
    "                try:\n",
    "                    inputs = self.tokenizer(\n",
    "                        prompt, \n",
    "                        return_tensors=\"pt\", \n",
    "                        truncation=True, \n",
    "                        max_length=512\n",
    "                    )\n",
    "                    \n",
    "                    if DEVICE == \"cuda\":\n",
    "                        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "                    \n",
    "                    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                    total_loss += outputs.loss.item()\n",
    "                    valid_samples += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Skipping sample due to error: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        avg_loss = total_loss / valid_samples if valid_samples > 0 else float('inf')\n",
    "        perplexity = np.exp(avg_loss)\n",
    "        \n",
    "        return {\n",
    "            \"average_loss\": avg_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"valid_samples\": valid_samples\n",
    "        }\n",
    "    \n",
    "    def run_benchmark(self, precision: str, prompts: List[str]) -> Dict:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"🧪 TESTING {precision.upper()} PRECISION\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Load model\n",
    "        start_load_time = time.time()\n",
    "        model = self.load_model_with_precision(precision)\n",
    "        load_time = time.time() - start_load_time\n",
    "        \n",
    "        # Calculate model size\n",
    "        model_size_mb = self.get_model_size_mb(model)\n",
    "        \n",
    "        # Measure inference time\n",
    "        avg_inference_time = self.benchmark_inference_time(model, prompts)\n",
    "        \n",
    "        # Evaluate quality\n",
    "        quality_metrics = self.evaluate_quality(model, prompts)\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "        \n",
    "        result = {\n",
    "            \"precision\": precision,\n",
    "            \"model_size_mb\": model_size_mb,\n",
    "            \"load_time_sec\": load_time,\n",
    "            \"avg_inference_time_sec\": avg_inference_time,\n",
    "            \"memory_usage_mb\": memory_usage,\n",
    "            **quality_metrics\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_all_benchmarks(self):\n",
    "        print(\"🚀 Starting Llama 3.2-1B Precision Comparison\")\n",
    "        print(f\"Device: {DEVICE}\")\n",
    "        print(f\"Samples: {NUM_SAMPLES}\")\n",
    "        \n",
    "        # Prepare dataset\n",
    "        prompts = self.prepare_dataset()\n",
    "        \n",
    "        # Test each precision\n",
    "        precisions = [\"float32\", \"float16\", \"bfloat16\", \"int8\"]\n",
    "        \n",
    "        for precision in precisions:\n",
    "            try:\n",
    "                result = self.run_benchmark(precision, prompts)\n",
    "                self.results.append(result)\n",
    "                print(f\"✅ {precision} completed successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {precision} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        self.display_results()\n",
    "    \n",
    "    def display_results(self):\n",
    "        if not self.results:\n",
    "            print(\"❌ No results to display\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"📊 LLAMA 3.2-1B PRECISION COMPARISON RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Storage Impact\n",
    "        print(\"\\n🗄️  STORAGE REQUIREMENTS:\")\n",
    "        print(f\"{'Precision':<12} {'Size (MB)':<12} {'vs Float32':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        float32_size = df[df['precision'] == 'float32']['model_size_mb'].iloc[0] if 'float32' in df['precision'].values else None\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            size_mb = row['model_size_mb']\n",
    "            reduction = f\"-{((float32_size - size_mb) / float32_size * 100):.1f}%\" if float32_size else \"N/A\"\n",
    "            print(f\"{row['precision']:<12} {size_mb:<12.1f} {reduction:<15}\")\n",
    "        \n",
    "        # Inference Time Impact\n",
    "        print(\"\\n⏱️  INFERENCE TIME:\")\n",
    "        print(f\"{'Precision':<12} {'Time (sec)':<12} {'vs Float32':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        float32_time = df[df['precision'] == 'float32']['avg_inference_time_sec'].iloc[0] if 'float32' in df['precision'].values else None\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            time_sec = row['avg_inference_time_sec']\n",
    "            speedup = f\"{(float32_time / time_sec):.2f}x faster\" if float32_time and time_sec > 0 else \"N/A\"\n",
    "            print(f\"{row['precision']:<12} {time_sec:<12.3f} {speedup:<15}\")\n",
    "        \n",
    "        # Performance Impact\n",
    "        print(\"\\n📈 MODEL PERFORMANCE:\")\n",
    "        print(f\"{'Precision':<12} {'Perplexity':<12} {'vs Float32':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        float32_ppl = df[df['precision'] == 'float32']['perplexity'].iloc[0] if 'float32' in df['precision'].values else None\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            ppl = row['perplexity']\n",
    "            degradation = f\"+{((ppl - float32_ppl) / float32_ppl * 100):.1f}%\" if float32_ppl else \"N/A\"\n",
    "            print(f\"{row['precision']:<12} {ppl:<12.2f} {degradation:<15}\")\n",
    "        \n",
    "        # Summary and Recommendation\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"🎯 QUANTIZATION ANALYSIS & RECOMMENDATION\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Find best balance\n",
    "        if len(df) > 1:\n",
    "            # Normalize metrics (lower is better for size and time, perplexity)\n",
    "            df_norm = df.copy()\n",
    "            df_norm['size_score'] = 1 - (df_norm['model_size_mb'] / df_norm['model_size_mb'].max())\n",
    "            df_norm['time_score'] = 1 - (df_norm['avg_inference_time_sec'] / df_norm['avg_inference_time_sec'].max())\n",
    "            df_norm['quality_score'] = 1 - ((df_norm['perplexity'] - df_norm['perplexity'].min()) / \n",
    "                                          (df_norm['perplexity'].max() - df_norm['perplexity'].min()) if df_norm['perplexity'].max() != df_norm['perplexity'].min() else 0)\n",
    "            \n",
    "            # Combined score (equal weights)\n",
    "            df_norm['combined_score'] = (df_norm['size_score'] + df_norm['time_score'] + df_norm['quality_score']) / 3\n",
    "            best_precision = df_norm.loc[df_norm['combined_score'].idxmax(), 'precision']\n",
    "            \n",
    "            print(f\"🏆 RECOMMENDED PRECISION: {best_precision.upper()}\")\n",
    "            \n",
    "            best_row = df[df['precision'] == best_precision].iloc[0]\n",
    "            print(f\"   • Model Size: {best_row['model_size_mb']:.1f} MB\")\n",
    "            print(f\"   • Inference Time: {best_row['avg_inference_time_sec']:.3f} sec\")\n",
    "            print(f\"   • Perplexity: {best_row['perplexity']:.2f}\")\n",
    "            \n",
    "            print(f\"\\n💡 INSIGHTS:\")\n",
    "            if 'int8' in df['precision'].values:\n",
    "                int8_row = df[df['precision'] == 'int8'].iloc[0]\n",
    "                if float32_size:\n",
    "                    size_reduction = (float32_size - int8_row['model_size_mb']) / float32_size * 100\n",
    "                    print(f\"   • INT8 quantization saves {size_reduction:.1f}% storage space\")\n",
    "                if float32_time:\n",
    "                    speed_improvement = float32_time / int8_row['avg_inference_time_sec']\n",
    "                    print(f\"   • INT8 quantization provides {speed_improvement:.2f}x speedup\")\n",
    "            \n",
    "            print(f\"   • Quantization IS worth it for deployment scenarios prioritizing:\")\n",
    "            print(f\"     - Reduced memory usage\")\n",
    "            print(f\"     - Faster inference\")\n",
    "            print(f\"     - Lower computational costs\")\n",
    "            print(f\"   • Use float32 only when maximum precision is critical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Required packages:\n",
      "   pip install torch\n",
      "   pip install transformers\n",
      "   pip install datasets\n",
      "   pip install evaluate\n",
      "   pip install bitsandbytes\n",
      "   pip install accelerate\n",
      "   pip install psutil\n",
      "   pip install pandas\n",
      "   pip install numpy\n",
      "\n",
      "==================================================\n",
      "🚀 Starting benchmark...\n",
      "==================================================\n",
      "🚀 Starting Llama 3.2-1B Precision Comparison\n",
      "Device: cpu\n",
      "Samples: 50\n",
      "📚 Loading SQuAD dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e1e210a15f4cf09632bcd2a2a6c63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyad3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\zyad3\\.cache\\huggingface\\hub\\datasets--squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625c432f0a3947378e2e0bb2eb7c6734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6736d31ebf44615aadee32282ae2b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20b503aa4f14d0a9c4439cf0eb000fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658d395dca2942129ae4d935a8cb5056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🧪 TESTING FLOAT32 PRECISION\n",
      "==================================================\n",
      "\n",
      "🔄 Loading model in float32 precision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Measuring inference time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating model quality...\n",
      "✅ float32 completed successfully\n",
      "\n",
      "==================================================\n",
      "🧪 TESTING FLOAT16 PRECISION\n",
      "==================================================\n",
      "\n",
      "🔄 Loading model in float16 precision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Measuring inference time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating model quality...\n",
      "✅ float16 completed successfully\n",
      "\n",
      "==================================================\n",
      "🧪 TESTING BFLOAT16 PRECISION\n",
      "==================================================\n",
      "\n",
      "🔄 Loading model in bfloat16 precision...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Measuring inference time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating model quality...\n",
      "✅ bfloat16 completed successfully\n",
      "\n",
      "==================================================\n",
      "🧪 TESTING INT8 PRECISION\n",
      "==================================================\n",
      "\n",
      "🔄 Loading model in int8 precision...\n",
      "❌ int8 failed: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
      "\n",
      "================================================================================\n",
      "📊 LLAMA 3.2-1B PRECISION COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "🗄️  STORAGE REQUIREMENTS:\n",
      "Precision    Size (MB)    vs Float32     \n",
      "----------------------------------------\n",
      "float32      4714.3       -0.0%          \n",
      "float16      2357.1       -50.0%         \n",
      "bfloat16     2357.1       -50.0%         \n",
      "\n",
      "⏱️  INFERENCE TIME:\n",
      "Precision    Time (sec)   vs Float32     \n",
      "----------------------------------------\n",
      "float32      15.478       1.00x faster   \n",
      "float16      14.586       1.06x faster   \n",
      "bfloat16     15.525       1.00x faster   \n",
      "\n",
      "📈 MODEL PERFORMANCE:\n",
      "Precision    Perplexity   vs Float32     \n",
      "----------------------------------------\n",
      "float32      4.64         +0.0%          \n",
      "float16      4.64         +0.1%          \n",
      "bfloat16     4.63         +-0.2%         \n",
      "\n",
      "================================================================================\n",
      "🎯 QUANTIZATION ANALYSIS & RECOMMENDATION\n",
      "================================================================================\n",
      "🏆 RECOMMENDED PRECISION: BFLOAT16\n",
      "   • Model Size: 2357.1 MB\n",
      "   • Inference Time: 15.525 sec\n",
      "   • Perplexity: 4.63\n",
      "\n",
      "💡 INSIGHTS:\n",
      "   • Quantization IS worth it for deployment scenarios prioritizing:\n",
      "     - Reduced memory usage\n",
      "     - Faster inference\n",
      "     - Lower computational costs\n",
      "   • Use float32 only when maximum precision is critical\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    benchmark = ModelBenchmark()\n",
    "    benchmark.run_all_benchmarks()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    required_packages = [\n",
    "        \"torch\", \"transformers\", \"datasets\", \"evaluate\", \n",
    "        \"bitsandbytes\", \"accelerate\", \"psutil\", \"pandas\", \"numpy\"\n",
    "    ]\n",
    "    \n",
    "    print(\"📦 Required packages:\")\n",
    "    for package in required_packages:\n",
    "        print(f\"   pip install {package}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚀 Starting benchmark...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
