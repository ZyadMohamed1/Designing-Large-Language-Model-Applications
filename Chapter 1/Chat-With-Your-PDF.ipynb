{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73751620",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea600e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import uuid\n",
    "import asyncio\n",
    "import logging\n",
    "import mimetypes\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from pathlib import Path\n",
    "\n",
    "# Core frameworks\n",
    "import aiofiles\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException, Depends\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import uvicorn\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.llms import OpenAI, Anthropic\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import Document\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Document processing\n",
    "import PyPDF2\n",
    "from pdf2image import convert_from_bytes\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import docx\n",
    "import pandas as pd\n",
    "\n",
    "# Vector store and database\n",
    "import chromadb\n",
    "from pymongo import MongoClient\n",
    "import motor.motor_asyncio\n",
    "\n",
    "# Utilities\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c77a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa82e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # API Keys\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    \n",
    "    # Database\n",
    "    MONGODB_URL = os.getenv(\"MONGODB_URL\", \"mongodb://localhost:27017\")\n",
    "    DATABASE_NAME = \"chat_pdf_db\"\n",
    "    \n",
    "    # Vector Store\n",
    "    CHROMA_PERSIST_DIR = \"./chroma_db\"\n",
    "    FAISS_INDEX_PATH = \"./faiss_index\"\n",
    "    \n",
    "    # File Processing\n",
    "    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB\n",
    "    SUPPORTED_FORMATS = {\n",
    "        'application/pdf': 'pdf',\n",
    "        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',\n",
    "        'application/msword': 'doc',\n",
    "        'text/plain': 'txt',\n",
    "        'application/vnd.ms-excel': 'xls',\n",
    "        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',\n",
    "        'image/png': 'png',\n",
    "        'image/jpeg': 'jpg',\n",
    "        'image/tiff': 'tiff'\n",
    "    }\n",
    "    \n",
    "    # Text Processing\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    MAX_TOKENS = 4000\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef5c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Models\n",
    "# Document metadata\n",
    "class DocumentMetadata(BaseModel):\n",
    "    file_name: str\n",
    "    file_type: str\n",
    "    file_size: int\n",
    "    page_count: int = 0\n",
    "    word_count: int = 0\n",
    "    character_count: int = 0\n",
    "    upload_date: datetime = Field(default_factory=datetime.now)\n",
    "    processing_status: str = \"pending\"\n",
    "    error_message: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b246885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatMessage(BaseModel):\n",
    "    role: str  # 'user' or 'assistant'\n",
    "    content: str\n",
    "    timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    sources: Optional[List[str]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a046d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSession(BaseModel):\n",
    "    session_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    document_id: str\n",
    "    messages: List[ChatMessage] = []\n",
    "    created_at: datetime = Field(default_factory=datetime.now)\n",
    "    updated_at: datetime = Field(default_factory=datetime.now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b756fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "    session_id: Optional[str] = None\n",
    "    document_id: str\n",
    "    max_tokens: Optional[int] = 1000\n",
    "    temperature: Optional[float] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79f021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryResponse(BaseModel):\n",
    "    answer: str\n",
    "    sources: List[str]\n",
    "    session_id: str\n",
    "    confidence_score: Optional[float] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b0849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Manager\n",
    "class DatabaseManager:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = motor.motor_asyncio.AsyncIOMotorClient(config.MONGODB_URL)\n",
    "        self.db = self.client[config.DATABASE_NAME]\n",
    "        self.documents = self.db.documents\n",
    "        self.sessions = self.db.chat_sessions\n",
    "        \n",
    "    async def save_document(self, doc_id: str, metadata: DocumentMetadata) -> bool:\n",
    "        try:\n",
    "            await self.documents.update_one(\n",
    "                {\"_id\": doc_id},\n",
    "                {\"$set\": metadata.dict()},\n",
    "                upsert=True\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving document metadata: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def get_document(self, doc_id: str) -> Optional[DocumentMetadata]:\n",
    "        try:\n",
    "            result = await self.documents.find_one({\"_id\": doc_id})\n",
    "            if result:\n",
    "                result.pop(\"_id\")\n",
    "                return DocumentMetadata(**result)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving document: {e}\")\n",
    "            return None\n",
    "    \n",
    "    async def save_chat_session(self, session: ChatSession) -> bool:\n",
    "        try:\n",
    "            session.updated_at = datetime.now()\n",
    "            await self.sessions.update_one(\n",
    "                {\"session_id\": session.session_id},\n",
    "                {\"$set\": session.dict()},\n",
    "                upsert=True\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving chat session: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def get_chat_session(self, session_id: str) -> Optional[ChatSession]:\n",
    "        try:\n",
    "            result = await self.sessions.find_one({\"session_id\": session_id})\n",
    "            if result:\n",
    "                result.pop(\"_id\")\n",
    "                return ChatSession(**result)\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving chat session: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4509ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Processor\n",
    "class DocumentProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.CHUNK_SIZE,\n",
    "            chunk_overlap=config.CHUNK_OVERLAP,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    async def process_document(self, file: UploadFile, doc_id: str) -> tuple[List[Document], DocumentMetadata]:\n",
    "        try:\n",
    "            # Read file content\n",
    "            content = await file.read()\n",
    "            \n",
    "            # Detect file type\n",
    "            mime_type = file.content_type or mimetypes.guess_type(file.filename)[0]\n",
    "            if mime_type not in config.SUPPORTED_FORMATS:\n",
    "                raise ValueError(f\"Unsupported file type: {mime_type}\")\n",
    "            \n",
    "            # Extract text based on file type\n",
    "            text, page_count = await self._extract_text(content, mime_type, file.filename)\n",
    "            \n",
    "            # Create metadata\n",
    "            metadata = DocumentMetadata(\n",
    "                file_name=file.filename,\n",
    "                file_type=config.SUPPORTED_FORMATS[mime_type],\n",
    "                file_size=len(content),\n",
    "                page_count=page_count,\n",
    "                word_count=len(text.split()),\n",
    "                character_count=len(text),\n",
    "                processing_status=\"completed\"\n",
    "            )\n",
    "            \n",
    "            # Split text into chunks\n",
    "            chunks = self.text_splitter.split_text(text)\n",
    "            \n",
    "            # Create Document objects\n",
    "            documents = [\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"document_id\": doc_id,\n",
    "                        \"file_name\": file.filename,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks)\n",
    "                    }\n",
    "                )\n",
    "                for i, chunk in enumerate(chunks)\n",
    "            ]\n",
    "            \n",
    "            return documents, metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing document: {e}\")\n",
    "            metadata = DocumentMetadata(\n",
    "                file_name=file.filename,\n",
    "                file_type=\"unknown\",\n",
    "                file_size=len(content) if 'content' in locals() else 0,\n",
    "                processing_status=\"failed\",\n",
    "                error_message=str(e)\n",
    "            )\n",
    "            return [], metadata\n",
    "    \n",
    "    async def _extract_text(self, content: bytes, mime_type: str, filename: str) -> tuple[str, int]:\n",
    "        \n",
    "        if mime_type == 'application/pdf':\n",
    "            return await self._extract_from_pdf(content)\n",
    "        elif mime_type in ['image/png', 'image/jpeg', 'image/tiff']:\n",
    "            return await self._extract_from_image(content), 1\n",
    "        elif mime_type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':\n",
    "            return await self._extract_from_docx(content), 1\n",
    "        elif mime_type == 'text/plain':\n",
    "            return content.decode('utf-8'), 1\n",
    "        elif mime_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:\n",
    "            return await self._extract_from_excel(content), 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported MIME type: {mime_type}\")\n",
    "    \n",
    "    async def _extract_from_pdf(self, content: bytes) -> tuple[str, int]:\n",
    "        try:\n",
    "            # Try standard PDF text extraction first\n",
    "            pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))\n",
    "            text = \"\"\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            \n",
    "            for page in pdf_reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                text += page_text + \"\\n\"\n",
    "            \n",
    "            # If no text extracted, use OCR\n",
    "            if not text.strip():\n",
    "                logger.info(\"No text found in PDF, using OCR...\")\n",
    "                text = await self._ocr_pdf(content)\n",
    "            \n",
    "            return text, page_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from PDF: {e}\")\n",
    "            # Fallback to OCR\n",
    "            return await self._ocr_pdf(content), 1\n",
    "    \n",
    "    async def _ocr_pdf(self, content: bytes) -> str:\n",
    "        try:\n",
    "            images = convert_from_bytes(content)\n",
    "            text = \"\"\n",
    "            \n",
    "            for image in images:\n",
    "                page_text = pytesseract.image_to_string(image)\n",
    "                text += page_text + \"\\n\"\n",
    "            \n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"OCR failed: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    async def _extract_from_image(self, content: bytes) -> str:\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(content))\n",
    "            return pytesseract.image_to_string(image)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Image OCR failed: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    async def _extract_from_docx(self, content: bytes) -> str:\n",
    "        try:\n",
    "            doc = docx.Document(io.BytesIO(content))\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DOCX extraction failed: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    async def _extract_from_excel(self, content: bytes) -> str:\n",
    "        try:\n",
    "            df = pd.read_excel(io.BytesIO(content))\n",
    "            return df.to_string()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Excel extraction failed: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e1213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store Manager\n",
    "class VectorStoreManager:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize embeddings\n",
    "        if config.OPENAI_API_KEY:\n",
    "            self.embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "        else:\n",
    "            # Fallback to local embeddings\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            )\n",
    "        \n",
    "        # Initialize vector store\n",
    "        self.chroma_client = chromadb.PersistentClient(path=config.CHROMA_PERSIST_DIR)\n",
    "        \n",
    "    async def add_documents(self, documents: List[Document], doc_id: str) -> bool:\n",
    "        try:\n",
    "            # Create collection for this document\n",
    "            collection_name = f\"doc_{doc_id}\"\n",
    "            \n",
    "            # Create Chroma vector store\n",
    "            vectorstore = Chroma(\n",
    "                client=self.chroma_client,\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            \n",
    "            # Add documents\n",
    "            await asyncio.get_event_loop().run_in_executor(\n",
    "                None, vectorstore.add_documents, documents\n",
    "            )\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding documents to vector store: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def get_retriever(self, doc_id: str, k: int = 4):\n",
    "        try:\n",
    "            collection_name = f\"doc_{doc_id}\"\n",
    "            \n",
    "            vectorstore = Chroma(\n",
    "                client=self.chroma_client,\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=self.embeddings\n",
    "            )\n",
    "            \n",
    "            return vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating retriever: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d902ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Manager\n",
    "class LLMManager:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        \n",
    "        # Initialize OpenAI\n",
    "        if config.OPENAI_API_KEY:\n",
    "            self.models['openai'] = ChatOpenAI(\n",
    "                openai_api_key=config.OPENAI_API_KEY,\n",
    "                model_name=\"gpt-3.5-turbo\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=config.MAX_TOKENS\n",
    "            )\n",
    "        \n",
    "        # Initialize Anthropic\n",
    "        if config.ANTHROPIC_API_KEY:\n",
    "            self.models['anthropic'] = ChatAnthropic(\n",
    "                anthropic_api_key=config.ANTHROPIC_API_KEY,\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=config.MAX_TOKENS\n",
    "            )\n",
    "    \n",
    "    def get_model(self, provider: str = \"openai\"):\n",
    "        return self.models.get(provider, list(self.models.values())[0] if self.models else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28ffb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Chat Service\n",
    "class ChatService:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.db_manager = DatabaseManager()\n",
    "        self.doc_processor = DocumentProcessor()\n",
    "        self.vector_manager = VectorStoreManager()\n",
    "        self.llm_manager = LLMManager()\n",
    "        \n",
    "    async def upload_document(self, file: UploadFile) -> str:\n",
    "        # Generate document ID\n",
    "        doc_id = str(uuid.uuid4())\n",
    "        \n",
    "        try:\n",
    "            # Validate file size\n",
    "            content = await file.read()\n",
    "            if len(content) > config.MAX_FILE_SIZE:\n",
    "                raise HTTPException(400, \"File too large\")\n",
    "            \n",
    "            # Reset file position\n",
    "            await file.seek(0)\n",
    "            \n",
    "            # Process document\n",
    "            documents, metadata = await self.doc_processor.process_document(file, doc_id)\n",
    "            \n",
    "            if metadata.processing_status == \"failed\":\n",
    "                raise HTTPException(400, f\"Document processing failed: {metadata.error_message}\")\n",
    "            \n",
    "            # Save to vector store\n",
    "            if documents:\n",
    "                success = await self.vector_manager.add_documents(documents, doc_id)\n",
    "                if not success:\n",
    "                    raise HTTPException(500, \"Failed to index document\")\n",
    "            \n",
    "            # Save metadata to database\n",
    "            await self.db_manager.save_document(doc_id, metadata)\n",
    "            \n",
    "            return doc_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error uploading document: {e}\")\n",
    "            raise HTTPException(500, str(e))\n",
    "    \n",
    "    async def chat_with_document(self, query: QueryRequest) -> QueryResponse:\n",
    "        try:\n",
    "            # Get or create chat session\n",
    "            session = None\n",
    "            if query.session_id:\n",
    "                session = await self.db_manager.get_chat_session(query.session_id)\n",
    "            \n",
    "            if not session:\n",
    "                session = ChatSession(\n",
    "                    document_id=query.document_id,\n",
    "                    session_id=query.session_id or str(uuid.uuid4())\n",
    "                )\n",
    "            \n",
    "            # Get retriever\n",
    "            retriever = await self.vector_manager.get_retriever(query.document_id)\n",
    "            if not retriever:\n",
    "                raise HTTPException(404, \"Document not found or not indexed\")\n",
    "            \n",
    "            # Get LLM\n",
    "            llm = self.llm_manager.get_model(\"openai\")\n",
    "            if not llm:\n",
    "                raise HTTPException(500, \"No LLM available\")\n",
    "            \n",
    "            # Create conversation chain\n",
    "            memory = ConversationBufferWindowMemory(\n",
    "                memory_key=\"chat_history\",\n",
    "                return_messages=True,\n",
    "                k=5  # Keep last 5 exchanges\n",
    "            )\n",
    "            \n",
    "            # Load previous conversation\n",
    "            for msg in session.messages[-10:]:  # Last 10 messages\n",
    "                if msg.role == \"user\":\n",
    "                    memory.chat_memory.add_user_message(msg.content)\n",
    "                else:\n",
    "                    memory.chat_memory.add_ai_message(msg.content)\n",
    "            \n",
    "            # Create chain\n",
    "            qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=llm,\n",
    "                retriever=retriever,\n",
    "                memory=memory,\n",
    "                return_source_documents=True,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Get answer\n",
    "            result = await asyncio.get_event_loop().run_in_executor(\n",
    "                None, qa_chain, {\"question\": query.question}\n",
    "            )\n",
    "            \n",
    "            answer = result[\"answer\"]\n",
    "            source_docs = result.get(\"source_documents\", [])\n",
    "            \n",
    "            # Extract sources\n",
    "            sources = [\n",
    "                f\"{doc.metadata.get('file_name', 'Unknown')} (chunk {doc.metadata.get('chunk_index', 0)})\"\n",
    "                for doc in source_docs\n",
    "            ]\n",
    "            \n",
    "            # Add messages to session\n",
    "            session.messages.append(ChatMessage(role=\"user\", content=query.question))\n",
    "            session.messages.append(ChatMessage(\n",
    "                role=\"assistant\", \n",
    "                content=answer, \n",
    "                sources=sources\n",
    "            ))\n",
    "            \n",
    "            # Save session\n",
    "            await self.db_manager.save_chat_session(session)\n",
    "            \n",
    "            return QueryResponse(\n",
    "                answer=answer,\n",
    "                sources=sources,\n",
    "                session_id=session.session_id\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in chat: {e}\")\n",
    "            raise HTTPException(500, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "824d20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# FastAPI Application\n",
    "app = FastAPI(title=\"Chat with PDF API\", version=\"1.0.0\")\n",
    "chat_service = ChatService()\n",
    "\n",
    "@app.post(\"/upload\", response_model=dict)\n",
    "async def upload_document(file: UploadFile = File(...)):\n",
    "    doc_id = await chat_service.upload_document(file)\n",
    "    return {\"document_id\": doc_id, \"status\": \"uploaded\"}\n",
    "\n",
    "@app.post(\"/chat\", response_model=QueryResponse)\n",
    "async def chat_with_document(query: QueryRequest):\n",
    "    return await chat_service.chat_with_document(query)\n",
    "\n",
    "@app.get(\"/document/{doc_id}\", response_model=DocumentMetadata)\n",
    "async def get_document_info(doc_id: str):\n",
    "    metadata = await chat_service.db_manager.get_document(doc_id)\n",
    "    if not metadata:\n",
    "        raise HTTPException(404, \"Document not found\")\n",
    "    return metadata\n",
    "\n",
    "@app.get(\"/session/{session_id}\", response_model=ChatSession)\n",
    "async def get_chat_session(session_id: str):\n",
    "    session = await chat_service.db_manager.get_chat_session(session_id)\n",
    "    if not session:\n",
    "        raise HTTPException(404, \"Session not found\")\n",
    "    return session\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6fbe22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['d:\\\\Designing-Large-Language-Model-Applications\\\\Chapter 1']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [19936] using WatchFiles\n",
      "INFO:     Stopping reloader process [19936]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(\n",
    "        \"main:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        reload=True,\n",
    "        log_level=\"info\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c45221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
